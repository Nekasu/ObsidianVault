---

---
# 今日生活

| 昨天睡觉时间 | 今早起床时间 | 今日体重 | 今日锻炼 | 剩余资金 (支) | 剩余资金 (微) |
| ------ | ------ | ---- | ---- | ------- | ------- |
|        |        |      |      |         |         |

1. 昨天睡觉时间：
2. 今早起床时间：
3. 今日体重：

# 新年计划

![[2024-01-01#新的一年]]

# 近日安排

# 今日计划

1. [ ] 看看<mark style="background: #BBFABBA6;">待研究</mark>与<mark style="background: #BBFABBA6;">待整理</mark>,  <mark style="background: #BBFABBA6;">近日安排</mark>标签

## 科研

1. [ ] 现在神经网络拥有什么组件, 各个组件的功能是什么, 具有什么样的效果. 记录在[[神经网络组件目录文件_README]] 中
2. [x] 阅读[小波卷积文章](https://arxiv.org/abs/2407.05848)
3. [ ] 思考在代码层面如何将小波卷积与部分卷积结合
4. [x] 修改 PartialOct
	1. 首先使用新的数据集进行训练, 查看报错信息, 以找到需要修改的代码
		1. 发现问题：
			1. 在第一轮循环中, 损失函数中的 G_loss、G_Contrast 均存在 nan 的部分,  G_Percept 则没有这个问题.
			2. 而在第二论循环中, 三个损失函数均为 nan
	2. 首先解决第一轮循环的问题.
		1. 观察源码可以发现, `G_loss=G_Contrast + G_Percept`, 所以真正的问题在损失函数的 `G_Contrast` 部分.
		2. [x] 现检查损失函数 `G_Contrast` 部分的错误
		3. 为了得知是什么地方出现了错误,  需要进行以下操作
			1. [x] 检查该损失函数的计算输出, 查看输出是否为 nan
				1. 检查后发现, 从第 0 个 epoch 的第 0 次循环开始, 该损失函数一直为 nan, 说明损失函数 `G_Contrast` <mark style="background: #6495EDA6;">计算过程</mark>或<mark style="background: #ADCCFFA6;">输入数据</mark>有误
				2. [ ] 首先查看该损失函数的输入, 查看是否为 nan. 若输入数据为 nan, 则说明输入数据有误, 应检查输入数据. 输入数据不为 nan, 则应检查损失函数的计算过程.
					1. 检查结果显示：
						1. 由于结果过长, 所以请参阅文件 [[2024年12月1日_G_contrast函数检查]]
					2. 基于检查结果的推理：
						1. [[2024年12月1日_G_contrast函数检查#基于检查结果的推理]]
					3. 根据以上推理, 可以得出结论：
						1. [[2024年12月1日_G_contrast函数检查#根据以上推理, 可以得出结论]]
					4. 从而可以得出以下解决方案
						1. [[2024年12月1日_G_contrast函数检查#解决方案]]
				4. 其次应检查损失函数 `G_Contrast` 的计算过程
		4. ~~杨欢提示：可能是反向传播时梯度爆炸产生的问题, 可以考虑将学习率缩小多个数量级进行检查~~
			1. ~~2024-12-01-16:53：缩小三个数量级后发现依旧有问题, 所以不是学习率出现了问题~~
	3. 其次解决第二轮及以后循环的问题.
		1. 首先查看损失函数 `G_Percept` 的输入, 查看是否为 nan. 若输入数据为 nan, 则说明输入数据有误, 应检查输入数据. 输入数据不为 nan, 则应检查损失函数的计算过程.
			1. 检查结果显示：
				1. 第 0 个 epoch 的第 1 次循环中, 输入数据均不为 nan；
				2. 而之后的循环中, 三个输入--real_A 均不为 nan, real_B 均不为 nan, trs_AtoB 均为 nan
			2. 基于检查结果的推理：
				1. 第 0 个 epoch 的第 1 次循环中, 输入数据均不为 nan. 在输入数据均不为 nan 的情况下, 该损失函数计算结果为也不为 nan, 说明该损失函数 `G_Percept` 的计算过程应该无误
				2. 之后的循环中, 三个输入--real_A 均不为 nan, real_B 均不为 nan, trs_AtoB 均为 nan. 三个输入数据中, 风格迁移结果 trs_AtoB 为 nan, 说明生成器 Generator 出现了错误.
			3. 根据以上推理, 可以得出结论：
				1. 生成过程 `Encoder-Decoder` 计算过程有误
			4. 从而可以得出以下解决方案
				1. 生成过程 `Encoder-Decoder` 计算过程有误, 首先检查 `Encoder` 的输出结果是否有误, 其次检查 `Deocder` 的输出结果是否有误. 深入计算过程, 逐步检查其中数据, 查看哪里出现错误
			5. 现开始推进上述解决方案
				1. 结果显示, StyleEncoder 返回的 o 7 中两个张量均为 nan, 而 ContentEncoder、StyleEncoder 返回的 o 13 均不为 nan. 
				2. 结果表明, 是 StyleEncoder 出现了问题.
	4. [x] 2024 年 12 月 1 日-21:57 周日：结合上述错误排查 2 与 3 可以发现, 是 StyleEncoder 出现了问题, 目前应按照 [[2024年12月1日_G_contrast函数检查#解决方案]]解决该问题
		1. 2024-12-03-22:05 周二：测试表明, 确实是网络太浅的原因. 将网络深度从 2 层增加为 3 层后 (最后一层重复了一次), StyleEncoder 的编码结果不再为 nan 了. 同时, 为了和 StyleEncoder 输出张量 (Tensor)的维度一致, 对 ContentEncoder 做了相同的处理.
	5. [x] 在修改两个 Encoder 后, 损失函数依旧为 nan, 整理发现, Decoder 的结果也为 nan, 为了找到 Decoder 哪部分除了问题, 需要进行以下操作.
		1. [ ] 方案 1：缩小学习率
		2. [x] 方案 2：在 Decoder 的 forward 函数中逐个打印, 看看在前向传递时, 什么步骤导致了数据变为 nan.
			1. 2024-12-03-22:10 周二：开始该工作
			2. 2024-12-03-22:42 周二：输入数据并非为 nan, 而在第一个输出时就变成了 nan. 输入数据到第一个输出之间, 经过了一层 AdaOctConv 1_1, 现在进入该层的 forward 函数中, 查看什么时候数据变为了 nan
			3. 2024-12-03-23:09 周二：找到问题所在, blocks. Py 文件里的 AdaOctConv 类、forward 函数中的语句 `ouput=self.OctConv(output)` (目前在 315 行)导致了数据变为 nan. 
				1. 应该进入 OctConv 类的 forward 函数, 查看是什么时候数据变为了 nan.
			4. 2024-12-05-9:50 周四：发现 oct_conv_aftup_2 也有问题. 在第一次循环时, 代码运行到此处, 原本输入均不为 nan 的高低频信息经过 oct_conv_aftup_2 的处理后, 低频信息变为了 nan. 需要检查 oct_conv_aftup 的问题.
				1. 现排查 oct_conv_aftup 代码
			5. 2024-12-05-16:56 周四：发现实际上, 是数据过大, 溢出了 float 32 位上限导致的. 通过检查数据的 max 与 min 可以发现, 在 StyleEncoder 中的数据产生了爆炸, 
				1. 具体如下
					1. StyleEncoder 的输入数据最大为 2.6, 最小为-2.11
					2. 经过第一个 conv (即 PartialConv 2 d)后, 最大值为 2314, 最小值为-9443
					3. 经过第一层后, 最大值为 1.57832 e 5, 最小值为-0.0791
					4. 经过第二层后, 数据最大、最小值为 17612916, -0.1370
					5. 第三层,  数据最大、最小值为 6.6367 e+09, -5.0717,.
				2. 而在 ContentEncoder 中, 则不会出现这样数据爆炸的问题：
					1. In ContentEncoder 第二层,  数据最大、最小值为 0.1408, -0.0016
					2.  ContentEncoder 第三层,  数据最大、最小值为 0.0268, -0.0003
				3. 故而推测, 整个代码出现 nan 问题的原因在于, PartialConv 的代码导致了数据数量级的爆炸, 最终导致了数据超出 float 32 的上限, 变成了 nan.
				4. 为了解决这个问题, 需要排查 PartialConv 代码, 以保证数据经过该神经网络组件后, 不会出现数量级爆炸问题
			6. 2024-12-05-17:07 周四：现应单独检查 PartConv. 逐步输出, 查看是哪里造成了数据数量级爆炸
				1. 2024-12-05-17:36 周四：发现问题所在. 代码中的
					1. `self.conv_1 = self.conv`      
					2. `nn.init.constant_(self.conv_1.weight, 1)` 
				2. 这两句的本意是创建一个与 `conv` 同样大小的且数值全为 1 的卷积核 `conv_1`. 但是执行第二句后, 卷积核 `conv` 与 `conv_1` 中的权重全变为了 1. 这导致了整体梯度的爆炸.
				3. 修改：不再使用赋值语句创建 conv_1, 而是使用创建 conv 的代码创建一个与 conv 完全相同的 conv_1, 并使用 `nn.init.constant_(self.conv_1.weight, 1)` 初始化为 1. 最后测试两个卷积, 查看其中权重是否一致.
				4. 2024-12-05-21:35 周四：修改完毕, 现在 Encoder 中不再出现 nan 问题
5. [x] 2024-12-05-21:35 周四：在完成 Encoder 的修改后, StyleEncoder 与 ContentEncoder 均不再生成过大的 nan 数据. 但是网络中的损失函数计算依旧为 nan (不是 inf, 且该问题在 iteration=1 时, 即第二轮循环时才会出现 ). 为了找到问题所在, 需要进行工作.
	1. 检查损失函数的输入数据, 以确实是输入数据出现了问题还是函数计算出现了问题
	2. 2024-12-05-22:31 周四：跟据下图可以发现, 在 PartialConv 中, 第二步的输出还是一个较小的值, 而最后的返回值却变成了一个很大的值. 这可能导致了数据的溢出. 现在需要检查 PartialOct 第二步到返回数据之间的代码. 相信问题就在其中.
		1. ![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20241205223112.png)
		2. 需要检查所有设计 out 计算的代码, 并逐步打印 out 数据, 以查看在哪个步骤中出现了数据爆炸的问题.
	3. 2024-12-05-22:46 周四：实际上, 第二步的输出还是一个较小的值, 而最后的返回值可能变成一个较大的值, 也可能变成一个较小的值, 如下图所示. 
		1. ![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20241205224855.png)
		2. 所以在第二步与第四中有某个或某些操作导致了数据的剧烈变化
		3. 而在检查后发现, 第二步与第四步之间的数据中, 仅仅有一句 `        self.out: torch.Tensor = self.out * self.ratio # out2` 改变了 `out` 的值.
		4. 总上所述, 导致 `out` 数值不稳定的核心问题在于 ratio 的值.
		5. 需要检查/更改/舍弃 ratio 的计算
			1. 实际上, 我们可以这样认为, ratio 这个值是在存在 mask 时, 为了弥补 mask 的值而存在的. 
			2. 而在 mask 中的数值全部为 1 时, 即可认为 mask 没有任何作用, 故而根据 mask 计算的 ratio 也没有任何意义.
		6. 打印 ratio 的最大最小值如下：
			1. ![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20241206002928.png)
			2. 发现 ratio 变化很大, 而且数值出现异常
			3. 根据 ratio 的公式：$\frac{sum(I)}{sum(M)}$, 分子应为定值 (卷积窗口的大小), 分母应为一个在\[0, 窗口大小\]之间的整数, 所以这个分式的最小值为 0 (分母为 0 时), 最大值为 $sum(I)$ (当分母为 1 时), 即卷积窗口的大小. 可以确定的是, 卷积窗口并不是一个很大的值, 所以 ratio 的计算一定出现了错误.
		7. 解决方案：需要排查 ratio 的计算过程, 单独测试 PartialConv 2 d 类. 若 ratio 类计算无误, 在也应该进行归一化处理 <mark style="background: #FF5582A6;">(实际上, ratio 的归一化处理也可以算一个创新点)</mark>
			1. 2024-12-06-11:08 周五：发现问题实际出在 sum_mask 的计算. 整个计算过程中存在接近于零的数. 可能是 in_mask 中具有非 0 且非 1 的值导致的.
				1. 在 PartialConv 2 d 的 forward 中, 将 in_mask 提前处理为非 0 及 1 即可
			2. 2024-12-06-11:45 周五：发现问题, 如果卷积核大小为 1 , ratio 会变的很小, 所以有以下优化. 当 kernelsize 为 1 时, 无须计算 ratio, 并跳过与 ratio 计算的步骤
			3. 2024-12-06-15:01 周五：发现 sum_mask 均是 in_mask. Shape\[1\]的倍数, 且除以这个倍数后得到的结果恰好为正确结果 
				1. 修改后缓解了数据爆炸的速度
				2. 但是数据依旧会不停膨胀. 所以考虑在 PartialConv2d 返回前, 使用 ReLu 进行约束.
	4. [x] 2024-12-05-23:04 周四, 发现一个更为致命的问题, 现决定先解决该问题, 稍后解决上面的数据爆炸问题
		1. 问题描述
			1. 问题 1：经过更细致的检测后可以发现, 输入的掩膜数据有问题, 其最大最小值不为 (1,0), 可能是经过了 Normalize 处理. 如图
				1. ![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20241205230714.png)
				2. 解决方案：看看 mask 是否也经过了 Normalize. Mask 不应经过 Normalize 层的处理.
				3. 2024-12-05-23:21 周四：问题解决, 若 mask 不经过 Normalize 操作, 则不会影响.
			2. 问题 2：当上一个输出的 mask 最大最小值均为 1.0 时, 作为下一个 PartialConv 的输入时, 最大最小值却变成了 2.0. 如图
				1. ![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20241205230751.png)
				2. 解决方案：首先应该检查是什么地方出现了这种问题, 然后查看对应代码, 为何会从 1.0 变为 2.0, 最后对症下药.
					1. 检查发现, 在 StyleEncoder 中的 PartialOctConv 1_2 到 PartialOctConv 1_3 之间, mask 的值会从 1.0 变成 2.0
					2. 发现问题, 在 blocks. Py 文件的 PartialOctConv 类的 forward 函数中, 最后一个 else 分支里, 当 `is_dw` 为 False 时, 返回的 mask 有误.  实际上, 在 PartConv 2 d 中显示的 mask 生成正确, 而最终返回的 mask 错误. 所以应该是返回时出现了问题.
					3. 检查语句后发现, 以下两个语句：
						1. `hf, h_mask = tuple(x + y for x, y in zip(a, b))`
						2. `lf, l_mask = tuple(x + y for x, y in zip(c, d))`
					4. 在返回前, 将 mask 的值相加了.
					5. 为了解决这个问题, 可能只需要返回 H 2 H 的掩膜即可, 因为该掩膜没有进行任何上下采样操作, 是一个精度较高的掩膜 
					6. <mark style="background: #6495EDA6;">实际上, 应该存在更好的方法, 但是偷懒了, 就先这样的</mark>
				3. 2024-12-05-23:57 周四：该问题解决
6. [ ] 开展小波卷积与部分卷积结合的代码编写工作
7. [ ] 准备修改综述论文

## 学业

1. [ ] 
2. [ ] 
3. [ ] 
4. [ ] 

## 兴趣

1. [ ] 
2. [ ] 
3. [ ] 

## 工作

1. [ ] 准备扬州会议报销工作

# 其他今日所学



# 今日趣事



# 今日口诀


