<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diary-2025年1月20日</title>
    <link rel="stylesheet" href="/Template/styles.css">
    <script id="MathJax-script" async src="/Template/js/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>今日生活</h1>
        <table id="table-blue">
            <thead>
                <tr>
                    <th>日期</th>
                    <th>昨天睡觉时间</th>
                    <th>今天起床时间</th>
                    <th>今日体重</th>
                    <th>今日锻炼</th>
                    <th>昨日资金剩余(支付宝)</th>
                    <th>昨日资金剩余(微信)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>0</td>
                    <td>1</td>
                    <td>2</td>
                    <td>3</td>
                    <td>4</td>
                    <td>5</td>
                    <td>6</td>
                </tr>
            </tbody>
        </table>

        <h1>新年计划</h1>
        <span class="color-DrRatio-blue">“要么立刻行动, 要么一事无成.” </span> -- 真理医生\(\cdot\)星野<br>
        <span class="color-huohuo-green">“太大的目标容易吓到自己……所以要把大目标化成多个小目标哦……”</span>--藿藿\(\cdot\)星野
        <hr>
        <h1>近日安排</h1>
        <ol>
            <del><li>完成论文校对工作
                <ol>
                    <del><li>已阅读与观看 Elsevier 官网的文字教程和视频教程, 已大致了解的工作流程</li></del>
                    <del><li>需要校对论文引用是否出错, 有几个重要的地方<br></del>
                        <ol>
                            <del><li>介绍风格迁移应用的地方, 引用可能有误, 需要重新确认</li></del>
                            <del><li>介绍风格迁移领域的文章应用是否出错, 需要一一对照</li></del>
                        </ol>
                    </li>
                    <del><li>最后一一回复 “Queries to Authoer” 的 13 个问题. 这些问题完全回复后, 才能点击 submit 按钮. 将这个工作最后完成是为了当作保障, 防止在尚未完成校对的情况下, 不小心按到 submit 提前结束校对.</li></del>
                    <li>这篇文章使用了AIGC辅助初稿撰写, 我害怕被编辑部发现, 进而退稿. 更害怕被读者发现, 导致我在学术圈的学术信用破产. 现在要对此反思, 以确定是否有必要进行下一步行动, 如何进行下一步行动.
                        &ensp; &ensp; 首先需要问自己一个问题: 担心是否是必要的? 是的, 我认为这样的担心是有必要的. 使用 AI 辅助撰写初稿, 是不争的事实; 使用 AIGC 翻译初稿, 也是不争的事实. 而使用 AIGC 可能对自己造成很大的影响, 所以被人看出这篇文章使用的 AIGC 的担忧是有必要的. 而我担心的点就在这里: 担心AIGC辅助的文章, 会对我之后的科研路造成影响.<br>
                        &ensp; &ensp; 如何消除自己的担心? 我担心的本质是害怕被发现使用了 AIGC 撰写初稿, 使用 AIGC 翻译目前是被 Neurocomputing 接受的. 为何学术期刊要禁止使用AIGC? 我认为, 为了防止学术内容出现差错，确保研究的准确性与可靠性，学术期刊通常会限制或禁止AIGC的使用，尤其是在文章的核心内容生成和分析部分。所以, 为了消除自己的担忧, 只需要做一件即可. 即确认自己文章的准确性. 只要自己的文章没有大错误, 整体语言是否使用 AI 翻译, 并不是一个很重要的点. 为了确认自己的文章是否正确, 可以使用不同的 AI 判断文章所写内容是否与对应的论文一致, 如果一致则可大胆放心. 更不用说, 目前已经被接收, 且已经完成校对工作. <br>
                        <span class="color-huohuo-green">所以! 最核心的点, 就是检查自己增加的部分(第一次修改后, 增加的部分)的文章, 是否与原文一致.</span><br>
                        2025-01-21-0:20 周二: 检查完毕, 发现两处问题. 其一, 文章内容与描述不一致, 文章名为“ReCoNet: Real-Time Coherent: Video Style Transfer Network”. 其二, 在介绍 3D 风格迁移时, 误将 2D 风格迁移的内容放到该环节中. 已向编辑部发送邮件请求再次修改的机会, 等待邮件中......今晚先好好休息吧......
                    </li>
                </ol>
            </li></del>
            <li>完成新论文的撰写工作(需要细化)</li>
            <li>锻炼！</li>
            <li>协助董仕豪审稿(需要细化)</li>
        </ol>
        <hr>
        <h1>今日计划</h1>
        <h2>科研</h2>
        <ol>
            <li>
                2025-01-14-15:08 周二: 对比实验训练完毕, 分析结果记录在 10.15.114.228 中 /mnt/sda/zxt/3_code_area/code_develop/PartialConv_AesFA/ckpt/dunhuang_white_main/readme_dunhuang_white_main.html 的文件中. 现根据分析记录进行以下任务：需要加大训练轮次(5倍), 重新开始训练, 以完成 <span class="color-huohuo-green">re-dh-white-main</span>项目(下称该项目). 在训练期间, 需要同步进行以下任务：
                <ol>
                    <del><li>清理杯子、倒水</li></del>
                    <del><li>将值日表放到实验室门后</li></del>
                    <del><li>提交开题报告到研究生管理系统. 2025-01-14-16:55 周二：完成</li></del>
                    <del><li>为该项目撰写分析报告. 2025-01-14-20:38 周二, 已完成当前能完成的所有报告内容, 记录在 10.15.114.228 中/mnt/sda/zxt/3_code_area/code_develop/PartialConv_AesFA/ckpt/re-dh-white-main/readme_re-dh-white-main.html 中</li></del>
                    <li>更改 Partial_AesFA 中 部分卷积的代码, 为训练 <span class="color-huohuo-green">reparitial-dh-white-main</span>项目做准备.</li>
                    <del><li>查看自己的人文素质学分是否够, 如果不够就需要在 2025-01-15 上午 10:00 抢活动. 首先要找到自己记录人文素质学分的小册子……</li></del>
                </ol>
            </li>
            <li>审稿
                <ul>
                    <li>
                        Paper ID: 2396. <br>
                        <h3><strong>问卷填写</strong></h3>
                        <ol>
                        <li>
                        <p><strong>How confident are you in your evaluation of this paper?</strong><br>
                        <strong>选择</strong>：Very Confident<br>
                        <strong>理由</strong>：本文方法设计清晰，实验充分，结果具有显著优势，且用户研究支持其实际应用价值，因此对评估结果非常有信心。</p>
                        </li>
                        <li>
                        <p><strong>Importance/relevance to ICME</strong><br>
                        <strong>选择</strong>：Of broad interest<br>
                        <strong>理由</strong>：艺术字体生成是计算机视觉与图形学交叉领域的重要研究方向，本文提出的多模态引导与多层次控制框架具有广泛的应用前景，符合ICME的主题。</p>
                        </li>
                        <li>
                        <p><strong>Justification for importance/relevance</strong><br>
                        <strong>填写</strong>：本文提出的ArtTypo框架解决了艺术字体生成中的关键挑战，如艺术表达与可读性的平衡、非拉丁文字的支持以及精细控制问题。其多模态引导与迭代反馈机制为设计领域提供了新的工具，具有广泛的应用价值，如广告设计、品牌标识、多语言排版等。因此，本文的研究内容与ICME的广泛兴趣领域高度契合。</p>
                        </li>
                        <li>
                        <p><strong>Novelty/originality</strong><br>
                        <strong>选择</strong>：Very Original<br>
                        <strong>理由</strong>：本文提出了多模态意图提取、自动路径匹配与变换、背景保留的扩散过程等创新模块，结合迭代反馈机制，显著提升了艺术字体生成的灵活性与可控性，具有较高的原创性。</p>
                        </li>
                        <li>
                        <p><strong>Justification for novelty/originality</strong><br>
                        <strong>填写</strong>：本文的创新性体现在以下几个方面：</p>
                        <ul>
                        <li>提出<strong>多模态意图提取模块</strong>，结合Chain-of-Thought策略与多模态大语言模型（MLLMs），将抽象用户意图转化为具体设计提示，解决了现有方法灵活性不足的问题。</li>
                        <li><strong>自动路径匹配与变换模块</strong>通过向量路径提取与对齐技术，显著提升了非拉丁文字的可控性和一致性。</li>
                        <li><strong>背景保留的扩散过程</strong>与多掩码引导机制，实现了对局部纹理的精细控制，同时保持背景稳定。</li>
                        <li>迭代反馈模块（用户评分、标签反馈、自动化评估）的引入，使生成结果能够动态优化。这些创新点均为现有文献中未充分解决的问题，具有较高的原创性。</li>
                        </ul>
                        </li>
                        <li>
                        <p><strong>Technical correctness</strong><br>
                        <strong>选择</strong>：Definitely Correct<br>
                        <strong>理由</strong>：本文方法设计合理，实验验证充分，定量与定性结果均支持其技术正确性。</p>
                        </li>
                        <li>
                        <p><strong>Justification for technical correctness</strong><br>
                        <strong>填写</strong>：本文的技术正确性通过以下方面得到验证：</p>
                        <ul>
                        <li>定量实验在CLIP、DINO、OCR和Hausdorff距离等指标上全面对比了6种基线方法，数据充分证明了ArtTypo的优越性。</li>
                        <li>用户研究（30名评估者）与定性分析展示了ArtTypo在风格一致性、形状保真度等维度的显著优势。</li>
                        <li>消融实验验证了各模块的必要性，增强了方法设计的可信度。</li>
                        <li>生成效率较高（约3秒/图），具备实际部署潜力。因此，本文的技术设计与实验结果均表明其技术正确性。</li>
                        </ul>
                        </li>
                        <li>
                        <p><strong>Experimental validation and reproducibility</strong><br>
                        <strong>选择</strong>：Sufficient validation/theoretical paper<br>
                        <strong>理由</strong>：本文的实验设计充分，定量与定性分析全面，且提供了详细的实现细节，具备较高的可复现性。</p>
                        </li>
                        <li>
                        <p><strong>Justification for experimental validation and reproducibility</strong><br>
                        <strong>填写</strong>：本文的实验验证较为充分，具体体现在：</p>
                        <ul>
                        <li>定量实验在CLIP、DINO、OCR和Hausdorff距离等指标上全面对比了6种基线方法，数据充分证明了ArtTypo的优越性。</li>
                        <li>用户研究（30名评估者）与定性分析展示了ArtTypo在风格一致性、形状保真度等维度的显著优势。</li>
                        <li>消融实验验证了各模块的必要性，增强了方法设计的可信度。</li>
                        <li>提供了详细的实现细节（如随机增强、采样步骤、控制权重等），便于复现。因此，本文的实验验证充分且具备较高的可复现性。</li>
                        <li>但是实验多以中文、韩文、英文为对象, 缺少对其他非拉丁文字的说明(如泰语、蒙古语等)</li>
                        </ul>
                        </li>
                        <li>
                        <p><strong>Clarity of presentation</strong><br>
                        <strong>选择</strong>：Very Clear<br>
                        <strong>理由</strong>：论文逻辑清晰，图表设计直观，语言表达流畅，技术细节描述详细。</p>
                        </li>
                        <li>
                        <p><strong>Justification for clarity of presentation</strong><br>
                        <strong>填写</strong>：本文的写作结构清晰，逻辑连贯，图表（如框架图、对比示例）设计直观，增强了可读性。技术细节（如路径预处理公式、反馈机制）描述详细，便于读者理解。尽管部分技术描述（如路径预处理公式）可进一步简化，但整体表达非常清晰。</p>
                        </li>
                        <li>
                        <p><strong>Reference to prior work</strong><br>
                        <strong>选择</strong>：Excellent References<br>
                        <strong>理由</strong>：本文引用了大量相关文献，覆盖了多模态大语言模型、扩散模型、艺术字体生成等领域的最新研究，参考文献全面且相关。</p>
                        </li>
                        <li>
                        <p><strong>Justification for references</strong><br>
                        <strong>填写</strong>：本文引用了大量相关文献，覆盖了多模态大语言模型（如MLLMs）、扩散模型（如Stable Diffusion）、艺术字体生成（如FontDiffuser、AnyText）等领域的最新研究。参考文献全面且相关，能够有效支持本文的研究背景与技术贡献。尽管部分引用格式需统一（如[10]的会议信息缺失），但整体参考文献质量较高。</p>
                        </li>
                        <li>
                        <p><strong>Overall evaluation of the paper</strong><br>
                        <strong>选择</strong>：Strong Accept<br>
                        <strong>理由</strong>：本文在方法创新性、实验设计以及实际应用价值方面表现突出，具备较高的学术贡献和技术实用性，建议强接收。</p>
                        </li>
                        <li>
                        <p><strong>Justification for overall evaluation (required)</strong><br>
                        <strong>填写</strong>：本文提出了一种新颖的多模态引导、多层次控制框架ArtTypo，通过迭代反馈机制平衡艺术表达与可读性，并支持非拉丁文字。方法设计创新，实验验证充分，定量与定性结果均支持其技术优势。用户研究进一步证明了其实际应用价值。尽管存在少量细节需完善（如非拉丁文字的控制点预计算、反馈模块的权重调整机制），但整体贡献显著，建议强接收并推荐为会议亮点论文。</p>
                        </li>
                        </ol>
                    </li>
                    <li>
                        Paper ID: 2810.
                        <ol>
                        <li>How confident are you in your evaluation of this paper? *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        Confident</li>
                        <li>Importance/relevance to ICME *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        Of sufficient interest</li>
                        <li>Justification for importance/relevance *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)<br>
                        The paper addresses the important problem of inpainting damaged traditional Chinese paintings, which is relevant to the ICME community’s interest in cultural heritage preservation and multimedia restoration. The proposed method, TCSMAF, offers a novel approach to inpainting that could be valuable for both research and practical applications in the field.</li>
                        <li>Novelty/originality *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        Moderate Original</li>
                        <li>Justification for novelty/originality *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)<br>
                        The paper presents several novel contributions:<br>
                        Twin Cascade Architecture: The use of a symmetric multi-scale dual-branch architecture for parallel processing of structure and semantic features is a novel approach that could improve inpainting performance.<br>
                        Spatial Kernel Module (SKM): The integration of spatial coordinate encoding into the filtering process is an innovative way to enhance spatial perception and improve inpainting results.<br>
                        Multi-scale Spatial and Channel Attention (MSCA): The use of progressive convolution kernel sizes to leverage features across different scales and channels is a novel design that could improve texture reconstruction.</li>
                        <li>Technical correctness *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        Probably Correct</li>
                        <li>Justification for technical correctness *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)<br>
                        The paper provides a clear description of the proposed method and its components. The theoretical foundations and motivations behind each component are well explained. However, a more detailed analysis of the convergence properties and stability of the model would be beneficial.</li>
                        <li>Experimental validation and reproducibility *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        Limited but Convincing</li>
                        <li>Justification for experimental validation and reproducibility *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)<br>
                        The paper presents experiments on two datasets and compares TCSMAF with several state-of-the-art methods. The results show that TCSMAF achieves competitive performance in terms of quantitative metrics. However, the lack of ablation studies and a detailed analysis of the impact of each component on the overall performance makes it difficult to fully assess the effectiveness of the proposed method.</li>
                        <li>Clarity of presentation *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        Clear Enough</li>
                        <li>Justification for clarity of presentation *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)<br>
                        The paper is well-written and organized. The figures and tables are informative and help to illustrate the proposed method and its results. However, some parts of the paper could be improved for clarity, such as the description of the MSCA module and the loss function. In addition, Figure 2 lacks correspondence with the parameters in the text, which makes reading more difficult. </li>
                        <li>Reference to prior work *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        References Adequate</li>
                        <li>Justification for references *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)<br>
                        The paper includes a comprehensive list of relevant references, covering both image inpainting and spatial filtering techniques. However, some additional references related to traditional Chinese painting and cultural heritage preservation could be included.</li>
                        <li>Overall evaluation of the paper *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        Weak Accept</li>
                        <li>Justification for overall evaluation (required) *   (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)<br>
                        The paper presents a novel approach to inpainting damaged traditional Chinese paintings with promising results. However, the lack of detailed analysis of the proposed components and the limited experimental validation make it difficult to fully assess the effectiveness of the method. I recommend a weak acceptance of the paper with the following suggestions for improvement:<br>
                        Conduct ablation studies to analyze the impact of each component (SKM, MSCA, etc.) on the overall performance.<br>
                        Provide a more detailed analysis of the convergence properties and stability of the model.<br>
                        Include additional experiments on more diverse datasets and with different mask ratios.<br>
                        Improve the clarity of the description of the MSCA module and the loss function.<br>
                        Add references related to traditional Chinese painting and cultural heritage preservation.<br>
                        Additional Comments:<br>
                        The paper could benefit from a discussion of the limitations of the proposed method and potential future research directions.<br>
                        The authors should consider providing code or pre-trained models to facilitate reproducibility of the results.</li>
                        </ol>
                    </li>
                    <li>Paper ID: 2951
                        <ol>
                        <li>Confidence in Evaluation<br>
                        Confident: Although I have research experience in deep learning, I do not understand EEG analysis and neuroaesthetics. However, this article provides sufficient information to evaluate its methods and results.</li>
                        <li>Importance/Relevance to ICME<br>
                        Of sufficient interest: The paper presents a novel application of deep learning to EEG analysis in the context of art appreciation, which is a relevant topic for ICME as it explores the intersection of technology, neuroscience, and the arts.</li>
                        <li>Justification for Importance/Relevance<br>
                        The paper addresses a growing interest in understanding the neural mechanisms behind aesthetic experiences. It also demonstrates the potential of deep learning for EEG analysis in non-traditional applications, which could inspire further research in the field.</li>
                        <li>Novelty/Originality<br>
                        Moderate Original: The paper proposes a new model, ArtEEGAttention, which combines sliding window convolution and self-attention mechanisms for EEG classification. While these techniques are not novel individually, their application to this specific problem is innovative.</li>
                        <li>Justification for Novelty/Originality<br>
                        The paper clearly outlines the novelty of the proposed model and its potential advantages over existing methods. However, it would be beneficial to discuss the limitations of previous approaches in more detail to further highlight the novelty of the proposed method.</li>
                        <li>Technical Correctness<br>
                        Probably Correct: The paper provides a clear description of the model architecture and training process. However, the lack of details regarding the EEG data preprocessing steps and the specific implementation of the model raises some concerns about the technical correctness of the results.</li>
                        <li>Justification for Technical Correctness<br>
                        The paper would benefit from a more detailed description of the EEG data preprocessing steps, including filtering, artifact removal, and feature extraction. Additionally, providing the source code or a detailed description of the model implementation would increase the confidence in the technical correctness of the results.</li>
                        <li>Experimental Validation and Reproducibility<br>
                        Limited but Convincing: The paper presents results from five-fold cross-validation on a dataset of 16 subjects, which provides some evidence for the effectiveness of the proposed model. However, the limited sample size and the lack of details regarding the dataset and experimental setup raise concerns about the generalizability of the results.</li>
                        <li>Justification for Experimental Validation and Reproducibility<br>
                        The paper would benefit from a larger dataset and more diverse experimental conditions to evaluate the generalizability of the proposed model. Additionally, providing the dataset and code would enable other researchers to replicate the experiments and verify the results.</li>
                        <li>Clarity of Presentation<br>
                        Clear Enough: The paper is well-written and organized, with clear explanations of the methods and results. However, some sections could be improved for clarity, such as the description of the EEG data preprocessing steps and the discussion of the limitations of the study.</li>
                        <li>Justification for Clarity of Presentation<br>
                        The paper could be improved by providing a more detailed description of the EEG data preprocessing steps and the specific implementation of the model. Additionally, the discussion of the limitations of the study could be more thorough and critical.</li>
                        <li>Reference to Prior Work<br>
                        References Adequate: The paper provides a comprehensive overview of the relevant literature in neuroaesthetics and EEG deep learning. However, some additional references to recent advancements in these fields could be included.</li>
                        <li>Justification for References<br>
                        The paper would benefit from including references to more recent studies in neuroaesthetics and EEG deep learning, particularly those that address similar research questions or use similar methodologies.</li>
                        <li>Overall Evaluation of the Paper<br>
                        Borderline: The paper presents a novel application of deep learning to EEG analysis in the context of art appreciation, which is a relevant and interesting topic. However, the limited sample size, lack of details regarding the data preprocessing and model implementation, and concerns about the generalizability of the results raise some concerns.</li>
                        <li>Justification for Overall Evaluation<br>
                        The paper presents a promising approach for decoding EEG signals related to visual art appreciation. However, further research is needed to address the limitations of the study and evaluate the generalizability of the proposed model. The authors should consider conducting experiments with a larger dataset, including more diverse experimental conditions, and providing more details regarding the data preprocessing and model implementation. Additionally, exploring the interpretability of the model and its potential implications for understanding the neural mechanisms behind aesthetic experiences would be valuable.</li>
                        </ol>
                    </li>
                    <li>
                        Paper ID: 3176
                        <h3>Review Form for ICME 2025 Submission 3176: “SLME: Strokes Learning Model for Sketch-less Facial Image Retrieval”</h3>
                        <hr>
                        <h4>1. <strong>How confident are you in your evaluation of this paper?</strong></h4>
                        <p><strong>Confident</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>2. <strong>Importance/relevance to ICME</strong></h4>
                        <p><strong>Of sufficient interest</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>3. <strong>Justification for importance/relevance</strong></h4>
                        <p>The paper addresses a relevant problem in the field of sketch-based image retrieval, particularly in the context of facial recognition. The proposed method has practical applications in law enforcement and other domains where quick and accurate retrieval from incomplete sketches is crucial. While the focus is on facial images, the techniques could potentially be extended to other sketch-based retrieval tasks, making it of sufficient interest to the ICME community. However, the paper could benefit from a broader discussion on the generalizability of the approach to other domains, which would increase its relevance further.<br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)</p>
                        <hr>
                        <h4>4. <strong>Novelty/originality</strong></h4>
                        <p><strong>Moderate Original</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>5. <strong>Justification for novelty/originality</strong></h4>
                        <p>The paper introduces a novel Stroke Learning Model (SLME) that adaptively identifies informative sketch features from incremental stroke inputs, which is a significant contribution to the field of sketch-based facial image retrieval. The integration of textual descriptions to mitigate the noise introduced by sparse initial strokes is also a noteworthy innovation. However, the paper lacks a deeper theoretical justification for why the proposed Mixture-of-Experts (MoE) architecture and the integration of textual descriptions work better than other approaches. This limits the perceived originality of the work.<br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)</p>
                        <hr>
                        <h4>6. <strong>Technical correctness</strong></h4>
                        <p><strong>Probably Correct</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>7. <strong>Justification for technical correctness</strong></h4>
                        <p>The paper presents a technically sound approach, with a well-designed two-stage model (FAIP and SLME) and a robust experimental setup. The use of contrastive learning and multiple loss functions (ITC, ISC, ITSC) demonstrates a solid technical foundation. However, the lack of theoretical analysis and the absence of a discussion on computational cost and scalability raise some concerns about the technical depth of the paper. Overall, the methodology appears to be correct, but further validation and theoretical grounding would strengthen the paper.<br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)</p>
                        <hr>
                        <h4>8. <strong>Experimental validation and reproducibility</strong></h4>
                        <p><strong>Sufficient validation/theoretical paper</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>9. <strong>Justification for experimental validation and reproducibility</strong></h4>
                        <p>The paper provides extensive experimental validation, including comparisons with baseline methods and ablation studies, which demonstrate the effectiveness of the proposed approach. The results are convincing, particularly in the early stages of sketch retrieval. However, the paper could improve by providing more details on the reproducibility of the experiments, such as hyperparameter settings, dataset splits, and computational resources used. This would enhance the reproducibility of the work.<br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)</p>
                        <hr>
                        <h4>10. <strong>Clarity of presentation</strong></h4>
                        <p><strong>Clear Enough</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>11. <strong>Justification for clarity of presentation</strong></h4>
                        <p>The paper is generally well-written and organized, with a clear explanation of the proposed method and its components. However, some sections, particularly the technical details of the MoE architecture and the integration of textual descriptions, could be explained more clearly. Additionally, the paper would benefit from a more detailed discussion of the limitations and potential future work. Overall, the presentation is clear enough, but there is room for improvement in terms of clarity and depth.<br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)</p>
                        <hr>
                        <h4>12. <strong>Reference to prior work</strong></h4>
                        <p><strong>References Adequate</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>13. <strong>Justification for references</strong></h4>
                        <p>The paper adequately references prior work in the field of sketch-based image retrieval and multimodal learning. However, it could benefit from a more thorough discussion of related work, particularly in the context of Mixture-of-Experts (MoE) architectures and their applications in other domains. This would provide a more comprehensive background for the proposed method and highlight its novelty more effectively.<br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers, visible to senior meta-reviewers)</p>
                        <hr>
                        <h4>14. <strong>Overall evaluation of the paper</strong></h4>
                        <p><strong>Borderline</strong><br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <h4>15. <strong>Justification for overall evaluation (required)</strong></h4>
                        <p>The paper presents a novel and technically sound approach to improving sketch-based facial image retrieval, particularly in the early stages of sketching. The experimental results are strong, and the integration of textual descriptions is a notable contribution. However, the lack of theoretical justification, limited discussion on generalization, and absence of computational cost analysis are significant weaknesses. Given these considerations, I recommend this paper as a <strong>borderline accept</strong>. With additional theoretical analysis and a broader discussion on generalization and computational cost, the paper could be strengthened for acceptance.<br>
                        (visible to authors during feedback, visible to authors after notification, visible to other reviewers, visible to meta-reviewers)</p>
                        <hr>
                        <p>This review provides a balanced assessment of the paper’s strengths and weaknesses, with constructive suggestions for improvement.</p>
                    </li>
                </ul>
            </li>
        </ol>
        <h2>兴趣</h2>
        <h2>工作</h2>
        <h2>生活</h2>

        <h1>今日趣事</h1>
        <hr>
    </div>
</body>
</html>