
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Diary</title>
    <link rel="stylesheet" href="/Template/styles.css">
    <script id="MathJax-script" async src="/Template/js/tex-mml-chtml.js"></script>
</head>
<body>
    <div class="container">
        <h1>今日生活</h1>
        <table id="table-blue">
            <thead>
                <tr>
                    <th>日期</th>
                    <th>昨天睡觉时间</th>
                    <th>今天起床时间</th>
                    <th>今日体重</th>
                    <th>今日锻炼</th>
                    <th>昨日资金剩余(支付宝)</th>
                    <th>昨日资金剩余(微信)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>0</td>
                    <td>1</td>
                    <td>2</td>
                    <td>3</td>
                    <td>4</td>
                    <td>5</td>
                    <td>6</td>
                </tr>
            </tbody>
        </table>

        <h1>新年计划</h1>
        <span class="color-DrRatio-blue">“要么立刻行动, 要么一事无成.” </span> -- 真理医生\(\cdot\)星野<br>
        <span class="color-huohuo-green">“太大的目标容易吓到自己……所以要把大目标化成多个小目标哦……”</span>--藿藿\(\cdot\)星野
        <hr>
        <h1>近日安排</h1>

        <hr>
        <h1>今日计划</h1>
        <h2>科研</h2>
        <h2>兴趣</h2>
        <h2>工作</h2>
        <ol>
            <li>
                完成周报: 总结 天津大学 "人工智能实践能力提升行动(Deepseek专题)", 完成周报.
                <ol>
                    <li>
                        DeepSeek: 从原理到模型<br>
                        &ensp;&ensp;主讲人：熊德意，国家重点研发计划项目首席科学家，天津大学智能与计算学部教授，自然语言处 理实验室负责人，天津市“一带一路”联合实验室语言智能与技术中外联合研究中心主任，获天 津市引进领军人才资助。主要研究方向为自然语言处理，特别专注于大语言模型、机器翻译、AI 对齐与安全、AlforScience等方向的研究。<br>
                        &ensp;&ensp;讲座的第一部分为 大语言模型的发展路线, 介绍 DeepSeek 出现的行业背景.<br>
                        &ensp;&ensp;首先介绍了 2014-2024 年生成式AI的发展与背景. 介绍了注意力机制(2014年), 并与人的注意力机制结合; 随后介绍了 2017 年出现的 Transformer 模型, 并说明了是 OpenAI 将 Transformer 模型推广开的. 再次介绍了 2020 年的 Scaling Laws, 说明了模型与数据之间的关系, 通过这种关系可以预测模型在下游任务的性能. 在 Scaling Laws 的指导下, AI 生成的数据已经足够优秀, 那么如何控制生成的数据与人的价值观相符合? 在 2022 年, Chatgpt 出现前夕, 使用了 人类反馈强化学习(RLHF)的方式训练AI, 以确保AI生成的数据能与人类价值对齐. 而现在, 我们希望 AI 能解决人类的所有问题, 希望 AI 能通过学到的知识求解现实的问题, 这就是 ChatGPT o1 与 DeepSeek R1 所研究的问题.<br>
                        &ensp;&ensp;其次介绍了什么是"大模型", 所谓的大模型实际上是 "大语言模型" 的简写. 熊老师以 人脑中的语言组织能力与语感为类比, 介绍了大语言模型, 并强调了大语言模型的职能: 利用已生成的语句推测下一个可能的语句. 从发展调度看, 大语言模型于 2018年 左右开始大规模发展. <br>
                        &ensp;&ensp;从技术栈角度看, 大语言模型由高到低包括 应用层、行业模型、通用模型、数据处理与管理、算力管理几个层次. 在有了一定算力的基础上, 如何管理算力与数据? 如何使用算力与数据训练通用模型? 如何将通用模型专业化为一个特有领域的模型? 最后如何将这个模型推出到市场上使用? 这些问题都是在部署一个商业大模型时需要考虑的问题. <br>
                        &ensp;&ensp;从生命周期的角度看, 大模型存在 4 个关键环节: 1. 数据处理, 即如何处理大量数据的环节、2. 模型预训练环节, 即如何使用数据训练模型的环节, 该环节将生成一个基座模型、3. 模型后处理环节, 该环节中需要注意如何将基座模型与人类的价值观统一, 或使基座模型能完成我们希望它能完成的任务、4. 模型应用部署. 而在2、3两个训练环节中, 熊老师总结了一个包含三项的训练范式. 熊老师认为, 大语言模型的训练需要包含三个阶段: 预训练阶段, 以生成基座模型、后训练阶段, 以生成对齐模型、推理训练阶段, 以生成推理模型. 除了训练外, 模型架构与扩展法则也是大语言模型的关键. 模型架构决定了模型能学习到什么信息, 从而间接决定了模型能力的强弱. 而扩展法则决定模型学习能力的上限, 如何扩展这样的上限. 在将大模型应用到商业界的核心则是 大语言模型的性能价格比, 这也是 DeepSeek 生成的重要原因.<br>
                        &ensp;&ensp;随后熊老师从 大语言模型 切入到 通用人工智能模型: "我们希望人工智能模型能获取各个学科的知识, 能与人类聊天, 这些都已经做到了, 但我们更希望利用大模型的这些知识解决各种问题." 而这个问题涉及到逻辑推理. 有些人希望能介入大模型的推理逻辑链, 通过强化学习等方式强化大模型的推理链, 以增强模型在这方面的能力.<br>
                        &ensp;&ensp;讲座的第二部分为 DeepSeek V2-V3/R1 的技术原理.<br>
                        &ensp;&ensp;回顾第一部分的背景, 可以发现有两个关键：1. 模型架构问题. 模型架构决定了模型的学习能力. 许多公司在研究这个问题, 但不敢做过多的创新, 因为这种行为具有较高的试错成本. 2. 推理模型问题. 大部分实验室都不能做出较好的推理模型, 他们认为 OpenAI 的 Q* 模型是一个这样的推理模型, 并不断进行猜测. 而 DeepSeek 正好解决了这两个问题. DeepSeek 在 2023 年成立后, 推出了第一个模型 V1, 该模型主要复刻其他先进模型, 并无过多创新, 所以熊老师没有进行过多的介绍. 真正的创新在 V2 模型中. V2 模型解决了一个问题: Transformer模型如果想处理更多的数据, 那么就需要更大的参数量, 且每次计算时都需要激活所有的参数. 所有的参数都将参与计算, 计算的成本也随模型的扩大而增加. 而 V2 使用了 细粒度专家模型(MoE), 使得模型的训练或推理时能仅激活部分参数, 而非全部的参数, 这样就确保了计算成本不再随着模型的增大而增加, 所以降低了成本. 除此之外, DeepSeek 还对 MoE 进行了优化, 增加了不同专家系统之间的通信的带宽, 加速了计算速度. DeepSeek 还对矩阵进行了优化. 随着数据的增加, Transformer 生成的矩阵也会越来越大, 大到无法放到内存中计算. 为了解决这个问题, DeepSeek 团队提出了使用矩阵的 秩分解, 将一个大矩阵分解为多个小的矩阵, 在运算时再将这些小矩阵组合起来, 实现了计算与训练的加速. 总而言之, 这些操作都是为了降低训练成本. 而降低训练成本就是在企业中的杀手锏, 而 DeepSeek 在 V2 时就 "尝到了这个甜头".<br>
                        &ensp;&ensp;V3 模型与 V2 模型相比, 首先是能预测 token 数量上的差距. V2时期, DS仅仅能单个单个 token 的预测, 而在 V3 时期, DS 能一次性预测多个 token, 但这不是 DS V3 的原创. 熊老师认为, V3 真正的创新在于 模型与底层硬件的优化, 在 Infrustructures 上, 一些关键问题都通过算法的改进解决了. 如我们在进行分布式任务时, 希望所有进程能同时运行, 而实际上, 由于先后顺序的存在, 有些进程无法同时运行, 这种现象被称为 "气泡", DS 就降低了这种气泡的存在. 再如上一段中所说的 "矩阵秩分解", 配合不同精度的数据混合利用等多种方法, 降低了训练的成本. 在 Infrustructure 中, 还存在大量这样的操作, 降低了成本. 通过 V2 与 V3 的发展, DS 在训练时的花费, 已经达到了较高的性价比: 训练一轮仅需要 2 个月以及 600 万美元, 这引起了国外同行的恐慌. 但熊老师又补充到, 这仅仅是训练的成本, 而模型优化的成本、器材采购的成本、用人成本等非训练成本并未包含在其中.<br>
                        &ensp;&ensp;DS 在 V2、V3上的创新, 尽管其中部分可能不是 DS 首先提出的, 但在具有很高试错成本的前提下, 将这些技术集成的这么好, 也是值得赞赏的, 熊老师夸奖道. 这些创新都围绕一个核心, 如何"降本增效": 在不损害基本性能的前提下, 尽可能通过算法挖掘和提升硬件训练和解码效率. 在美国采取芯片禁令的大背景下, DS 通过绕过美国这种算力 "护城河"的方式, 做出了与 OpenAI 具有相近性能的大模型, 为国内 AI 发展做出的巨大贡献, 让全世界看见, 随着技术的发展, 算力瓶颈可能并不是一个瓶颈. 这对美国的芯片禁令造成了沉重的打击.<br>
                        &ensp; &ensp; 而 DeepSeek R1 Zero 使用了大规模的强化学习（Reinforcement Learning, RL）技术, 并发现了 RL 领域的 Scaling Law. 业内通常进行几十次的 RL 训练, 而 DS 进行了 几千 次的 RL 训练. 业内少量的 RL 训练导致了 RL 的 Scaling Law 并为被发现. DS R1 Zero 可以说是首先发现了 Scaling Law 在 RL领域 也存在, 即随着思考时间的增加, 生成的 token 也会越来越优秀. 这种以 RL 为核心的训练方式, 进一步降低了成本, 提升了性价比. DS R1 在 OpenAI o1的基础上, 独立探索出了基于大规模强化学习的大语言模型推理技术路线. 这种方法避开了过去一年多由OpenAI 在社交媒体引导的 Q* 方法, 即目前被业内广泛实验的"在训练中显式搜索、过程奖励模型" 时间推理模型构建的误区. 这种独立探索出推理模型构建技术路径, 并将该技术路径公开, 结合模型开源的组合, 打破了美国企业以闭源形成的技术护城河, 动摇了美国的AI霸权.<br>
                        &ensp; &ensp; 在第三部分中, 熊老师介绍了为何 DS 会对世界产生冲击. 首先, 在算力价格战方面, DS 以更低的价格实现了与 OpenAI 类似甚至超过的性能, 这与 OpenAI 宣称的 大模型需要高算力不匹配, 使世界重新思考算力在大模型中的作用. 其次, DS 将技术开源, 是首个将开源技术推进到与闭源技术类似甚至超过的大模型, 挥发了开源的精神. 再次, DS 颠覆了 ChatGPT 构造的 AI 认知. 长久以来, 美国认为中国在AI科技创新上更多是跟随者的角色, 但是 DS 打破了这一认知. 同时, 业内认知中, 大模型研发成本需要千万乃至上亿美元, 而 DS 以更低成本实现了类似的效能, 打破了这一认知.<br>
                        &ensp; &ensp; 最后, 熊老师畅想未来, 从技术角度考虑, 认为人类所有职业实现AI自动化仅需要 30 年, 并希望 AI 领域发展越来越好.<br>
                    </li>
                </ol>
            </li>
        </ol>
        <h2>生活</h2>

        <h1>今日趣事</h1>
        <hr>
    </div>
</body>
</html>