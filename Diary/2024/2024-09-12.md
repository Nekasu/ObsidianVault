# 今日生活

| 昨天睡觉时间 | 今早起床时间 | 今日体重 | 今日锻炼 | 剩余资金(支) | 剩余资金(微) |
| ------ | ------ | ---- | ---- | ------- | ------- |
|        |        |      |      |         |         |

1. 昨天睡觉时间：
2. 今早起床时间：
3. 今日体重：

# 新年计划

![[2024-01-01#新的一年]]

# 近日安排

# 今日计划

1. [ ] 看看<mark style="background: #BBFABBA6;">待研究</mark>与<mark style="background: #BBFABBA6;">待整理</mark>,  <mark style="background: #BBFABBA6;">近日安排</mark>标签

## 科研

1. [ ] 现在神经网络拥有什么组件, 各个组件的功能是什么, 具有什么样的效果. 记录在[[神经网络组件目录文件_README]] 中
2. [ ] 按代理人要求, 修改专利初稿
3. [ ] 阅读一些 2023 与 2024 年风格迁移新文章, 并记录在 [[2024-09-12#其他今日所学]]中

## 学业

1. [ ] 
2. [ ] 
3. [ ] 
4. [ ] 

## 兴趣

1. [ ] 
2. [ ] 
3. [ ] 

## 工作

1. [ ] 

# 其他今日所学

## 2024 风格迁移文章

### DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models

本文以风格迁移、文生图辅助艺术创作为基础, 考虑到风格信息无法使用文字描述清楚, 导致文生图时无法获得准确且高质量的图像. 

为了解决以上问题, 本文将风格迁移与基于 Stable Diffusion 的文生图任务结合, 提出了 DreamStyler 框架. 该框架能够将图像中的风格信息提取到 CLIP 文本空间中. 为了能将文本描述与 Stalbe Diffusion 结合, 作者基于文本反转 (Textual Inversion, TI)提出了扩展文本嵌入空间的想法. 该想法将扩散模型的时间步分为多个组, 每组称为一个时间步块 (A Chunk of Timesteps). 称一个时间步块与对应文本描述的组合为一个文本反转阶段 (TI Stage). 通过组合多个文本反转阶段的方式, Stable Diffusion 能够在图像合成的不同时间步块中理解相似但有区别的风格描述嵌入. 这被作者称为多阶段文本反转 (Muilti-Stage Textual Inversion). 此外, 作者等人还提出了一种上下文感知的提示增强，可以将风格图像中的将风格和上下文信息解耦合. 在解耦合后, 风格信息可以被编码为特殊的文本嵌入, 提供更为准确的风格描述词供多阶段文本反转使用.

### Regional Style and Color Transfer

本文是一篇水刊的文章, 但是具有一定的参考价值, 所以在此记录

本文认为, 现有方法通常存在在整个图像上均匀应用样式的缺点，当应用于具有前景元素（例如人物）的图像时，会导致样式不一致或前景对象扭曲. 为了解决这个限制，作者等人提出了一种新方法，利用语义分割网络来精确隔离输入图像中的前景对象。随后，风格迁移仅应用于背景区域, 而对前景区域使用颜色迁移技术, 以确保前景对象不存在扭曲现象。最后，将孤立的前景对象仔细地重新整合到风格转移的背景中。

#### 肖桐的话

虽然这篇文章显的很简单, 但是对我的工作有一定的指导意义, 具有一些学习的点, 在此记录

1. Boundary Optimization 图像分割后的边界优化
	1. 问题描述：在图像分割过程中, 分割出的前景图像可能具有一定的伪影, 本文通过某些方法解决了这样的伪影问题
	2. 问题类比：与本人的工作对比, 在深度估计过程中, 分割出的主体图像可能具有一定的伪影, 我也需要通过某种方法解决这样的伪影问题
	3. 要不结合深度估计与图像分割……
2. Blending Style and Color 前景与背景分别风格化后的合并
	1. 问题描述：将前景图像与背景图像合并时, 可能出现一定的锐利边缘, 本文通过某些方法弱化了这种不和谐的边界问题
	2. 解决方法：使用 alpha 混合技术
	3. 以下是 ChatGPT 解释 Alpha Blending 的话语：
		1. [[10_Alpha_Blending]]
	4. 

### Rethink arbitrary style transfer with transformer and contrastive learning

本文是一篇基于 Transformer 的风格迁移文章, 结合对比学习, 以求生成具有精细效果的风格迁移结果.

- 关键词：SCIN、对比学习、VGG 改进、Transformer
	- SCIN 用于解决：
	- 对比学习用于解决
	- 用于改进 VGG 的 Transformer 用于解决

本文认为, 当前类似于 AdaIN 的方法具有一些缺陷. 在生成高质量风格化图像时，容易出现内容与风格不匹配、图像伪影以及风格特征提取不准确等问题。造成该现象的原因有 2 个：使用 VGG 网络进行特征提取、在实例归一化时, 使用简单的固定参数进行调整,. 

对于第一个问题, 文章认为 VGG 是一个用于图像分类的网络, 该网络在用于风格迁移时, 会过多的关注不需要的图像分类信息. 为了解决这个问题, 文章提出了一个基于 Transformer 的风格图像特征提取器, 并称之为感知编码器 (Perception Encoder, PE). PE 通过捕捉风格图像的长程依赖信息和高频风格细节，避免了 VGG 只专注于显著分类特征（如边缘或形状）的局限性，从而更准确地提取风格信息，减少伪影并提升风格迁移的整体效果

对于第二个问题, 文章提出了风格一致性实例归一化 (Style Consistency Instance Normalization, SCIN). SCIN 与 AdaIN 仅根据风格特征的均值和方差进行简单对齐不同，SCIN 使用 Transformer 捕捉风格特征图中的长程、非局部依赖关系，提供更丰富的风格信息。此外，SCIN 生成的缩放和平移参数是通过学习得到的，能够更好地适应不同风格的图像分布，而不仅仅依赖于固定的统计特征（如均值和方差）。这一改进使得 SCIN 在风格与内容特征对齐时更加灵活和准确，减少了伪影并提升了风格化图像的质量​。

为了进一步提升风格迁移结果质量, 使不同风格的图像具有较大区分度, 文章还提出了基于实例的对比学习 (Instance-based Contractive Learning, ICL). ICL帮助模型学习风格化图像之间的关系，确保相同内容或风格的图像嵌入更接近，不同风格的图像则远离，从而增强了风格化图像的质量。

### Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer

本文着眼于扩散模型在进行风格迁移工作时, 需要大量时间进行推理的问题. 为了解决这个问题, 本文提出了一种免训练的调整大型扩散模型的方法. 对于这种方法其中出现的两个问题--内容中断与颜色错误, 本文提出了“查询保存”与“注意力温度缩放”以解决前者, 并使用了 AdaIN 以解决后者.

尽管有前人有类似的免训练方法, 但前人的免训练方法无法应用到大型扩散模型上, 这也是本文需要解决的问题之一, 即将免训练的方法推广到大型扩散模型上. 为了推广免训练的方法, 本文翻阅了使用大型扩散模型进行图像翻译的工作, 并发现两个特点：1. 注意力图决定空间布局; 2. 调整交叉注意力机制中的 Query 与 Key 可以调整生成图像中填充的内容. 基于上述特点, 文章提出了一种不用重新训练扩散模型便能完成风格迁移的成果, 其基本思想是用风格图像自注意力图中 Query 与 Key 代替内容图像自注意力图中的 Query 与 Key. 这样做的优势有两个. 首先, 如果内容图像的 query(即内容信息)如果具有相似的语义，它们会使用相似的 key(即风格信息)。这样，在风格迁移之后，内容图像 Query 之间的关系能够得到保持。其次, 在迁移结果中, 每个内容图像的 query 都可以表现出与具有相似纹理和语义的 key 之间的高度相似性。

### Z*: Zero-shot Style Transfer via Attention Reweighting

本文发现：基础的扩散模型可以直接用于提取风格信息. 

在现有的风格迁移研究中，通常使用编码器将风格图像编码为文本特征，然后利用这些特征来指导 Stable Diffusion 模型生成带有相应风格的图像。然而，部分研究指出这种基于文本的特征描述并不准确，且生成结果往往无法很好地捕捉图像的细节风格特征​。为了克服这一问题，有研究者提出，扩散模型中常用的 U-Net 结构可以直接用于提取风格信息，避免依赖文本嵌入。这为无需文本约束条件下的风格迁移提供了新的思路​。

本文提出了一种基于Stable Diffusion的双路径去噪模型来实现风格迁移。该模型包括两个独立且相同的扩散模型，分别用于处理内容和风格图像，均采用U-Net作为核心网络。为了便于描述，我们将这两个扩散模型称为风格扩散模型和内容扩散模型。风格扩散模型旨在通过T步扩散过程，从噪声图像逐步还原出原始风格图像；同样，内容扩散模型则试图还原内容图像​。

当扩散模型运行到任一时间步 $t \in [0, T]$ 时，U-Net 从内容扩散路径中提取的内容特征图 $X^c_t$ 与从风格扩散路径中提取的风格特征图 $X^s_t$ 结合，通过一种基于交叉注意力的机制生成风格化的潜在特征图 $\hat f_c$。然后，这些潜在特征图通过逆扩散过程生成最终的风格化图像。与传统的基于 Gram 矩阵计算风格特征的方法相比，该方法能够更好地保留内容图像的结构和细节​。

这一融合机制被称为“交叉注意力重构”，核心思想是将内容图像中的不同像素点视为查询(Query)，与风格图像中的特征（Key）进行关联。由于内容像素与风格信息的相关度可能不同，这种差异会影响风格迁移的效果。部分内容像素可能对风格信息的贡献较大，导致这些区域在风格化结果中表现得不够自然，甚至影响内容的保真度。因此，作者提出对这些低相关度的内容像素进行加权抑制，以减少它们在风格化潜在特征图$\hat f_c$中的影响​。

然而，由于Softmax函数的特性，低相关度的像素点（即$QK^T$值较小的点）在经过Softmax处理后，可能会获得相对较大的注意力权重，导致不理想的风格迁移效果。为了应对这一问题，本文提出了一种重加权的交叉注意力机制，通过在Softmax函数中引入一个可调整的权重参数$\lambda$，动态调节不同像素的注意力权重。这样既能避免低相关度像素的影响，又能有效增强与风格图像相关性较强的像素的表现​。

综上所述，该研究展示了一种无需文本嵌入、基于扩散模型的全新风格迁移方法，并通过交叉注意力重构与注意力加权策略，实现了内容和风格的平衡结合，在风格化结果中既保留了内容细节，又能精准地呈现风格特征​。

# 今日趣事



# 今日口诀


