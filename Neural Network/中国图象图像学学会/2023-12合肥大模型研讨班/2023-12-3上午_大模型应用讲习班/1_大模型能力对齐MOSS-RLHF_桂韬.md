# 大模型训练路线



1. 预训练阶段
	1. 使用原始数据训练一个语言模型的能力
2. 指令微调
	1. 经过指令微调后, 才能使机器能够理解人类的语言
3. 伦理道德问题
4. 通过「奖励函数」与「强化学习」实现将伦理道德融入模型中


# 大模型伦理的一些问题

## 预训练前的数据清洗

1. 网页数据上存在大量的「不真实数据」以及「毒性数据」
	1. 如果用这些数据进行训练, 则会得到一些错误的、甚至违背常识的回答

## 大模型的偏见问题

1. 如果一个模型接受了太多网络上的信息, 那么该模型可能与网络上一样, 对某些群体产生“偏见”
	1. 如让大模型进行以下的完形填空：“一个穆斯林走进了\_\_\_\_\_\_”
		1. 大模型的5次回答中有4次是带有侮辱性的
	2. 但是如果问“一个佛教徒走进了\_\_\_\_”
		1. 则回答可能并不带有侮辱性

## 大模型的隐私问题

- 网络上的一些数据可能带有隐私, 不应使用这些数据进行训练
- 同时向大模型的提问中, 不免会包含个人信息, 这些个人信息将以明文的形式在网络上传播
	- ![[Pasted image 20231203092413.png]]


## 大模型安全伦理的几个方面

1. 避免偏见
2. 隐私保护
3. 信息安全

因此需要在“奖励函数”以及“强化学习”阶段, 需要考虑以下几个问题
1. 排序数据的方案
2. 奖励模型的方案
3. 强化学习的方案

基于此, 应该有以下的大模型安全理论标准
1. 有效的回复--回复需要有意义、真实可信
	1. 能够对于问题进行相关的跟进与提问
	2. 对有问题的提问进行分析, 重新理解这些提问
2. 诚实的回复--知道自己应该做什么, 应该不做什么
3. 无害的回复--不能够有歧视, 不能伤害他人

同时, 以上三个标准存在一定的博弈性
1. 如果重视诚实的回复, 那么可能会伤害一些人的情感
2. …………
所以需要在数据集中上述三个部分对应的数据进行一定的比例调整


# 研究目标

1. 构建适配中文环境的奖励模型数据集
	1.  模型对齐基础
2. 设计指导PRO算法成功训练“指标”
3. 设计适合大模型的PRO-MAX算法（模型对齐核心）
4. 在大模型应用场景的实验验（模型对齐落地）

