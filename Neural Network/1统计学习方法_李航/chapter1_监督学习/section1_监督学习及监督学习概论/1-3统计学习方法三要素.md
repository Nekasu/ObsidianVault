 # 模型

模型是统计学习最终的学习的结果, 这些结果(模型)可以是*条件概率分布*$P(Y|X)$或者是*决策函数*$f(X)$*. 

包含了所有可能学习到的条件概率分布于决策函数的集合被称作`假设空间`. 举例来说, 假设我们需要学习的是一个线性的决策函数, 那么模型的假设空间就是所有线性函数构成的集合, 数量一般是无穷多个

## 决策函数

如果统计学习的结果是一个决策函数, 那么假设空间$\mathcal{F}$就是一个由决策函数$f(X)$构成的集合, 如下所示

$$
\mathcal{F} = \{f|Y=f(X)\}
$$
$X$表示定义在输入空间$\mathcal{X}$上的变量, $Y$表示定义在输出空间$\mathcal{Y}$上的变量. 
- 值得注意的是, $X$于$Y$都是<mark style="background: #FF5582A6;">真实的数据</mark>, 并不是利用模型进行预测的结果

对上述集合进行进一步的约束, 如果有一个$n$维向量$\theta$作为函数的约束, 即可得到如下集合
$$\mathcal{F}=\{f|Y=f_{\theta}(X), \theta \in R^n\}$$
向量$\theta$属于$n$维[[附录_名词解释#欧氏空间|欧氏空间]]$R^n$, 表示的是函数中的参数

## 条件概率

统计学习的结果也可以是一个*条件概率分布*, 这些*条件概率分布*组成的集合(也即假设空间)可以用如下形式表示
$$
\mathcal{F} = \{P|P(Y|X)\}
$$
$X$表示定义在输入空间$\mathcal{X}$上的变量, $Y$表示定义在输出空间$\mathcal{Y}$上的变量, 学习的结果就是一个**在输入为$X$的条件下, 输出为$Y$的概率**
- 值得注意的是, $X$于$Y$都是<mark style="background: #FF5582A6;">真实的数据</mark>, 并不是利用模型进行预测的结果

同样的, 对上述集合进一步约束, 可以得到一个由参数向量决定的集合:
$$\mathcal{F}=\{P|P_\theta(Y|X), \theta\in R^n \}$$
向量$\theta$属于$n$维[[附录_名词解释#欧氏空间|欧氏空间]]$R^n$, 表示的是函数中的参数

# 策略

## 损失函数与风险函数

损失函数(loss function)用于度量`一次预测`的好坏, 风险函数量`平均意义下`模型预测的好坏

### 常见的损失函数

监督学习的问题是在[[附录_名词解释#^2d755f|假设空间]]中寻找一个模型$f$作为决策函数, 这个模型的作用如下
$$Y_p=f(X)$$
给定一个输入$X$, 该模型应该能给出一个预测$Y_p$, 损失函数$L(Y_t,f(X))$用于度量`预测`$Y_p$也即$f(X)$与真实值$Y_t$之间的`差别程度`$L(Y_t,f(X))$, 以下是一些常见的损失函数

#### 1. 0-1损失函数

$$
\begin{aligned}
\begin{equation}
	L\left( Y_t,f(X) \right) = \begin{cases}
		1,& f(X) \ne Y_t\\
		0,& f(X) =Y_t
	\end{cases}
\end{equation}
\end{aligned}
$$

#### 2. 平方损失函数

$$
L(Y_t,f(X)) = (Y_t-f(X))^2
$$

#### 3. 绝对损失函数

$$
L(Y_t,f(X))=\left | Y_t-f(X) \right |
$$

#### 4. 对数损失函数/对数似然损失函数

$$
L(Y_t,P(Y_t|X))= - \log P(Y_t|X)
$$


### 期望损失(风险函数)

在介绍期望损失之前, 需要先复习一下二维连续型随机变量的函数的分布的定义. 

在概率论的知识中, 我们知道`二维连续型随机变量`的`数学期望`的定义如下

如果随机变量 $𝑋$ 和 $𝑌$ 是连续型随机变量, 且具有联合概率分布 $f(x,y)$, 则 与$x,y$相关的随机变量$z=\varphi(x,y)$ 具有数学期望
$$
E(z)=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} \varphi(x,y)f(x,y)dxdy
$$

如果将$x$看作监督学习中的输入$X$; 

将$y$看作统计学习的真实数据$Y_t$; 

则与$x,y$有关的函数关系$\varphi(x,y)$可被看作看作与输入$X$和真实数据$Y_t$有关的`损失函数`$L(Y_t,f(X))$;

基于[[附录_名词解释#统计学习的基本假设|统计学习的基本假设]], 输入$X$和输出$Y_p$具有联合概率分布$P(X,Y_p)$

那么损失函数$L(Y_t,Y_p)$的数学期望如下
$$
R_{exp}=E(L) = \int_{\mathcal{X}\times\mathcal{Y}} L(Y_t,Y_p)P(Y_t,X)dY_tdX
$$
该公式中, $\mathcal{X}$为输入的空间, $\mathcal{Y}$为监督学习标注的数据的空间, $Y_t$为真实数据(也即$\mathcal{Y}$中的元素), $Y_p$为模型预测的结果. 统计学习的最终目标就是习得一个`输入`$X$与`真实数据`$Y_t$之间的[[附录_名词解释#联合分布率P(X,Y_t)|联合概率分布]]

由于$Y_p=f(X)$, 上述公式也可以写成
$$
R_{exp}=E(L)=\int_{\mathcal{X}\times\mathcal{Y}} L(Y_t,f(X))P(Y_t,X)dY_tdX
$$
该公式为理论上模型$f(X)$的数学期望, 也即平均意义上的损失, 也即`风险函数`或`期望损失`(Expected Loss). 学习的目标就是选择`期望损失`最小的模型

之所以称之为期望损失(期望中可以算出来的损失值), 是因为该公式是不可计算的. 由于联合概率分布$P(Y_t,X)$是学习的结果, 所以在学习前是未知的. 如果已知$P(Y_t,X)$, 就可以通过`贝叶斯公式`直接计算出条件概率分布$P(Y|X)$, 也即学习的结果了, 从而也就无需学习. 因此想要通过计算`期望损失`判断模型的好坏是一个无法完成的任务.

### 经验风险

*经验风险*$R_{emp}$的解释如下.

给定一个训练数据集
$$
T=\{(x_1,y_1),(x_2,y_2),\cdots, (x_N,y_N)\}
$$
模型$f(X)$关于上述训练集$T$的损失函数的平均值被称作*经验风险*(empirical tisk)或*经验损失*(empirical loss), 记作$R_{emp}$, 利用计算平均数的方法, 可以得到*经验损失*的公式:
$$ 
R_{emp} = \frac1N \sum^{N}_{i=1} L(y_i. f(x_i))
$$

根据大数定律, 当$N\to\infty$(也即样本量趋于无穷)时, 经验风险$R_{emp}$无限趋近于$R_{exp}$. **同时**, 由于训练样本数$N$在实际中十分有限, **所以**利用$R_{emp}$估计$R_{exp}$在实际中常常不理想, **因此**需要对经验损失$R_{emp}$进行一定的`矫正`

在进行`矫正`时, 涉及到了监督学习的两个基本策略: `经验风险最小化`与`结构风险最小化`. 

## 经验风险最小化与结构风险最小化

### 经验风险最小化

在`假设空间`$\mathcal{F}$,`损失函数`$L(Y_t,f(X))$,`数据集`$T$确定的情况下, [[1-3统计学习方法三要素#经验风险|经验损失函数]]也能随之确定.

经验风险最小化(empirical risk minimization, ERM)策略认为, `经验损失函数`的最小化就是求解问题的最优解, 如下公式所示
$$
\min_{f\in \mathcal{F}} \frac 1N \sum_{i=1}^N L(y_i, f(x_i))
$$
当样本量$N$足够大时, 经验挂某抓囊性的策略可以取得较好的结果, 在现实中呗广泛采用, 如[[附录_名词解释#极大似然估计|极大似然估计]].

但若$N$较小, 则效果较差, 会产生[[附录_名词解释#过拟合|过拟合(over fitting)]]的现象. 


### 结构风险最小化

如果样本量$N$的数值较小, 可能会产生[[附录_名词解释#过拟合|过拟合(over fitting)]]的现象.

结构风险最小化就是为了防止[[附录_名词解释#过拟合|过拟合]]现象而提出的策略.  