
>[!warning] 提示
>点击右上角「书本」![[Pasted image 20231125105318.png]]图标, 进入阅读模式, 以获得更好的阅读体验！

本文旨在探寻风格迁移的评价标准. 众所周知, 风格迁移领域评价标准众说纷纭, 本文力求整理风格迁移领域顶会顶刊上文章中使用的定量评价指标

# 广泛收集阶段

此阶段用于广泛收集各种风格迁移领域的评价标准

1. [x] 欺骗率
	1. 来源：Sanakoyeu A, Kotovenko D, Lang S, et al. A style-aware content loss for real-time hd style transfer[C]//proceedings of the European conference on computer vision (ECCV). 2018: 698-714.
	2. 原始论文中对于欺骗率的描述
		1. We use a VGG16 network trained from scratch to classify 624 artists on Wikiart. Style transfer deception rate is calculated as the fraction of generated images which were classified by the network as the artworks of an artist for which the stylization was produced.
		2. 翻译：我们训练了一个VGG16网络, 使其能够区分Wikiart数据集上624个艺术家的不同艺术作品. 风格迁移欺骗率是这样一个数：首先风格迁移欺骗率是一个分数, 它的分母是「所有生成的风格图像」, 它的分子是「生成的风格图像中, 被认为是艺术家创作的艺术作品的数量」. 这也就是说, 风格迁移欺骗率表示了「生成图像被认为是画家画出的, 而非生成的图像的比率」
		3. 被引用次数：226(2024-1-17)
2. [x] FID
	1. 来源：Heusel M, Ramsauer H, Unterthiner T, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium[J]. Advances in neural information processing systems, 2017, 30.
	2. 描述：FID（Fréchet Inception Distance）是一种用于评估生成模型性能的指标，特别是在生成对抗网络（GANs）的上下文中。它基于两个概率分布之间的 Fréchet 距离，其中一个分布是真实数据的分布，另一个是生成器模型生成的数据的分布。
	3. 被引用次数：10301(2024-01-17)
3. [x] LPIPS（Perceptual Image Patch Similarity）
	1. 来源：Zhang R, Isola P, Efros A A, et al. The unreasonable effectiveness of deep features as a perceptual metric[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 586-595.
	2. 描述：LPIPS是一种用于评估图像质量和感知相似性的指标。与传统的像素级或结构相似性指标不同，LPIPS 主要关注图像在感知上的相似性，考虑人类视觉系统对图像中的感知信息的感知。
	3. 被引用次数：7428(2024-01-17)
4. [x] SSIM(Structural Similarity Index)
	1. 来源：
	2. 首次使用：An J, Huang S, Song Y, et al. Artflow: Unbiased image style transfer via reversible neural flows[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 862-871.
		1. 描述：受原始论文的启发，我们采用结构相似度指数（SSIM）和原始内容与风格化图像之间的内容损失作为度量来衡量风格迁移中内容信息保存的性能。
5. [x] Gram矩阵
	1. 来源
	2. 使用例子：An J, Huang S, Song Y, et al. Artflow: Unbiased image style transfer via reversible neural flows[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 862-871.
		1. 描述：为了衡量风格转移算法创建艺术效果的能力，受[32]的启发，我们使用风格图像和风格化图像之间的格拉姆矩阵的均方误差。
6. [x] Content Loss
	1. 来源：不知道
	2. 描述：![[Pasted image 20240117163602.png]]
		1. 本图来自文章Deng Y, Tang F, Dong W, et al. Stytr2: Image style transfer with transformers[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11326-11336.
	3. 使用这个标准的文章
		1. Li D, Luo H, Wang P, et al. Frequency domain disentanglement for arbitrary neural style transfer[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 1287-1295.
7. [x] Style Loss
	1. 来源：Huang X, Belongie S. Arbitrary style transfer in real-time with adaptive instance normalization[C]//Proceedings of the IEEE international conference on computer vision. 2017: 1501-1510.
		1. 描述：![[Pasted image 20240117161955.png]]
			1. 本图来自文章Deng Y, Tang F, Dong W, et al. Stytr2: Image style transfer with transformers[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11326-11336.
	2. 使用这个标准的文章：
		1. Li D, Luo H, Wang P, et al. Frequency domain disentanglement for arbitrary neural style transfer[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 1287-1295.
8. [x] PD(pixel distance)
	1. 来源：Wang Z, Zhao L, Chen H, et al. Diversified arbitrary style transfer via deep feature perturbation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 7789-7798.
		1. 描述：![[Pasted image 20240117162618.png]]
	2. 使用这个标准的文章：
		1. Cheng J, Wu Y, Jaiswal A, et al. User-controllable arbitrary style transfer via entropy regularization[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 433-441.
9. [ ] Content Fidelity (CF), 内容保真度
	1. 来源：Wang Z, Zhao L, Chen H, et al. Evaluate and improve the quality of neural style transfer[J]. Computer Vision and Image Understanding, 2021, 207: 103203.
	2. 描述：![[Pasted image 20240117165835.png]]
		1. 图像来自提出CF的原始论文, 即“来源一项”的文献
		2. 除此之外, 本文还提出了两个其他的评价标准
			1. Global Effects (GE).
			2. Local Patterns (LP).
10. [ ] $L_{sim}$
	1. 来源：Tang H, Liu S, Lin T, et al. Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 18329-18338.
	2. 描述：
		1. ![[Pasted image 20240117171741.png]]
		2. 图像来自原始论文, 即“来源一项”的文献

# VGG 19 作为损失函数基础

- 在阅读下述文章时, 发现 Gatys 等人使用 VGG 19 作为损失函数的基础的一些优势, 记录在此
	- 来源：Park T, Efros A A, Zhang R, et al. Contrastive learning for unpaired image-to-image translation \[C\] /a/Computer Vision–ECCV 2020: 16 th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16. Springer International Publishing, 2020: 319-345.
- 原文如下：
	- “Recently, the deep learning community has found that the VGG classification network [69] trained on ImageNet dataset [14] can be re-purposed as a “perceptual loss” [16,19,34,75,87,52], which can be used in paired image translation tasks [8,59,77], and was known to outperform traditional metrics such as SSIM [78] and FSIM [84] on human perceptual tests [87].” (Park 等, 2020, p. 3) 
	- 目前有人尝试使用 VGG 作为基础, 构建感知损失函数, 用以代替原本固定的距离眼熟.  
	- 基于 VGG 19 网络感知损失函数在评价人类感知上比 SSIM 与 FSIM 要好  
- 翻译如下
	- 🔤最近，深度学习社区发现在 ImageNet 数据集 [14] 上训练的 VGG 分类网络 [69] 可以重新用作“感知损失”[16,19,34,75,87,52]，这可以用于配对图像翻译任务 [8,59,77]，并且在人类感知测试 [87] 上优于 SSIM [78] 和 FSIM [84] 等传统指标。🔤

### 疑似可以当作损失函数的概念

#### Contextual Loss

- 在阅读下述文章时, 首次了解到 Contextual Loss 的概念
	- 来源：Park T, Efros A A, Zhang R, et al. Contrastive learning for unpaired image-to-image translation \[C\] /a/Computer Vision–ECCV 2020: 16 th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16. Springer International Publishing, 2020: 319-345.
	- 原文描述如下：
		- In particular, the Contextual Loss [52] boosts the perceptual quality of pretrained VGG features, validated by human perceptual judgments [51]
		- 🔤特别是，Contextual Loss [52] 提高了预训练 VGG 特征的感知质量，并通过人类感知判断进行了验证 [51]。🔤
- 根据上述文章的引用信息, Contextual Loss 来自以下文章
	- Mechrez, R., Talmi, I., Zelnik-Manor, L.: The contextual loss for image transformation with non-aligned data. In: European Conference on Computer Vision (ECCV) (2018) 3, 7, 23