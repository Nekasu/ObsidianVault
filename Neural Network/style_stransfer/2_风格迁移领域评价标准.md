
>[!warning] æç¤º
>ç‚¹å‡»å³ä¸Šè§’ã€Œä¹¦æœ¬ã€![[Pasted image 20231125105318.png]]å›¾æ ‡, è¿›å…¥é˜…è¯»æ¨¡å¼, ä»¥è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒï¼

æœ¬æ–‡æ—¨åœ¨æ¢å¯»é£æ ¼è¿ç§»çš„è¯„ä»·æ ‡å‡†. ä¼—æ‰€å‘¨çŸ¥, é£æ ¼è¿ç§»é¢†åŸŸè¯„ä»·æ ‡å‡†ä¼—è¯´çº·çº­, æœ¬æ–‡åŠ›æ±‚æ•´ç†é£æ ¼è¿ç§»é¢†åŸŸé¡¶ä¼šé¡¶åˆŠä¸Šæ–‡ç« ä¸­ä½¿ç”¨çš„å®šé‡è¯„ä»·æŒ‡æ ‡

# å¹¿æ³›æ”¶é›†é˜¶æ®µ

æ­¤é˜¶æ®µç”¨äºå¹¿æ³›æ”¶é›†å„ç§é£æ ¼è¿ç§»é¢†åŸŸçš„è¯„ä»·æ ‡å‡†

1. [x] æ¬ºéª—ç‡
	1. æ¥æºï¼šSanakoyeu A, Kotovenko D, Lang S, et al. A style-aware content loss for real-time hd style transfer[C]//proceedings of the European conference on computer vision (ECCV). 2018: 698-714.
	2. åŸå§‹è®ºæ–‡ä¸­å¯¹äºæ¬ºéª—ç‡çš„æè¿°
		1. We use a VGG16 network trained from scratch to classify 624 artists on Wikiart. Style transfer deception rate is calculated as the fraction of generated images which were classified by the network as the artworks of an artist for which the stylization was produced.
		2. ç¿»è¯‘ï¼šæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªVGG16ç½‘ç»œ, ä½¿å…¶èƒ½å¤ŸåŒºåˆ†Wikiartæ•°æ®é›†ä¸Š624ä¸ªè‰ºæœ¯å®¶çš„ä¸åŒè‰ºæœ¯ä½œå“. é£æ ¼è¿ç§»æ¬ºéª—ç‡æ˜¯è¿™æ ·ä¸€ä¸ªæ•°ï¼šé¦–å…ˆé£æ ¼è¿ç§»æ¬ºéª—ç‡æ˜¯ä¸€ä¸ªåˆ†æ•°, å®ƒçš„åˆ†æ¯æ˜¯ã€Œæ‰€æœ‰ç”Ÿæˆçš„é£æ ¼å›¾åƒã€, å®ƒçš„åˆ†å­æ˜¯ã€Œç”Ÿæˆçš„é£æ ¼å›¾åƒä¸­, è¢«è®¤ä¸ºæ˜¯è‰ºæœ¯å®¶åˆ›ä½œçš„è‰ºæœ¯ä½œå“çš„æ•°é‡ã€. è¿™ä¹Ÿå°±æ˜¯è¯´, é£æ ¼è¿ç§»æ¬ºéª—ç‡è¡¨ç¤ºäº†ã€Œç”Ÿæˆå›¾åƒè¢«è®¤ä¸ºæ˜¯ç”»å®¶ç”»å‡ºçš„, è€Œéç”Ÿæˆçš„å›¾åƒçš„æ¯”ç‡ã€
		3. è¢«å¼•ç”¨æ¬¡æ•°ï¼š226(2024-1-17)
2. [x] FID
	1. æ¥æºï¼šHeusel M, Ramsauer H, Unterthiner T, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium[J]. Advances in neural information processing systems, 2017, 30.
	2. æè¿°ï¼šFIDï¼ˆFrÃ©chet Inception Distanceï¼‰æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°ç”Ÿæˆæ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ï¼Œç‰¹åˆ«æ˜¯åœ¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„ä¸Šä¸‹æ–‡ä¸­ã€‚å®ƒåŸºäºä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„ FrÃ©chet è·ç¦»ï¼Œå…¶ä¸­ä¸€ä¸ªåˆ†å¸ƒæ˜¯çœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œå¦ä¸€ä¸ªæ˜¯ç”Ÿæˆå™¨æ¨¡å‹ç”Ÿæˆçš„æ•°æ®çš„åˆ†å¸ƒã€‚
	3. è¢«å¼•ç”¨æ¬¡æ•°ï¼š10301(2024-01-17)
3. [x] LPIPSï¼ˆPerceptual Image Patch Similarityï¼‰
	1. æ¥æºï¼šZhang R, Isola P, Efros A A, et al. The unreasonable effectiveness of deep features as a perceptual metric[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 586-595.
	2. æè¿°ï¼šLPIPSæ˜¯ä¸€ç§ç”¨äºè¯„ä¼°å›¾åƒè´¨é‡å’Œæ„ŸçŸ¥ç›¸ä¼¼æ€§çš„æŒ‡æ ‡ã€‚ä¸ä¼ ç»Ÿçš„åƒç´ çº§æˆ–ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ ‡ä¸åŒï¼ŒLPIPS ä¸»è¦å…³æ³¨å›¾åƒåœ¨æ„ŸçŸ¥ä¸Šçš„ç›¸ä¼¼æ€§ï¼Œè€ƒè™‘äººç±»è§†è§‰ç³»ç»Ÿå¯¹å›¾åƒä¸­çš„æ„ŸçŸ¥ä¿¡æ¯çš„æ„ŸçŸ¥ã€‚
	3. è¢«å¼•ç”¨æ¬¡æ•°ï¼š7428(2024-01-17)
4. [x] SSIM(Structural Similarity Index)
	1. æ¥æºï¼š
	2. é¦–æ¬¡ä½¿ç”¨ï¼šAn J, Huang S, Song Y, et al. Artflow: Unbiased image style transfer via reversible neural flows[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 862-871.
		1. æè¿°ï¼šå—åŸå§‹è®ºæ–‡çš„å¯å‘ï¼Œæˆ‘ä»¬é‡‡ç”¨ç»“æ„ç›¸ä¼¼åº¦æŒ‡æ•°ï¼ˆSSIMï¼‰å’ŒåŸå§‹å†…å®¹ä¸é£æ ¼åŒ–å›¾åƒä¹‹é—´çš„å†…å®¹æŸå¤±ä½œä¸ºåº¦é‡æ¥è¡¡é‡é£æ ¼è¿ç§»ä¸­å†…å®¹ä¿¡æ¯ä¿å­˜çš„æ€§èƒ½ã€‚
5. [x] GramçŸ©é˜µ
	1. æ¥æº
	2. ä½¿ç”¨ä¾‹å­ï¼šAn J, Huang S, Song Y, et al. Artflow: Unbiased image style transfer via reversible neural flows[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 862-871.
		1. æè¿°ï¼šä¸ºäº†è¡¡é‡é£æ ¼è½¬ç§»ç®—æ³•åˆ›å»ºè‰ºæœ¯æ•ˆæœçš„èƒ½åŠ›ï¼Œå—[32]çš„å¯å‘ï¼Œæˆ‘ä»¬ä½¿ç”¨é£æ ¼å›¾åƒå’Œé£æ ¼åŒ–å›¾åƒä¹‹é—´çš„æ ¼æ‹‰å§†çŸ©é˜µçš„å‡æ–¹è¯¯å·®ã€‚
6. [x] Content Loss
	1. æ¥æºï¼šä¸çŸ¥é“
	2. æè¿°ï¼š![[Pasted image 20240117163602.png]]
		1. æœ¬å›¾æ¥è‡ªæ–‡ç« Deng Y, Tang F, Dong W, et al. Stytr2: Image style transfer with transformers[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11326-11336.
	3. ä½¿ç”¨è¿™ä¸ªæ ‡å‡†çš„æ–‡ç« 
		1. Li D, Luo H, Wang P, et al. Frequency domain disentanglement for arbitrary neural style transfer[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 1287-1295.
7. [x] Style Loss
	1. æ¥æºï¼šHuang X, Belongie S. Arbitrary style transfer in real-time with adaptive instance normalization[C]//Proceedings of the IEEE international conference on computer vision. 2017: 1501-1510.
		1. æè¿°ï¼š![[Pasted image 20240117161955.png]]
			1. æœ¬å›¾æ¥è‡ªæ–‡ç« Deng Y, Tang F, Dong W, et al. Stytr2: Image style transfer with transformers[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11326-11336.
	2. ä½¿ç”¨è¿™ä¸ªæ ‡å‡†çš„æ–‡ç« ï¼š
		1. Li D, Luo H, Wang P, et al. Frequency domain disentanglement for arbitrary neural style transfer[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 1287-1295.
8. [x] PD(pixel distance)
	1. æ¥æºï¼šWang Z, Zhao L, Chen H, et al. Diversified arbitrary style transfer via deep feature perturbation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 7789-7798.
		1. æè¿°ï¼š![[Pasted image 20240117162618.png]]
	2. ä½¿ç”¨è¿™ä¸ªæ ‡å‡†çš„æ–‡ç« ï¼š
		1. Cheng J, Wu Y, Jaiswal A, et al. User-controllable arbitrary style transfer via entropy regularization[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 433-441.
9. [ ] Content Fidelity (CF), å†…å®¹ä¿çœŸåº¦
	1. æ¥æºï¼šWang Z, Zhao L, Chen H, et al. Evaluate and improve the quality of neural style transfer[J]. Computer Vision and Image Understanding, 2021, 207: 103203.
	2. æè¿°ï¼š![[Pasted image 20240117165835.png]]
		1. å›¾åƒæ¥è‡ªæå‡ºCFçš„åŸå§‹è®ºæ–‡, å³â€œæ¥æºä¸€é¡¹â€çš„æ–‡çŒ®
		2. é™¤æ­¤ä¹‹å¤–, æœ¬æ–‡è¿˜æå‡ºäº†ä¸¤ä¸ªå…¶ä»–çš„è¯„ä»·æ ‡å‡†
			1. Global Effects (GE).
			2. Local Patterns (LP).
10. [ ] $L_{sim}$
	1. æ¥æºï¼šTang H, Liu S, Lin T, et al. Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 18329-18338.
	2. æè¿°ï¼š
		1. ![[Pasted image 20240117171741.png]]
		2. å›¾åƒæ¥è‡ªåŸå§‹è®ºæ–‡, å³â€œæ¥æºä¸€é¡¹â€çš„æ–‡çŒ®

# VGG 19 ä½œä¸ºæŸå¤±å‡½æ•°åŸºç¡€

- åœ¨é˜…è¯»ä¸‹è¿°æ–‡ç« æ—¶, å‘ç° Gatys ç­‰äººä½¿ç”¨ VGG 19 ä½œä¸ºæŸå¤±å‡½æ•°çš„åŸºç¡€çš„ä¸€äº›ä¼˜åŠ¿, è®°å½•åœ¨æ­¤
	- æ¥æºï¼šPark T, Efros A A, Zhang R, et al. Contrastive learning for unpaired image-to-image translation \[C\] /a/Computer Visionâ€“ECCV 2020: 16 th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part IX 16. Springer International Publishing, 2020: 319-345.
- åŸæ–‡å¦‚ä¸‹ï¼š
	- â€œRecently, the deep learning community has found that the VGG classification network [69] trained on ImageNet dataset [14] can be re-purposed as a â€œperceptual lossâ€ [16,19,34,75,87,52], which can be used in paired image translation tasks [8,59,77], and was known to outperform traditional metrics such as SSIM [78] and FSIM [84] on human perceptual tests [87].â€ (Park ç­‰, 2020, p. 3) 
	- ç›®å‰æœ‰äººå°è¯•ä½¿ç”¨ VGG ä½œä¸ºåŸºç¡€, æ„å»ºæ„ŸçŸ¥æŸå¤±å‡½æ•°, ç”¨ä»¥ä»£æ›¿åŸæœ¬å›ºå®šçš„è·ç¦»çœ¼ç†Ÿ.  
	- åŸºäº VGG 19 ç½‘ç»œæ„ŸçŸ¥æŸå¤±å‡½æ•°åœ¨è¯„ä»·äººç±»æ„ŸçŸ¥ä¸Šæ¯” SSIM ä¸ FSIM è¦å¥½  
- ç¿»è¯‘å¦‚ä¸‹
	- ğŸ”¤æœ€è¿‘ï¼Œæ·±åº¦å­¦ä¹ ç¤¾åŒºå‘ç°åœ¨ ImageNet æ•°æ®é›† [14] ä¸Šè®­ç»ƒçš„ VGG åˆ†ç±»ç½‘ç»œ [69] å¯ä»¥é‡æ–°ç”¨ä½œâ€œæ„ŸçŸ¥æŸå¤±â€[16,19,34,75,87,52]ï¼Œè¿™å¯ä»¥ç”¨äºé…å¯¹å›¾åƒç¿»è¯‘ä»»åŠ¡ [8,59,77]ï¼Œå¹¶ä¸”åœ¨äººç±»æ„ŸçŸ¥æµ‹è¯• [87] ä¸Šä¼˜äº SSIM [78] å’Œ FSIM [84] ç­‰ä¼ ç»ŸæŒ‡æ ‡ã€‚ğŸ”¤

### ç–‘ä¼¼å¯ä»¥å½“ä½œæŸå¤±å‡½æ•°çš„æ¦‚å¿µ

#### Contextual Loss

- åœ¨é˜…è¯»ä¸‹è¿°æ–‡ç« æ—¶, é¦–æ¬¡äº†è§£åˆ° Contextual Loss çš„æ¦‚å¿µ
	- æ¥æºï¼šPark T, Efros A A, Zhang R, et al. Contrastive learning for unpaired image-to-image translation \[C\] /a/Computer Visionâ€“ECCV 2020: 16 th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part IX 16. Springer International Publishing, 2020: 319-345.
	- åŸæ–‡æè¿°å¦‚ä¸‹ï¼š
		- In particular, the Contextual Loss [52] boosts the perceptual quality of pretrained VGG features, validated by human perceptual judgments [51]
		- ğŸ”¤ç‰¹åˆ«æ˜¯ï¼ŒContextual Loss [52] æé«˜äº†é¢„è®­ç»ƒ VGG ç‰¹å¾çš„æ„ŸçŸ¥è´¨é‡ï¼Œå¹¶é€šè¿‡äººç±»æ„ŸçŸ¥åˆ¤æ–­è¿›è¡Œäº†éªŒè¯ [51]ã€‚ğŸ”¤
- æ ¹æ®ä¸Šè¿°æ–‡ç« çš„å¼•ç”¨ä¿¡æ¯, Contextual Loss æ¥è‡ªä»¥ä¸‹æ–‡ç« 
	- Mechrez, R., Talmi, I., Zelnik-Manor, L.: The contextual loss for image transformation with non-aligned data. In: European Conference on Computer Vision (ECCV) (2018) 3, 7, 23