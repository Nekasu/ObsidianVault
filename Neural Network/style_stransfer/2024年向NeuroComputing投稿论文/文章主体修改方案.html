<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>文章主体修改方案</title>
    <link rel="stylesheet" href="/Template/styles.css">
</head>
<body>
   <div class="container">
        <h1>文章主体修改方案</h1>
        <div class="note-section">
            <h2>细分领域添加</h2>
            工作进度记录：
            <ul>
                <li>2024-12-11-16:37 周三：由于考虑到本文的核心是2D风格迁移, 所以在文章主体中, 不应出现过多的其他细分领域内容, 所以接受GPT的意见, 重新规划文章主体的修改方案. 文章主体指的是Neural Computing 一节. </li>
                <li> <span style="color: #FF5582A6;"> 2024-12-11-22:45 周三：找到了快速完成论文添加的方法：先自己找找最新的文章, 随后在自己找到的文章中挑选几篇, 将其中的Related Works部分中涉及的文章发给GPT或智谱清言, 让他用2~3句话总结. 复制粘贴到文章中即可. 当前添加了3D风格迁移中自己找到的文章, 明天添加那些最新文章中提到的文章. 使用该方法, 明天完成细分领域添加这一部分.</span> </li>
                <li>2024-12-12-16:50 周四：完成3D风格迁移与视频风格迁移文章的简单添加, <span style="color: #6495EDA6;">如果有时间, 可以再添加一点音频风格迁移的内容</span></li>
            </ul>
            <span style="color: #6495EDA6;"> <p>GPT的建议如下：在有限篇幅内，增加一个简短的章节或小节概述3D风格迁移的进展及潜力，回应审稿人的期待，同时保持文章聚焦于2D研究的主线。</p> </span>
            <p>但实际上, 我希望有一个章节介绍细分领域, 而不仅仅是介绍3D风格迁移.</p>
            <p>根据GPT的建议, 关于新增章节的放置位置和标题建议如下 </p>
            <ul>
                <li>位置: 放在主线内容结束后，作为补充内容，位于总结或未来研究方向之前。 </li>
                <li>标题: 推荐使用 “扩展领域：视频、三维与多模态风格迁移”，既概括主题，又能体现技术的多样性与进步。</li>
            </ul>
            <br>
        </div>
        <div class="note-section">
            <h2>近期研究讨论</h2>
            问题: 未涉及多个最新相关工作。
        </div>
        <div class="note-section">
            <h2>未涵盖领域迁移的相关性</h2>
            问题: 对领域迁移的覆盖不足。
            <br>
            建议:
            <br>
            增加对风格迁移与领域迁移之间联系的讨论，例如通过领域适配技术优化风格迁移模型。
            <br>
            引入相关工作，如Li等人提出的基于最大均值差异（MMD）的领域适配方法。
            <br>
            示例:
            <br>
            “领域迁移技术为风格迁移提供了新的视角。例如，Li等人[引用]利用MMD方法，提出了一种基于领域适配的风格迁移框架，显著提高了XXX的性能。”
        </div>
    </div>

    <br>

    <div class="container">
        <h1>修改方案执行</h1>
        <div class="note-section">
            <h2>细分领域-原始文稿撰写(该章节名为"风格迁移：从二维到多模态的探索")</h2>
            <h3>引言：领域推广的意义</h3>
            <ul>
                <li>强调这些领域不属于文章核心, 但是与2D基础密切相关
                    <p>风格迁移任务以二维图像作为为原始的输入(Gatys),  但是也有一些工作在扩展风格迁移的边界, 如视频风格迁移(引用一些文章), 3D风格迁移(引用一些文章), 多模态风格迁移(引用一些文章). </p>
                    <p>尽管本文重点在于介绍二维图像风格迁移成果, 但是为了能使研究者对这些领域有一些简单的认知, 所以该部分简单介绍一些风格迁移下细分领域的研究现状.</p>
                    <p> 风格迁移任务最初是基于二维图像进行的，其中Gatys等人于2015年提出的基于深度神经网络的风格迁移方法开创了这一领域。自此，风格迁移技术便主要集中于图像风格的迁移，通过对输入图像的内容和风格特征进行提取和结合，实现了高质量的艺术风格图像生成。然而，随着研究的不断深入，风格迁移的应用边界逐渐扩展，不仅涵盖了视频风格迁移（如[引用文章]）、3D风格迁移（如[引用文章]）和多模态风格迁移（如[引用文章]）等多个领域，也引发了关于如何在这些新领域中保持风格一致性与跨模态协调性的研究。 </p>
                    <p> 尽管本文的重点在于介绍二维图像风格迁移的最新成果，但为了让研究者对风格迁移在其他领域的应用有一个初步的了解，本文将简要概述风格迁移在视频、3D及多模态场景中的研究现状。特别是随着计算能力的提升和技术的不断演进，风格迁移从二维图像扩展到视频、3D场景以及多模态数据处理，不仅拓宽了应用场景，也为实际问题提供了更灵活、更具创意的解决方案。 </p>
                </li>
                <li>风格迁移从2D扩展到视频、3D与多模态的背景与驱动力</li>
            </ul>
            <h3>视频风格迁移</h3>
            <ul>
                <li>从Stylizing 3D Scene中查找</li>
            </ul>
            <p>实际上, 大部分实时风格迁移成果均可在一定程度上应用于视频风格迁移, 但是由于这些方法没有为视频风格化任务进行专门的优化, 所以在进行视频风格迁移时可能在时序一致性上有所欠缺, 其表现为在视频帧与帧之间往往会产生闪烁和错误的不连续性. 同时, 在生成高分辨率的风格化长视频时, 可能需要耗费大量的时间.</p>
            <p>现有的视频风格迁移方法根据是否使用光流(optical flow)可以大致分为两类.</p>
            <span style="color: #6495EDA6;"><p>使用光流的视频风格迁移方法较早出现.</p></span>
            <p>据我们所知, Ruder等人(98ruder2016artistic)率先将光流信息与视频风格迁移任务结合, 提出了一种将艺术风格迁移到视频序列的方法. 它利用深度神经网络，并结合了光流信息和时间一致性损失函数，生成了具有艺术风格且保持时间一致性的视频。为了解决视频风格迁移中闪烁和不连续的问题，作者引入了短期和长期一致性损失，并通过多遍算法进一步提高了视频的视觉效果。</p>
            <p>Gao等人(99gao2019reconet)提出了一种名为 ReCoNet 的新型实时视频风格迁移网络，该网络能够生成在近实时内具有时间一致性的风格化视频序列。 ReCoNet 通过将流子网络和掩码子网络集成到预训练的风格化网络的中间层，实现了短期一致性，并通过循环神经网络结构将其传播到长期，从而保证了更长时间的一致性.</p>
            <p>Chen等人(100chen2020optical)将知识蒸馏与光流信息结合,用于训练高效且稳定的视频风格迁移网络。 该方法利用知识蒸馏技术，将包含光流模块的稳定风格迁移网络（教师网络）的知识迁移到不包含光流模块的轻量级网络（学生网络）。通过引入残差蒸馏损失和低秩蒸馏损失，学生网络能够学习到教师网络中光流带来的时间一致性信息，并生成具有相似低秩特性的风格化视频，从而提高视频风格迁移的稳定性和效率。</p>
            <p>基于光流的视频风格迁移, 往往未能很好地平衡稳定性和效率。Gao等人(99gao2019reconet)仅在训练阶段使用光流, 试图提升推理过程的效率。然而，与Ruder等人(98ruder2016artistic)在推理阶段也使用光流的方法相比，仅在训练阶段使用光流的方法生成的结果不太稳定。而Chen等人(100chen2020optical)提出的光流蒸馏方法(optical flow distillation)一定程度上平衡了效率和稳定性，但它需要为每种风格训练一个单独的模型，无法实现任意风格的风格迁移.</p>
            <span style="color: #6495EDA6;"><p>今年来, 不使用光流约束作为时序一致性保证的视频风格迁移方法正崭露头角. </p></span>
            <p>Li等人(101li2019learning)的成果通过学习线性变换矩阵来实现视频风格迁移，该方法能够有效地保留内容信息, 从而能输出稳定的结果.</p>
            <p>Wang等人(102wang2020consistent)提出了一种新的视频风格迁移框架，该框架能够生成在时间和视觉上均具有一致性的风格化视频序列。 为了解决视频风格迁移中闪烁和不连续的问题，本文提出了一个新的复合正则化项，该正则化项能够更好地拟合时间变化的本质，并有效地平衡时间和风格化的效果。 此外，Wnag等人还设计了一种序列级全局特征共享策略，以实现长期时间一致性，并提出了一个动态通道间滤波器模块，以提高风格迁移的效果并支持端到端训练。</p>
            <p>Deng等人(97deng2021arbitrary)提出了一种名为 MCCNet 的视频风格迁移网络，该网络通过跨域特征对齐，在保持视频内容结构的同时，实现了高质量的任意风格迁移。该网络无需光流计算，直接在内容和风格特征空间中对特征进行重新排列和融合，确保风格模式适应内容结构。此外, 为增强在复杂光照条件下的模型稳定性，本文通过加入模拟光照变化的照明损失，以提升生成视频的时序一致性。</p>
            <p>Xia等人(103xia2021real)另辟蹊径, 选择使用双边学习(bilateral learning)来实现实时的局部光真实视频风格迁移。他们提出了一种基于深度神经网络的新方法，能够将艺术风格局部传递到视频的语义区域，同时保持光真实感和时间一致性。通过引入时空特征传递层（ST-AdaIN），增强分割掩码，并在低分辨率网格空间进行颜色变换融合，显著提升了结果的质量和效率。</p>
            <h3>3D风格迁移</h3>
            <p>2024-12-12-14:15 周四：完成3D风格迁移部分撰写</p>
            <p> 与Gatys等人于xxxx年提出的2D图像风格迁移相比, 3D风格迁移是一个较新的概念. 3D风格迁移根据目标的不同可以分成几何形状迁移(Geometry Stylization)(84liu2019cubic)和模型纹理迁移(Texture Stylization). </p>
            <p>liu等人(84liu2019cubic)工作是一个3D风格迁移领域工作的先驱, 他们将输入的形状转换为固定的立方体风格, 实现了几何形状的3D风格迁移. Wang等人的成果(85yifan2020neural)则是一个更为成熟的3D几何风格迁移成果. 他们的工作可以扭曲源的几何形状以匹配目标的几何形状, 并在此基础上保留源数据的表面细节.</p>
            <p><del>To our best knowledge, 3D风格迁移的第一个成果由Mattia(83segu20203dsnet)等人于2020年发表。本文提出了3DSNet，这是一种基于生成式学习的无监督方法，用于3D形状的风格迁移。通过解耦内容和风格表示，该方法实现了点云和网格之间的风格转移，同时保留了源形状的内容。模型还通过学习多模态风格分布，进一步提高了风格转移的多样性。</del> </p>
            <br>
            <p>3D纹理迁移技术近年来逐渐兴起，其核心挑战在于如何在保留三维物体几何形状的同时，将目标风格准确地映射到表面纹理和细节上。与2D图像风格迁移相比，3D纹理风格迁移需要处理额外的复杂性，包括多视角一致性（Multi-view Consistency）、三维形状的几何完整性（Geometric Integrity）以及高分辨率纹理映射（High-resolution Texture Mapping）。这些特性使得3D风格迁移在虚拟现实（VR）、游戏开发和建筑渲染等领域具有广泛的应用前景。</p>
            <p>Huang等人(87huang2021learning)提出了一种基于点云的3D场景风格化方法，通过将图像特征反投影到3D空间来构建点云，并使用点云变换模块将参考图像的风格信息转移到点云上，最终生成具有一致风格的新视角图像。 </p>
            <p>Yin等人(89yin20213dstylenet)提出了一种名为 3DStyleNet 的神经风格迁移方法，它能够同时迁移 3D 物体的几何形状和纹理风格。 3DStyleNet 首先通过学习语义部件之间的全局几何关系来定义风格，然后预测一个部件感知仿射变换场来扭曲源形状，使其模仿目标形状的整体几何风格。 此外，3DStyleNet 还利用可微渲染器和预训练的图像风格迁移网络，将目标形状的纹理风格迁移到扭曲后的源物体上。 通过联合优化几何形状和纹理，3DStyleNet 可以生成具有逼真风格细节和连贯内容结构的 3D 物体，并将其应用于 3D 内容创建和风格化数据增强等任务。</p>
            <p>Chiang等人(90chiang2022stylizing)以3D风格迁移中的视角不一致性问题为切入点,提出了一种基于NeRF的3D场景风格迁移方法.该方法通过学习3D场景的隐式表示，使用超网络将参考图像的风格信息转移到场景表示中，最终生成具有一致风格的任意新视角图像。</p>
           <p>Huang等人提出了名为StylizedNeRF(93huang2022stylizednerf) 的 3D 场景风格化方法，它通过 2D-3D 互学习框架，将 2D 图像风格化网络和 NeRF 结合，实现了高质量且保持几何一致性的 3D 场景风格化。 该方法首先训练一个标准的 NeRF 来表示 3D 场景，并用风格化网络替换其颜色预测模块，得到一个风格化的 NeRF。 然后，通过引入一致性损失，将 NeRF 中的空间一致性先验知识蒸馏到 2D 风格化网络中。 此外，该方法还引入了可学习的潜在码，以处理 2D 风格化结果的歧义性，并实现条件风格化。 </p>
            <p>Hollein等人(86hollein2022stylemesh)的工作聚焦于3D室内场景重建, 他们扫描并构建室内场景的RGB-D图, 以生成3D网格(Mesh), 结合深度与表面法线数据, 实现了3D空间中一致且我视点依赖性的风格迁移. 但是该方法需要对每个场景单独优化, 从而导致了该方法的效率不高.</p>
            <p>Thu等人(91nguyen2022snerf)以生成清晰且详细的风格话3D场景为目标, 提出了StyleMesh模型. 该方法通过优化重建网格的显示纹理, 并结合来自所有可用图像的风格信息, 实现了3D一致的风格化场景生成.</p>
            <p>Liu等人(82liu2023stylerf)提出了StyleRF模型.该模型包含三个核心组件：显式特征网格(Feature Grid 3D Representation),采样不变的内容变换(Sampling-invariant Content Transformation, SICT)以及延迟风格变换(Deferred Style Transformatio).显式特征网格(Feature Grid 3D Representation)采用高层特征网格表示3D场景，通过体积渲染实现高保真几何重建。采样不变的内容变换(Sampling-invariant Content Transformation, SICT)通过体积自适应归一化和通道自注意力机制，消除对采样点统计的依赖，从而确保多视角一致性.延迟风格变换(Deferred Style Transformation, DST)将风格变换延迟到体积渲染后的2D特征图上进行。 使用显式特征网格(Feature Grid 3D Representation)使得该模型提高了几何重建的精确度. SICT实现了多视角的风格统一, DST将风格迁移过程推迟到渲染后的2D特征图上降低了计算复杂度. 三者结合, 做到了几何重建精确度与计算复杂度方面的提升.</p>
            <p>NeRF-Art是由Wang等人(94wang2023nerf)提出的一种基于文本驱动的 NeRF 风格化方法，它可以通过简单的文本提示来改变预训练的 NeRF 模型的风格。 该方法结合了 CLIP 模型和 NeRF，并通过引入全局-局部对比学习策略和方向约束，同时控制目标风格的方向和强度，实现了外观和几何形状的联合风格化。 此外，该方法还采用了权重正则化技术，有效地减少了风格化过程中产生的云状伪影和几何噪声。</p>
            <p>与Wang(94wang2023nerf)等人类似, Haque等人(95haque2023instruct)也试图使用自然语言引用3D场景风格化, 为此他们提出了 Instruct-NeRF2NeRF, 这是一种新的 3D 场景编辑方法，它可以通过自然语言指令来编辑预捕获的 NeRF 场景。 该方法首先使用 InstructPix2Pix 扩散模型根据指令编辑 NeRF 渲染的图像，然后将编辑后的图像用于更新 NeRF 训练数据集，并重新训练 NeRF，从而将编辑结果整合到 3D 场景中。 该方法能够进行各种局部和全局场景编辑，包括改变纹理、替换物体、改变场景的全球属性等，且能够保持较好的一致性。</p>
            <p>Chen等人(104chen2024stylecity)提出了StyleCity，这是一种大规模3D城市场景的风格化框架，结合图像和文本输入，实现全局与局部风格特征的语义一致性。通过神经纹理场、渐进式多尺度优化和生成扩散模型，StyleCity在保持光真实感的同时，自动生成高质量的纹理和全景天空背景，为虚拟场景的设计和呈现提供了一种高效解决方案。</p>
            <p>Chen等人(92chen2024upst)提出了一种名为 UPST-NeRF 的通用逼真风格迁移方法，它可以将任意风格图像的风格迁移到 3D 场景中，并保证从不同视角渲染的图像的一致性。 该方法首先使用体素网格重建场景的几何结构，然后利用超网络将风格图像的特征作为场景风格化的潜在码，并通过预训练的 2D 逼真风格迁移网络约束渲染图像的风格，从而实现高质量的 3D 场景风格化。</p>
            </p>
            <p>He等人(111he2025freditor)从频域分解的角度出发, 试图优化3D风格迁移方法面临的视图一致性问题. 他们提出的Freditor 是一种基于频率分解技术的 3D 风格迁移方法. 该方法主要基于两个观察：图像的低频部分在编辑后比高频部分具有更好的多视图一致性, 以及外观风格主要体现在低频部分，而内容细节则主要集中在高频部分。基于这两个观察, 他们提出了频率分解、特征空间编辑的方法. 其中, 频域分解工作主要 将 NeRF 渲染的图像分解为低频和高频部分。低频部分用于进行风格编辑，而高频部分用于保留内容细节。随后,  在特征空间中进行风格编辑，而不是直接在 RGB 空间中进行。这可以实现更稳定的强度控制和风格迁移. 通过这两个方法, Freditor一定程度上缓解了 3D 风格迁移中的多视图不一致性问题。</p>
            <p>Gao等人(117gao2023clip3dstyler)提出了一种名为 CLIP3Dstyler 的新方法，旨在解决语言引导的 3D 任意风格迁移问题。该方法的目标是通过文本描述将任意风格应用于 3D 场景，并生成新的风格化视图. Gao等人认为由于点云和文本特征存在于不同的特征空间，直接匹配会导致风格迁移效果不佳。 CLIP3Dstyler 通过提取局部点云描述符和全局点云特征，并将其与文本特征进行匹配，解决了这一问题。通过这种独特的解决方案，Gao等人成功实现了语言引导的 3D 任意风格迁移，为 3D 场景的美化提供了新的可能性。</p>
            <p>综合上述文章可以发现, 当前3D风格迁移存在几何重建精度、高质量风格化以及对任意新风格的泛化能力的三重困境, 若试图提高几何重建精度, 那么可能需要较大的计算量. 若试图提升风格化质量, 那么可能无法做到迁移任意风格. 如何权衡这三者之间的矛盾, 是3D风格迁移的一个核心问题.</p>
            <h3>音频风格迁移</h3>
            <span style="color: #FF5582A6;">如果还有时间, 可以添加几篇这方面的文章.</span>
        </div>
        <div class="note-section">
            <h2>近期研究讨论</h2>
            需要增加多篇近期的文章, 不管是否为顶会顶刊.
            <h3>ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation</h3>
            <p>本文的核心骨干为扩散模型.</p>
            <p>Chen等人(105chen2024artadapter)以则考虑从文本角度出发合成风格图像, 提出了一种名为ArtAdapter的文本到图像(T2I)风格迁移框架. 该框架的核心在于其多层风格编码器和显式自适应机制的结合，这使得ArtAdapter在风格迁移保真度方面具有一定提升，并确保与文本描述的一致性.</p>
            <p>多层风格编码器能从预训练的 VGG 网络中提取不同层次的特征（低、中、高），并分别使用 MLPs 进行编码，生成丰富的风格嵌入，类似于用伪词对风格参考进行“描述”。而显式自适应机制主要作用于在扩散模型的交叉注意力层中，专门对风格编码进行自适应，而保留文本编码的完整性，确保模型对风格细节的精准捕捉，同时保持文本的泛化能力。此外, 在训练阶段, 辅助内容适配器 (ACA)可以提供弱内容指导，帮助分离风格参考中的内容结构和风格特征，避免生成图像被风格参考的内容语义所主导。</p>
            <h3>You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval</h3>
            <p>本文的核心骨干为CLIP</p>
            <p>Koley等人(106koley2024you)试图将手绘草图、文本引导与风格迁移结合, 提出了一种名为 “You’ll Never Walk Alone” 的图像检索框架，该框架通过结合手绘草图和文本描述，实现了对细粒度图像的精准生成。本文认为, 传统的方法主要依赖于草图或文本描述，但本文则认为草图和文本在细粒度表示方面具有协同作用。通过将草图转换为伪词元，并与文本描述进行组合，该框架能够有效地捕捉到图像的细粒度特征，例如颜色、纹理和上下文信息。 为了解决训练过程中缺乏细粒度文本描述的问题，作者提出了一个新颖的组合性约束，该约束利用草图和照片之间的差异信号来模拟缺失的文本描述。 此外，为了增强模型的泛化能力，作者还引入了文本到文本的泛化损失，使学习到的提示向量与实际的文本提示更加相似。</p>
            <h3>Foreground and background separated image style transfer with a single text condition</h3>
            <p>本文为基于CLIP的风格迁移成果</p>
            <p>Yu等人(107yu2024foreground)将目标检测与风格迁移结合, 提出了一种名为 SA2-CS 的图像风格迁移方法, 实现了基于文本描述的显著目标风格迁移, 本文无须风格图像作为输入, 而是使用一段文字作为风格的描述. 本文将内容图像区分为前景与背景, 分别进行风格迁移任务. 具体来说, 本文 利用 U2-Net 显著目标检测网络区分图像中的前景和背景区域，并分别进行不同程度的风格迁移，避免了前景内容的过度风格化，增强了前景与背景的对比度。为了进一步防止前背景风格相互影响, 本文 引入全局背景损失函数，通过遮罩的方式避免前景内容在背景风格迁移过程中被破坏，同时增强了背景的风格迁移效果。针对传统全局 CLIP 损失的不足，提出了语义感知 PatchCLIP 损失，通过语义感知随机裁剪、语义感知权重惩罚、语义感知阈值正则化和语义感知可调节图像块等机制，实现了对不同语义区域风格迁移程度的精确控制。</p>
            <h3>S2WAT: Image Style Transfer via Hierarchical Vision Transformer Using Strips Window Attention</h3>
            <p>本文以Transformer作为骨干网络</p>
            <p>Zhang等人(108zhang2024s2wat)观察到, 基于窗口注意力机制的Transformer模型用于风格迁移工作时, 容易产生类似于网格的图案, 导致风格迁移效果不佳. 他们认为, 窗口注意力机制强大的局部能力导致他们在长程信息提取能力的缺失. 为此, 他们提出了一种名为 S2WAT 的图像风格迁移方法，该方法引入了条带窗口注意力机制(Strips Window Attention , SpW Attention)和 Attn Merge 技术, 以提升网络的长程信息提取能力. 具体来说,S2WAT 采用类似于 Swin Transformer 的层次架构，通过逐步降低图像分辨率并提取不同尺度的特征，能够有效地捕捉图像的局部和全局信息。SpW Attention 是 S2WAT 的核心模块，它结合了不同形状的窗口注意力，包括水平条带状窗口、垂直条带状窗口和正方形窗口。水平/垂直条带状窗口注意力用于提取非局部特征，而正方形窗口注意力则专注于捕获局部特征。这种设计平衡了短程和远程依赖关系的建模，避免了传统窗口注意力导致的局部性问题。Attn Merge 技术用于融合不同窗口注意力输出的结果。它通过计算输入特征与不同窗口注意力输出之间的空间相关性，并根据相关性得分进行加权求和，动态地确定不同窗口注意力的重要性，从而增强风格迁移效果。</p>
            <h3>Regional Style and Color Transfer</h3>
            <p>自搭建网络</p>
            <p>Ding等人(109ding2024regional)将风格迁移与图像分割结合, 提出了一种名为区域风格和颜色迁移（Regional Style and Color Transfer）的图像处理方法，旨在解决传统全局风格迁移方法在处理包含前景物体（如人物）的图像时，容易产生不自然效果的问题。本文首先使用使用 DeepLabv3+ 深度学习模型对图像进行语义分割，将前景物体（如人物）与背景区域分离, 随后分别对背景与前景进行风格迁移. 为了使前景物体与风格迁移后的背景更加协调，本文使用 lαβ 颜色空间和主成分分析 (PCA) 对前景物体进行颜色迁移。这种方法可以有效地改变前景物体的颜色分布，使其与背景颜色更加和谐。最后, 使用 alpha 混合技术将风格迁移后的背景和颜色迁移后的前景物体融合, 以消除图像区域的边界，减轻前背景的割裂感.</p>
            <h3>StyleFormer: Real-Time Arbitrary Style Transfer via Parametric Style Composition</h3>
            <span style="color: #FF5582A6;">StyleFormer: Real-Time Arbitrary Style Transfer via Parametric Style Composition</span>
            <ol>
                <li>应添加于3.3.3 基于注意力机制的风格迁移一段的开篇, 作为第一篇文章讲解.</li>
                <li>本文(110wu2021styleformer)发表于2021年, 其核心目标是实现任意且实时的风格迁移.<span style="color: #6495EDA6;">实际上并不是3D风格迁移.</span></li>
                <li>Wu(110wu2021styleformer)等人观察到了Transformer的多头注意力机制能捕获长程依赖性的特点, 并认为该特点使得Transformer在捕捉全局风格信息与内容语义上具有较大优势, 故而提出了StyleFormer. </li>
                <li>
                    StyleFormer的网络架构可以描述为Encoder-基于Transformer的风格迁移核心-Decoder. 除了基于VGG16的Encoder与Decoder之外, 本文网络核心可以细化为三个部分：1. Style Bank Generation Module 2. 基于Transformer的风格组合模块Transformer-driven Style Composition Module, 3. 参数化内容调制模块(Parametric Content Modulation Module)
                    <ul>
                        <li>Style Bank Generation Module: 该模块以风格图像的编码结果作为输入, 并将其表示为有限数量的风格代码（Style Codes），每个代码包含一个风格键(Key)以及对应的风格值(Value). 风格键(Key)用于表示风格特征, 而对应的风格值(Value)是一个仿射变换矩阵, 用于根据风格特征调整内容图像.</li>
                        <li>Transformer-driven Style Composition Module: 该模块根据内容特征动态组合风格代码(Style Codes)，通过多头注意力机制实现风格与内容的全局一致性。该模块使用风格键(Key)和风格值(Value)分别作为Transformer的Key和Value，将内容特征作为Query。并多头注意力机制（Multi-head Attention）学习风格与内容的全局关联，为不同组别生成内容一致的仿射系数, 从而确保风格迁移结果在全局上保持风格特征的细腻表达，同时语义结构与内容特征一致。</li>
                        <li>Parametric Content Modulation Module模块将Transformer-driven Style Composition Module生成的仿射系数应用于内容特征，完成风格化特征的生成. Decoder以该风格化特征作为输入, 解码为最终的风格化图像.</li>
                    </ul>
                </li>
            </ol>
            <h3>QuantArt: Quantizing Image Style Transfer Towards High Visual Fidelity</h3>
            <p>注意力机制</p>
            <p>Huang等人(112huang2023quantart)认为当前的风格迁移成果忽略了生成的风格化图像的保真度, 故而提出了 QuantArt, 这是一种基于向量量化的图像风格迁移方法，旨在提高生成图像的视觉保真度，使其更接近真实艺术作品。 QuantArt中, 向量量化和风格引导注意力 (SGA) 是两个关键的技术，它们共同实现了高视觉保真度的图像风格迁移。向量量化是一种将连续特征表示转换为离散编码的技术。它通过学习一个码本，将特征空间划分为多个聚类，并将每个特征映射到距离最近的聚类中心，从而实现特征的量化。SGA 是一种基于注意力机制的风格迁移模块，它通过学习风格参考和内容特征之间的关系，将风格信息迁移到内容特征上。向量量化将生成图像的特征表示向真实艺术作品分布的聚类中心靠近，提高视觉保真度；SGA 将风格信息迁移到内容特征上，实现更有效的风格迁移。两者结合，QuantArt 实现了高视觉保真度的图像风格迁移。</p>
            <h3>Name Your Style: Text-Guided Artistic Style Transfer</h3>
            <p>CLIP</p>
            <p>Liu等人(113liu2023name)指出, 现有的风格迁移方法通常需要额外的风格图像作为参考，这限制了其灵活性和便利性。为了解决这个问题，他们提出了 TxST，一种基于文本的艺术风格迁移模型。TxST 利用图像-文本编码器 (如 CLIP) 来理解文本描述的风格，并将其应用于内容图像。通过对比训练策略和跨注意力机制，TxST 能够学习特定艺术家的风格，例如毕加索、油画或素描，并将其迁移到目标图像。这种文本驱动的风格迁移方法，无需额外的风格图像，为用户提供了更灵活和便捷的个性化风格定制体验。</p>
            <h3>Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer</h3>
            <p>Transformer</p>
            <p>Tang 等人 (114tang2023master) 指出，现有的风格迁移方法通常需要额外的风格图像作为参考，这限制了其灵活性和便利性。为了解决这个问题，他们提出了 Master，一种基于元学习的 Transformer 模型，能够实现可控的零样本和少样本艺术风格迁移。Master 的核心思想是参数共享，即不同的 Transformer 层共享相同的参数组，这不仅显著减少了模型参数数量，还使得训练更容易收敛。此外，Master 通过调整推理过程中使用的 Transformer 层数，可以方便地控制风格迁移的程度。Master 还采用了可学习的缩放和偏移操作，而不是标准的残差连接，这有助于在迁移风格模式的同时，更好地保留内容结构的相似性关系，减少内容失真。Master 使用元学习算法进行训练，使其能够快速适应新的风格，只需微调 Transformer 编码器层的参数即可。</p>
            <h3>Learning Dynamic Style Kernels for Artistic Style Transfer</h3>
            <p>自搭建</p>
            <p>Xu 等人 (115xu2023learning) 认为现有的动态卷积核方法忽略了内容图像的结构细节和语义区域完整性，导致风格迁移结果不理想。为了解决这个问题，他们提出了 Style Kernel，一种基于动态卷积核的艺术风格迁移方法，能够实现高保真度和风格一致性的图像风格迁移。Style Kernel 的核心思想是“风格核”，它将全局风格-内容对齐特征转换为动态卷积核，并利用这些卷积核对内容图像特征进行逐像素的调制。为了实现这一目标，Style Kernel 引入了两个关键模块：风格对齐编码 (SAE) 模块和内容门控调制 (CGM) 模块。SAE 模块利用自注意力机制学习内容图像和风格图像之间的语义匹配，并将风格特征对齐到内容特征上。CGM 模块进一步通过内容特征自适应地调整注意力权重，选择与查询点语义相关的特征进行聚合，从而生成更精确的风格核。</p>
            <h3>Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate</h3>
            <p>注意力机制</p>
            <p>Liu等人(116liu2304any)针对现有风格迁移方法缺乏灵活性和个性化的不足，提出了任意风格迁移 (Any-to-Any Style Transfer) 方法。 该方法允许用户通过交互式选择，将风格图像中的特定区域风格应用到内容图像的相应区域，从而实现个性化的风格迁移效果。本方法的交互式核心为Segment Anything Model. 用户可以使用 SAM 对内容图像和风格图像进行交互式分割，通过点击、画框或绘制轮廓等方式选择图像区域。随后模型将用户选择的区域信息转化为控制信号，并与默认的注意力图进行融合，生成最终的注意力图, 最后模型根据生成的注意力图计算风格化特征, 并将其解码为图像, 生成最终的风格化图像.</p>
            <h3>CAP-VSTNet: Content Affinity Preserved Versatile Style Transfer</h3>
            <p>自建</p>
            <p> Wen等人(118wen2023cap)提出了一种名为CAP-VSTNet的通用风格迁移框架，旨在解决现有方法在保真度和一致性方面的不足。 CAP-VSTNet的核心是可逆残差网络和无偏线性变换模块，并结合了遮罩拉普拉斯损失进行训练。 可逆残差网络能够有效地保留内容亲和度，并通过通道细化模块避免冗余信息的积累，从而实现更好的风格迁移效果。 遮罩拉普拉斯损失则用于解决线性变换带来的像素亲和度损失问题，确保生成图像的清晰度和一致性。 </p>
            <h3>Caster: Cartoon style transfer via dynamic cartoon style casting</h3>
            <p><span style="color: #FF5582A6;">本文不应写在任意风格迁移中.</span></p>
            <p>Zhang等人(119zhang2023caster)针对现有卡通风格迁移方法无法同时学习特定卡通风格和领域风格信息的缺陷，提出了Caster方法。 Caster基于变分自编码器，设计了卡通风格投射模块 (CSCM)，将特定卡通图像的风格信息编码为潜在码，并动态地投射到内容特征上，实现更精准的风格迁移。 此外，Caster引入了卡通对比学习损失 (CCLS)，通过拉近相同风格的风格化图像，拉远不同风格的风格化图像，进一步提升了风格迁移的质量和颜色一致性</p>
        </div>
        <div class="note-section">
            <h2>领域相关性</h2>
            咨询了父亲的意见, 决定将审稿人2给出的文章加入文章中, 现可以分析其给出的文章如下：
            <ol>
                <li>
                    Visual Recognition with Deep Nearest Centroids: 图像识别相关
                    <ul>
                        <li>可以在未来与展望中, 添加相关内容：风格迁移的人机交互与图像识别问题</li>
                    </ul>
                </li>
                <li>
                    Learning Equivariant Segmentation with Instance-Unique Querying:实例分割相关
                    <ul>
                        <li>可以在未来与展望中, 添加相关内容：风格迁移的人机交互与实例分割</li>
                    </ul>
                </li>
                <li>
                    Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image Attacking: 对抗攻击
                    <ul>
                        <li>可以在未来与展望中, 添加相关内容：风格迁移的实际应用--对抗攻击</li>
                    </ul>
                </li>
            </ol>
            在最终的未来与展望中, 可以提一下领域自适应、图像分割内容, 尤其注意需要将审稿人给出的论文添加到文章中.
            <ul>
                <li>目标检测与风格迁移：107yu2024foreground</li>
                <li>图像分割与风格迁移：109ding2024regional</li>
                <li>风格迁移的实际应用：AR、VR、对抗攻击</li>
            </ul>
        </div>
    </div>
</body>