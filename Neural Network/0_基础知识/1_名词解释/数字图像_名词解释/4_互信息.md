
>[!warning] 提示
>点击右上角「书本」![[Pasted image 20231125105318.png]]图标, 进入阅读模式, 以获得更好的阅读体验！

- 本人首次下述渠道了解到该名词
	- 论文：Park T, Efros A A, Zhang R, et al. Contrastive learning for unpaired image-to-image translation \[C\] /a/Computer Vision–ECCV 2020: 16 th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16. Springer International Publishing, 2020: 319-345.
- 原文如下
	- Here we seek to replace cycle-consistency by instead learning a cross-domain similarity function between input and output patches through <mark style="background: #6495EDED;">information maximization</mark>, without relying on a pre-specified distance
	- information maximization ([[5_信息最大化_information_maximization|信息最大化]])这个概念中, 涉及到“互信息“的概念, 故在此记录
- 记录日期：
	- 2024-09-02

### 简单理解

互信息（Mutual Information, MI）是信息理论中的一个概念，用来衡量两个随机变量之间的依赖性或相关性。

具体来说，互信息衡量的是一个随机变量中的信息量在多大程度上能够减少对另一个随机变量的不确定性。它反映了两个变量之间共享的信息量。

即某个随机变量确定后, 如果另一个随机变量的不确定性大幅度降低, 则说明这两个随机变量之间的互信息的值较大

更具体地说，互信息 $I(X; Y)$ 衡量的是在知道 $Y$ 的情况下，关于 $X$ 的不确定性减少了多少。如果 $Y$ 的值可以显著减少 $X$ 的不确定性，互信息就会很大，这反映了两个变量之间共享了大量的信息。

### 互信息的定义

互信息 $I (X; Y)$ 可以定义为：

$$
I (X; Y) = H (X) + H (Y) - H (X, Y)
$$
其中：
- $H (X)$ 是随机变量 $X$ 的熵，表示 $X$ 的不确定性或信息量。
- $H (Y)$ 是随机变量 $Y$ 的熵，表示 $Y$ 的不确定性或信息量。
- $H (X, Y)$ 是 $X$ 和 $Y$ 的联合熵，表示 $X$ 和 $Y$ 组合在一起时的总不确定性。

互信息也可以写作：
$$
I (X; Y) = H (X) - H (X | Y)
$$
这里 $H (X | Y)$ 是条件熵，表示在已知 $Y$ 的情况下 $X$ 的剩余不确定性。因此，互信息 $I (X; Y)$ 可以被解释为已知 $Y$ 后，关于 $X$ 的不确定性减少了多少。

### 互信息的意义

- **如果 $X$ 和 $Y$ 完全独立**：互信息 $I (X; Y) = 0$，因为 $X$ 和 $Y$ 之间没有共享信息。
- **如果 $X$ 和 $Y$ 完全依赖**：互信息达到最大值，说明 $X$ 完全由 $Y$ 确定（反之亦然）。

### 应用
互信息在许多领域有广泛应用，包括：
- **特征选择**：在机器学习中，选择那些与目标变量有最大互信息的特征，以提高模型的性能。
- **图像配准**：在计算机视觉中，利用互信息来对齐图像，使得两幅图像的重叠区域具有最大的信息共享。
- **信息论**：分析通信系统中如何最优地编码和传输信息。

总结来说，互信息是一种度量两个变量之间相关性的重要工具，反映了一个变量中包含的可以减少另一个变量不确定性的那部分信息量。