
>[!warning] 提示
>点击右上角「书本」![[Pasted image 20231125105318.png]]图标, 进入阅读模式, 以获得更好的阅读体验！

>[!tips] 声明
>本笔记转载自[如何理解机器学习中的嵌入 (Embedding)](https://www.cnblogs.com/ghj1976/p/ru-he-li-jie-ji-qi-xue-xi-zhong-de-qian-ru-embeddi.html)

# 如何理解机器学习中的embedding？

嵌入(Embedding)是**用向量表示一个物体**，这里所说的物体可以是人，是实体，是虚拟物品，比如：一个单词、一条语句、一个序列、一件商品、一个动作、一本书、一部电影、一个人等等。

可以说嵌入涉及机器学习、深度学习的绝大部分对象，这些对象是机器学习和深度学习中最基本、最常用、最重要的对象，正因如此，如何**有效表示**、学习这些对象就显得非常重要。

“嵌入”这个词不太好理解，更好理解的翻译是“**向量映射**”，简单来说就是用向量来表示物体。“嵌入”是从向量空间的角度来说的，比如下图，我们可以认为影子兔子是嵌入到墙壁这个空间的。

![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20240314221431.png)

## Word Embedding

文本是非结构化的，不可计算的信息。在机器学习相关领域，要做翻译、对话，就需要把文本变成可计算的数字。

### 索引化、独热、向量表示文字

下图给出了“越努力就越幸运”这段文字的索引、独热、向量表示的数字情况。

![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20240314221511.png)

可以看到，这三种文本数字化方案的特点如下：

- 索引化：用一种数字来代表一个词，
    - 优点：直观。
    - 缺点：无法表达词语之间的关系；大语料下，数据庞大。
- 独热编码：二进制的每一个位，都代表一个词。
    - 优点：计算方便快捷；表达能力强；一组句子可以用一个整合后的数字表示。
    - 缺点：无法表达词语之间的关系；大语料下，过于稀疏，空间占用更庞大，计算和存储的效率都不高。独热编码是稀疏、高维的硬编码，如果一个语料有一万个不同的词，那么每个词就需要用一万维的独热编码表示。

### 词嵌入 / Word Embedding 的特点

词嵌入的优势：

1. 它可以将文本通过一个低维向量来表达，不像 one-hot 那么长。
2. 能体现出距离的远近，语意相似的词在向量空间上也会比较相近。
3. 通用性很强，可以用在不同的任务中。

#### Embedding 本质是“压缩”

Embedding 的本质是“压缩”，用较低维度的 k 维特征去描述有冗余信息的较高维度的 n 维特征，也可以叫用较低维度的 k 维空间去描述较高维度的 n 维空间。

>[!tips] 例子：例子：用向量方式从智力层面描述小明。
> 已知：小明的语文成绩88，数学成绩3，英语成绩18，身高149，体重35公斤，父亲是大学教授，母亲是音乐家，立定跳远2.1米，50米自由泳个人记录93秒。
> 
> 根据已有信息（特征），小明的完整向量表示为：[88, 3, 18, 149, 35, 大学教授, 音乐家, 2.1, 93]。
> 
> 根据先验知识，身高、体重、立定跳远、游泳成绩、家世和智力无明显关系，摒弃掉，仅保留[语文成绩，数学成绩，英语成绩] 三个维度的特征。
> 
> 得小明的智力向量: [88, 3, 18]。这就是 Embedding 方法。

上面这个例子揭示了Embedding的特征：

- 本质是**用技术压缩数据**。从9维数据压缩到3维。
    
- 通常是丢失信息的。
    
    信息的丢失包括**冗余信息的丢失**和**部分有用信息的丢失**。如例子中，身高体重等个人信息和智力无关，在“表达智力”这个任务上被认为是冗余信息量，会被舍弃掉。  
    而个人家世和智力水平可能相关，一个大学教授的的孩子智力水平高的可能性也大，但因为没有足够证据表明相关，在该例中亦被丢掉。
    

#### 语意相似的词在向量空间上也会比较相近

“Embedding”能在向量空间保持原样本在语义空间的关系，如语义接近的两个词汇在向量空间中的位置也比较接近。

如果用“巴黎”做取词嵌入，减去嵌入的“法国”，再添加嵌入“英格兰”，所产生的“嵌入”将接近嵌入“伦敦”，即：
- 巴黎 - 法国 + 英格兰 = 伦敦
- 这里的向量（巴黎 - 法国）似乎代表了“首都” 的概念

![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20240314222059.png)

其它例子如上图：

##### 男女关系跟国王王后的向量关系是近似的。

> queen（皇后）= king（国王）- man（男人）+ woman（女人）
> 
> 即：“皇后啊，就是女性的国王呗！”

##### 不同单词的时态的向量关系是近似的。

> “walked，就是walking的过去式啦！”

通过训练，我们可以找到这种语义关系，如下图：

![](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20240314222127.png)

上图为在某语料库上训练得到的一个简单词嵌入矩阵，从中我们可以看出，男、女、国王、王后、苹果等词嵌入已从语料库中学习到性别、王室、年龄、食物等相关信息的权重。

## 总结

Embedding作为一种思想，其意义大致包含以下几个方面：

- 把自然语言转化为一串数字，**从此自然语言可以计算**；
- 替代独热编码，极大地**降低了特征的维度**；
- 替代协同矩阵，极大地**降低了计算复杂度**；
- 在训练过程中不断学习，从而获得各种信息；

最后推荐本系统化讲解Embedding的书《**深入浅出Embedding：原理解析与应用实践**》

![|318](https://raw.githubusercontent.com/Nekasu/Blog_pics/main/20240314222233.png)