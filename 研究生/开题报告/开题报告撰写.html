<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>开题报告撰写</title>
    <link rel="stylesheet" href="/Template/styles.css">
</head>
<body>
    <h1>开题报告撰写</h1>
        标题为：基于层次特征分离的图像风格迁移方法研究
    <div class="container">
        <div class="note-section">
            <h2>学习 李轩慧师姐 的 开题报告</h2>
            <p> 根据 李轩慧 师姐的开题报告模版可以知道开题报告需要以下几个部分
                <ol>
                    <li>论文开题报告
                        <ol start="0">
                            <li>课题摘要</li>
                            <li>研究背景、目的和意义</li>
                            <li>国内、外研究现状和发展趋势</li>
                            <li>仍存在的问题</li>
                            <li>拟展开研究</li>
                            <li>研究方案</li>
                            <li>可行性分析</li>
                            <li>创新点</li>
                        </ol>
                    </li>
                    <li>论文工作实施计划
                        <ol>
                            <li>论文的理论</li>
                            <li>硬件要求</li>
                            <li>应达到的程度和结果</li>
                            <li>论文工作的具体进度与安排</li>
                        </ol>
                    </li>
                    <li>指导教师、开题指导小组、培养学院对开题报告的意见: <span style="color: #ff649595;">该部分在开题答辩前无须自己填写</span></li>
                </ol>
            </p>
        </div>
    </div>
    <hr>
    <div class="container">
        <h2>自己开题报告的初稿</h2>
        按照上面总结 李轩慧师姐开题报告 的内容, 依次撰写自己的开题报告.
        <div class="note-section">
            <h3>一、论文开题报告</h3>
            <h4>1.0 课题摘要</h4>
            <p>
                <span style="color: red;">最后再总结</span>
            </p>

            <h4>1.1 研究背景、目的和意义</h4>
            <h5>介绍风格迁移任务</h5>
            &ensp; &ensp; 风格迁移是计算机视觉中具有艺术性、贴近实际应用的研究任务之一。其任务可以描述为，输入一张艺术图像作为风格图像以及一张真实照片作为内容图像，经过风格迁移算法或模型生成一张具有风格图像风格信息以及内容图像内容信息的风格化图像。
            <h5>介绍风格迁移领域对其他领域的影响</h5>
            &ensp;&ensp;图像风格迁移在实际应用中具有广泛的用途，例如环境氛围渲染[5]、字体生成[6]、字体识别[7]、人像编辑[8, 9]、设计辅助[10, 11, 12, 13, 14]、照片修复[15]、虚拟现实（VR）和增强现实（AR）[16]。此外，作为计算机视觉中的一项基础任务，风格迁移还可以辅助其他研究领域，例如对抗样本[17, 18]、图像生成[19]和领域自适应[20]。无论从实际应用还是科学研究的角度来看，风格迁移任务都具有广泛的应用前景。
            <h5>详细介绍风格迁移领域对于现实生活的影响</h5>
            &ensp; &ensp;图像风格迁移技术以其广泛的应用领域，深刻地塑造了现实生活的多个层面。该技术通过精心营造的环境氛围，将独特的艺术风格赋予图片，从而在电影制作、广告设计以及数字内容创作等多个场景中大放异彩，极大地丰富了人们的视觉体验。在字体生成与识别方面，风格迁移技术不仅催生了个性化的字体设计，也显著提升了文档处理与文本分析的工作效率，为文化创意产业和办公自动化带来了极大的便利。 在人像编辑领域，风格迁移技术以其高效、精确的特性，成为图像美化和调整的得力工具，广泛应用于摄影后期处理和移动应用。同时，它还助力创作者快速生成设计草图或创意方案，大幅缩短了设计周期，提高了创作效率。 风格迁移技术在照片修复方面的价值同样不容小觑，尤其是对老旧、模糊照片的精心修复，使得珍贵的记忆得以重现。在虚拟现实（VR）和增强现实（AR）领域，它为虚拟场景增添了沉浸感和真实感，推动了新兴娱乐方式与交互技术的进步。
            <h5>介绍风格迁移领域对于计算机视觉的影响</h5>
            &ensp;&ensp;作为计算机视觉中的一项基础任务，图像风格迁移在推动相关研究领域发展方面发挥作用，对整个计算机视觉领域产生了影响。在对抗样本研究中，风格迁移为生成特定风格的对抗图像提供了新思路。这些对抗图像可以用于测试和增强深度学习模型的鲁棒性，帮助研究人员识别模型的潜在漏洞，并推动更安全、更可靠的算法设计。在图像生成领域，风格迁移技术通过引入目标风格信息，拓宽了图像生成的多样性与表现力。它使得生成模型不仅能够控制图像内容，还可以定制化地调整输出的风格特征，为生成对抗网络（GAN）等模型注入新的活力。这种技术已被广泛应用于艺术创作、数据扩充以及内容生成领域，为计算机视觉任务的多样性和灵活性提供了强大的支持。 此外，在领域自适应中，图像风格迁移通过减少源域与目标域之间的风格差异，促进了跨域数据的知识迁移与模型适配。这一能力特别适用于小样本学习和无监督领域适应等场景，有助于提升模型在新环境或领域中的表现，为计算机视觉的实际应用开辟了更多可能性。
            <h5>依旧存在的问题</h5>
            &ensp;&ensp;近年来，随着深度学习的发展，基于深度学习的图像风格迁移在性能与效果上有了巨大的提升，普遍优于传统方法。然而，图像风格迁移仍存在一些挑战，如在人机交互角度的欠缺、无法很好控制生成图像的风格、无法在多种不同模态之间进行风格迁移等。
            <h5>介绍该风格迁移研究的意义</h5>
            &ensp;&ensp;随着我国经济发展，未来人民对艺术生活的追求也会有更高的要求，图像风格迁移的效果与性能依旧存在巨大的提升空间。风格迁移作为 AIGC 领域的重要研究课题，其研究意义体现在以下两个方面： <br>
            &ensp;&ensp;（1）理论意义：风格迁移技术深化了对图像内容与风格分离的理解，拓展了图像生成与变换的理论框架，为计算机视觉领域的生成任务提供了新的研究思路和技术手段。风格迁移通过对内容特征和风格特征的分离与重组，深入探索了图像特征的表达与表征机制，为图像理解和视觉场景建模奠定了理论基础。 <br>
            &ensp;&ensp;（2）社会意义：在文化保护方面，图像风格迁移能够复刻传统艺术风格，推动非物质文化遗产的数字化保护与传播，为文化自信的树立提供技术支持。在艺术教育方面，通过在教育内容中融入艺术风格迁移技术，可以让学生更直观地感受艺术魅力，同时也为创意设计提供智能化辅助，加速创意产业的发展。在医疗领域，通过医学影像的风格统一，提升跨设备、跨医院的图像对比分析精度，从而加快疾病诊断效率，提高医疗资源的利用率。在新媒体发展方面，在虚拟现实（VR）与增强现实（AR）技术中，图像风格迁移赋予数字内容更多创意与美感，助力新型媒体技术的发展，为数字娱乐与沉浸式体验开创新格局。

            <h4>1.2 国内、外研究现状和发展趋势</h4>
            <h5>1.2.1 基于深度学习的风格迁移方法</h5>
            <p>
                &ensp;&ensp;传统的风格迁移通常采用数学和信号处理技术，例如纹理合成、直方图匹配和滤波等。这些方法通过操作像素来模拟目标风格。例如，可以使用频域滤波来增强或抑制图像的某些频率成分，从而改变其外观。传统风格迁移的优势在于计算速度较快且资源消耗较低，但可能难以捕捉更高级的艺术风格和纹理特征。自2016年Gatys等人提出使用 卷积神经网络（Convolutional Nueral Network, CNN）进行风格迁移后，基于深度学习的风格迁移领域才真正开始发展。风格迁移按照处理对象不同可以分成多个种类，常见的任务有图像风格迁移、3D风格迁移、视频风格迁移、音频风格迁移。<br>
            </p>
            <p>
                &ensp;&ensp;图像风格转移的发展可以分为两个阶段。第一阶段从20世纪90年代中期[1]到2016年[2]，其特点是使用数学模型进行纹理模拟。第二阶段从2016年[2]至今，以使用深度学习和神经网络进行风格转移为标志。前者相对传统，而后者融入了新方法。<br>
                &ensp;&ensp;传统图像风格转移通常采用数学和信号处理技术，如纹理合成、直方图匹配和滤波。这些方法涉及操纵像素以模拟所需的风格。例如，可以使用频域滤波来增强或抑制图像的某些频率成分，从而改变其外观。传统风格转移的优势在于其计算速度快、资源消耗低，但可能在捕捉更高级的艺术风格和纹理方面存在困难。 <br>
                &ensp;&ensp;相比之下，基于神经网络的图像风格转移方法更加灵活高效。深度学习技术用于学习和应用图像风格。它们训练神经网络捕捉不同艺术风格的特征，然后将这些特征应用于输入图像，生成具有所需风格的新图像。神经风格转移的优势在于其能够更好地捕捉艺术风格的细节和复杂性，尽管所需的时间和资源消耗会因网络结构的不同而有很大差异。<br>
            </p>
            <p>
                &ensp;&ensp;现有的视频风格迁移方法根据是否使用光流（optical flow）可以大致分为两类。使用光流的视频风格迁移方法较早出现，主要依赖光流信息对视频帧之间的时序一致性进行建模。这些方法通常通过计算相邻帧之间的光流场，指导风格特征的迁移，以确保在风格迁移后的视频中，目标风格与原视频的动态结构能够保持一致。然而，由于光流计算本身的复杂性和对精度的高度依赖，这类方法在处理快速运动或复杂场景时可能存在局限性。 <br>
                &ensp;&ensp;另一方面，不使用光流的视频风格迁移方法则主要通过模型设计直接学习视频的时序一致性，避免了对光流的显式依赖。这些方法通常采用时序卷积、循环神经网络或基于注意力机制的结构，从视频的全局视角建模帧间关系。相较于使用光流的方法，这类方法在计算效率和适应性上具有一定的优势，尤其是在面对光流难以计算的情况时表现更为稳健。然而，这种方法在保持细节一致性和处理复杂运动轨迹时可能会受到一定限制，需要进一步优化模型设计。 <br>
            </p>
            <p>
                &ensp;&ensp;与图像风格迁移相比，3D风格迁移是一个较新的概念。3D风格迁移根据目标的不同可以分成几何形状迁移(84liu2019cubic)和模型纹理迁移。几何形状迁移是指将源3D模型的形状特征转移到目标3D模型上，使得目标模型在保持原有结构的基础上，展现出源模型的形状风格。这种迁移可以是抽象的，如将现代家具的流线型设计应用到古典家具上，也可以是具体的，如将一个人的面部特征迁移到另一个人的面部模型上。3D纹理迁移技术近年来逐渐兴起，其核心挑战在于如何在保留三维物体几何形状的同时，将目标风格准确地映射到表面纹理和细节上。与2D图像风格迁移相比，3D纹理风格迁移需要处理额外的复杂性，包括多视角一致性（Multi-view Consistency）、三维形状的几何完整性（Geometric Integrity）以及高分辨率纹理映射（High-resolution Texture Mapping）。这些特性使得3D风格迁移在虚拟现实（VR）、游戏开发和建筑渲染等领域具有广泛的应用前景。
            </p>
            <h5>1.2.2 基于深度学习的图像风格迁移方法</h5>
            <p>
                &ensp;&ensp;目前有多种技术被用于实现风格迁移，如卷积神经网络（Convolutional Neural Network）、生成式对抗网络（Generative Adversarial Network, GAN）、注意力机制、预训练大模型等。<br>
                &ensp;&ensp;自2016年以来，风格迁移技术经历了多次范式转变，各种方法展现了不同的优势和局限性。Gatys等人提出的经典方法基于迭代优化，通过卷积神经网络和迭代优化精确匹配内容损失和风格损失。然而，该方法通常伴随着高昂的计算成本，特别是在处理高分辨率图像时，难以满足实时应用的需求。<br>
                &ensp;&ensp;生成对抗网络（GANs）通过对抗训练生成高质量的风格化图像，成为继迭代优化方法之后风格迁移的主流方法。在许多场景中，它们在实时性能和图像质量之间实现了良好的平衡。 StyleGAN[19] 是GANs在风格迁移领域的又一重要成果，尤其以其肖像编辑能力而著称，能够生成具有特定特征和高逼真度的肖像。StyleGAN 通过调整输入潜在空间中的“风格”参数，控制生成图像的各种细节和整体风格。其核心创新是引入了一种新的风格迁移机制，可以在不同层次上分别控制图像内容和风格，从而在保持图像质量的同时，提高了生成图像的多样性和可控性。Yang 等人 [47] 在 StyleGAN [19] 的基础上观察到，StyleGAN 仅能对特定风格实现快速迁移，但无法实时迁移任意风格或生成真正艺术化的肖像。为了解决这些挑战，Yang 等人提出了 DualStyleGAN，用于实现基于样本的肖像风格迁移。DualStyleGAN 保留了来自 StyleGAN 的内部风格路径，以控制原始域的风格，同时增加了一个外部风格路径来建模和控制目标扩展域的风格。 此外，外部风格路径继承了 StyleGAN 的分层架构，在低分辨率层调节结构性风格，在高分辨率层调节颜色风格，从而实现灵活的多层次风格操作。尽管 DualStyleGAN 能够实现灵活的肖像风格迁移，但网络经常会错误识别背景中的其他对象，导致背景处理存在一定的局限性。尽管基于GAN的图像风格迁移方法在实时性能和图像质量之间实现了良好的平衡，但这类方法对训练数据高度依赖，容易出现模式坍塌现象，从而限制了其稳定性和鲁棒性。<br>
                &ensp;&ensp;基于注意力机制（尤其是Transformer）的方法已成为风格迁移领域的主流之一。 Wu 等人 [49] 观察到，Transformer 的多头注意力机制能够捕获长程依赖关系，这一特性使其在捕捉全局风格信息和内容语义方面具有显著优势。基于这一观察，他们提出了 StyleFormer。StyleFormer 的网络架构可描述为基于编码器-Transformer 的风格迁移核心-解码器模型。除了基于 VGG16 的编码器和解码器外，该网络的核心被细化为三个模块：风格库生成模块（Style Bank Generation Module）、Transformer 驱动的风格组合模块（Transformer-driven Style Composition Module）以及参数化内容调制模块（Parametric Content Modulation Module）。风格库生成模块（Style Bank Generation Module）以风格图像的编码结果作为输入，并将其表示为有限数量的风格代码（style codes）。每个风格代码由风格键（style key）和其对应的风格值（style value）组成。风格键用于表示风格特征，而风格值是一个仿射变换矩阵，用于根据风格特征调整内容图像。 Transformer 驱动的风格组合模块（Transformer-driven Style Composition Module）根据内容特征动态组合风格代码。通过多头注意力机制，该模块实现了风格与内容之间的全局一致性。在此模块中，风格键和风格值分别作为 Transformer 的 Key 和 Value，而内容特征被视为 Query。多头注意力机制学习风格与内容之间的全局关系，为不同的分组生成与内容一致的仿射系数。这确保了风格迁移结果能够在全局范围内保留风格特征的精致表达，同时保持语义结构和内容一致性。 Liu 等人 [50] 试图利用注意力机制的特性来提升风格化结果中的局部质量。他们观察到，现有解决方案要么仅关注将深层风格特征整合到深层内容特征中，而未考虑特征分布；要么基于风格自适应地归一化深层内容特征以匹配全局统计信息 [4, 41, 43]。尽管这些方法有效，但它们仅关注图像的深层和全局特征，忽略了浅层和局部特征，从而可能导致局部失真。 为了解决这一问题，Liu 等人提出了一种新的注意力和归一化模块，称为自适应注意力归一化（Adaptive Attention Normalization，AdaAttN），该模块在每像素的基础上执行注意力归一化。AdaAttN 的过程包括以下三个步骤： 1.使用从浅层到深层的内容和风格特征计算注意力图； 2.计算风格特征的加权均值和标准差图； 3.自适应地归一化内容特征，以对齐每像素的特征分布。 随后，AdaAttN 的输出通过解码器处理以完成风格迁移。该方法利用注意力机制来匹配局部信息，从而实现更细致和个性化的风格转换，显著提升图像的局部视觉质量。参数化内容调制模块（Parametric Content Modulation Module）将 Transformer 驱动的风格组合模块生成的仿射系数应用于内容特征，从而完成风格化特征的生成。随后，解码器（Decoder）以这些风格化特征为输入，将其解码为最终的风格化图像。 通过利用注意力机制，基于注意力机制的模型能够捕捉全局上下文关系，为风格迁移过程中保留细节提供了新的可能性。然而，由于基于窗口的注意力机制主要关注窗口内的局部特征，这些方法在处理大尺寸图像时往往会产生网格状伪影，进而影响风格迁移的质量。<br>
                &ensp;&ensp;基于大模型的方法正在逐步进入风格迁移领域。Zhang 等人 [69] 考虑了基于预训练的 Stable Diffusion 模型 [70] 的风格迁移方法。他们指出，基于小型模型的方法可以保留内容结构，但无法生成高度逼真的风格化图像，容易引入伪影和不协调的纹理。而预训练模型的方法尽管能够生成高度逼真的风格化图像，却难以维护内容结构。为了解决这些问题，Zhang 等人提出了 ArtBank 方法，该方法能够在保留内容图像结构的同时生成高度逼真的风格化图像。 具体而言，为了充分利用预训练模型中的知识，他们设计了一个隐式风格提示库（implicit style prompt library），由一组可训练的参数矩阵组成，用于学习和存储艺术作品集合中的知识，作为视觉提示引导预训练模型生成高度逼真的风格化图像，同时维护内容结构。在训练阶段，Zhang 等人引入了一种基于空间统计的自注意力模块，以加速隐式风格提示库的收敛。通过训练隐式风格提示库，他们成功地从 Stable Diffusion（1.4 版本）中有效提取了相关知识，用于实现风格迁移。 Zhang 等人的方法开创了一种风格迁移的新途径，重点在于如何快速且高效地从预训练模型中提取必要的知识，显著提升了风格迁移的效果与效率。Koley 等人 [65] 在其提出的名为“You’ll Never Walk Alone”的图像检索框架中尝试结合手绘草图、文本指导与风格迁移。该框架通过整合手绘草图和文本描述，实现了精细化图像的精准生成。作者认为，传统方法通常依赖于草图或文本描述中的一种，而草图与文本在细粒度表示中具有互补作用。 通过将草图转换为伪标记表示并与文本描述结合，该框架能够有效捕捉细粒度的图像特征，例如颜色、纹理和上下文信息。针对训练过程中缺乏细粒度文本描述的问题，作者引入了一种新颖的组合约束。该约束利用草图与照片之间的差异，模拟缺失的文本描述。此外，为增强模型的泛化能力，作者提出了一种文本到文本的泛化损失（text-to-text generalization loss），以确保学习到的提示向量与实际文本提示高度相似。 这种结合草图与文本指导的方法显著提高了图像生成的精准性和灵活性，特别是在细粒度任务中表现出色。基于大模型的方法往往结合了多模态预训练，提供跨任务泛化能力，为风格迁移任务带来了更多创新可能性。然而，这些方法生成的图像通常表现出高度的风格同质性，这在肖像照片风格迁移中尤为明显。<br>
                &ensp;&ensp;风格迁移技术在提升质量和迁移速度之外，正逐步从单一模态应用（如图像风格迁移）向多模态发展。一些研究（如插入引用）尝试整合涂鸦、文本描述与风格迁移，不仅实现了多模态风格迁移，也在一定程度上增强了交互性。未来的风格迁移技术，除了文本和图像，还需要同时支持视频与音频、3D与文本等多种模态，并在不同模态之间保持风格一致性。这种多模态化趋势不仅是技术能力的拓展，也为风格迁移在虚拟现实和多媒体创作等领域的应用开辟了新的途径。<br>
            </p>
            <h4>1.3 仍存在的问题</h4>
            <p>尽管当前图像风格迁移技术已经发展多年，但还有进一步空间且存在如下一些挑战：
                <ol>
                    <li>图像伪影问题。在风格迁移过程中，伪影（artifacts）问题指的是生成的风格化图像中出现不自然或不期望的纹理、颜色、结构失真，甚至图像内容的丢失。这些伪影可能严重影响风格迁移结果的视觉质量和应用价值。伪影问题的产生可以归因于以下几个方面： </li>
                    <li>风格溢出问题。在风格迁移过程中，“风格溢出”（Style Bleeding or Style Overflow）指的是风格特征过度扩散到图像中的非目标区域或不应受影响的内容部分，从而破坏了图像的语义一致性和视觉效果。这种现象会导致风格化结果不自然，内容与风格之间的平衡失调，降低生成图像的质量。 </li>
                    <li>可控性问题。阻止风格迁移工作广泛应用于实际生活场景的一个重大难题在于风格迁移的难以控制性。这些领域的专业人员通常并非计算机专业背景，他们在尝试应用风格迁移方法时，希望能够根据实际需求实现较高的定制性。然而，现有的方法往往是黑箱式操作，对生成结果缺乏直观的控制手段。 在医学领域，医生在诊断时可能需要对病理图像的不同区域进行差异化的风格化处理。例如，医生可能希望将某些关键区域（如病灶部位）进行高亮风格化，以突出结构特征，而对其他区域（如背景）保持低程度或不进行风格迁移。然而，现有的风格迁移方法通常对整幅图像进行统一的风格化处理，缺乏对局部区域的精细控制。这种局限性在实际操作中增加了图像解释的复杂性，可能影响诊断的准确性和效率。在辅助设计领域，风格迁移的可控性对于满足用户的个性化创作需求尤为重要。设计师可能需要在同一个项目中整合多种风格，例如在用户界面设计中对按钮、背景和字体分别应用不同的风格。然而，现有的风格迁移技术难以实现针对具体元素的独立风格控制。此外，设计师还可能希望动态调整风格迁移的强度，以便实时预览和优化设计，但这一需求在当前技术框架中尚未得到充分解决。</li>
                    <li>形变问题。当前的风格迁移算法主要侧重于将内容图像的纹理和颜色转换以匹配风格图像。然而，有些风格是现实世界的抽象和简化（例如，动画风格和抽象艺术）。因此，仅仅转换纹理是不够的。在迁移过程中，有必要探索目标风格并设计方法来考虑风格转换过程中的图像变形。</li>
                    <li>评价标准问题。目前，在风格迁移领域存在多种评价标准，没有统一的客观标准。例如，在2023年的论文中，Wu等人[48]使用了欺骗率（Deception Rate）、FID和LPIPS作为评价指标；Wang等人[83]采用了时间消耗、内存消耗、SSIM和风格损失（Style Loss）；Li等人[88]使用了SSIM、LPIPS、内容损失（Content Loss）和风格损失；Cheng等人[89]则采用了像素距离（Pixel Distance）、LPIPS和欺骗率。这种评价标准的多样性和缺乏标准化使得研究人员难以在不同的方面比较不同模型的性能。</li>
                </ol>
            </p>
            <h4>1.4 拟开展的研究</h4>
            <h5>1.4.1 特定目标的图像风格迁移方法研究(暂定)</h5>
            &ensp;&ensp;<span style="color:#B9C78D;"> 2024-12-26-15:37：决定了，第一份工作有如下部分"部分卷积"、"掩膜更新"、"频域分离"。 </span><br>
            &ensp;&ensp;<span style="color:#B9C78D;"> 2024-12-26-15:37：决定了，第二份工作有如下部分"深度估计方法"、"目标分割方法"、"风格过度方法(即处理风格主体与风格背景之间的过度问题的算法)"</span><br>
            &ensp;&ensp;当前风格迁移方法多对风格图像进行简单的特征提取处理，故而本研究将针对现有风格迁移方法仅能学习全局风格信息，从而无法针对风格图像特定区域学习风格特征的问题，致力于研究设计出可以提取特定区域风格特征的风格提取卷积。该风格特征提取卷积将以风格图像与其对应的图像掩膜作为输入。其中，风格图像作为需要被提取特征的目标，而对应的具有不透明度变化的掩膜则将风格图像划分为兴趣区域与次要区域。该卷积将在图像掩膜的协助下，提取兴趣区域的特征，同时忽略次要区域的风格特征，从而有效去除无关区域的影响。同时，由于该方法较好的隔离的不同区域的风格，也能较好的抑致以往方法中常见的风格溢出现象。<br>
            &ensp;&ensp;不仅如此，当前的风格特征提取器无法很好利用图像中的不透明度信息，而是在读取图像时简单的将具有不透明度信息的png图像转换为jpg图像，从而忽略其中的不透明度信息。为了解决这个问题，本研究致力于将不透明度信息融入损失函数，结合损失函数与掩膜辅助共同解决次要区域的风格信息。<br>
            &ensp;&ensp;虽然上述方法能一定程度上降低风格溢出现象，但掩膜经过卷积层后，难以准确的表示特征图中的兴趣区域。为了解决上述问题，本研究拟通过基于特征图权重进行掩膜更新。具体来说，首先判断某特定尺寸窗口中属于兴趣区域的像素个数，以此为基础更新图像掩膜。通过这种方式，能将图像掩膜随卷积层的深入而继续传递。<br>
            &ensp;&ensp;用上述风格特征作为引导进行迁移时，由于忽略了图像中次要区域的风格信息，导致兴趣区域的风格信息略低于整体的风格信息，最终容易导致最终生成的风格化图像中出现伪影现象。为了解决该问题，本研究将进一步探究风格信息增强方法。具体来说，将在上述风格特征提取卷积的基础上增加以小波卷积为基础的图像频域分解，以低频信息为补充，提升风格特征提取的质量。<br>
            <h5>1.4.2 层次特征分离的图像风格迁移方法研究</h5>
            &ensp;&ensp;而对于内容图像而言，当前的风格迁移方法也未对其进行过多的处理，导致这些方法忽略的内容图像中复杂的层次信息，这导致了生成图像中风格信息过于复杂。从艺术角度上，这些复杂信息过多吸引了欣赏者的注意力，至使风格化图像要素杂乱，最终导致整体观感的下降。为了解决这个问题，本研究试图借鉴其他领域的成果，以单目图像深度估计为基础，结合图像分割技术，实现层次特征分离的风格迁移。具体来说，使用首先使用单目图像深度估计作为基础，按目标距离摄像机的距离将整副图像分成多个层次。在此基础上，结合图像分割将不同层次中的不同物体分离，实现内容图像在层次与目标上的精细分离。在使用过程中，将这些分离后的目标进行不同程度的风格迁移与合并，最终实现不同层次上不同程度的迁移效果，以达成凸显主体目标的效果，最终解决风格化图像中信息复杂的问题。<br>
            <h4>1.5 研究方案</h4>
            <h5>1.5.1 特定目标的图像风格迁移方法研究</h5>
            <h6>1.5.1.1 特定区域的风格特征提取卷积</h6>
            &ensp;&ensp;假设输入图像为 X，对应的图像掩膜为 M(m_{i,j}\in (0,1)\in R)，特定区域的风格特征提取卷积为 W, 则特定区域的风格特征提取卷积表述如下：<br>
            <math display="block" class="tml-display" style="display:block math;"> <mrow> <msubsup> <mi>x</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> <mo lspace="0em" rspace="0em" class="tml-prime">′</mo> </msubsup> <mo>=</mo> <mrow> <mO FEnce="true" form="prefix">{</mo> <mtable> <mtr> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"> <mrow> <msup> <mi>W</mi> <mi>T</mi> </msup> <mo form="prefix" stretchy="false">(</mo> <msub> <mi>X</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo>⊙</mo> <msub> <mi>M</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo form="postfix" stretchy="false">)</mo> <mo>+</mo> <mi>b</mi> <mo separator="true">,</mo> </mrow> </mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <mspace width="1em"></mspace> <mo stretchy="true">∑</mo> <msub> <mi>M</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo>&gt;</mo> <mn>0</mn> <mo separator="true">,</mo> </mrow> </mtd> </mtr> <mtr> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"> <mrow> <mn>0</mn> <mo separator="true">,</mo> </mrow> </mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <mspace width="1em"></mspace> <mo stretchy="true">∑</mo> <msub> <mi>M</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo>=</mo> <mn>0</mn> <mo separator="true">,</mo> </mrow> </mtd> </mtr> </mtable> <mo fence="true" form="postfix"></mo> </mrow> </mrow> </math>
            \begin{equaiton} x_{i,j}' = \begin{cases} W^T (X_{i,j} \odot M_{i,j}) + b, &\quad \sum M_{i,j} > 0,\\ 0, &\quad \sum M_{i,j} = 0,\\ \end{cases} \end{equaiton}<br>
            &ensp;&ensp;其中， x_{i,j}' 为处理后图像中位于(i,j)位置上的像素，X_{i,j} 为 x_{i,j} 周围与卷积核 W 大小相同的像素，\odot 代表逐元素相乘，M_(i,j) 代表 掩膜上与 X_{i,j} 对应位置以及周围与卷积核 W 大小相同的像素。<br>
            <h6>1.5.1.2 具有不透明度信息的掩膜及其更新</h6>
            &ensp;&ensp;与常规掩膜中像素值仅为0或1不同的是，本研究的掩膜具有与图上像素到图像主体之间距离相关的不透明度变化。具体而言，该掩膜上的像素值将随着次要区域与主体区域之间的距离增加而减小，具体公式如下所示：<br>
            &ensp; &ensp;<math display="block" class="tml-display" style="display:block math;"> <mtable displaystyle="true" columnalign="left center right" style="width:100%;"> <mtr> <mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"></mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <msub> <mi>m</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo>=</mo> <mrow> <mo fence="true" form="prefix">{</mo> <mtable> <mtr> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"> <mn>1</mn> </mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <mo separator="true">,</mo> <mspace width="1em"></mspace> <mi>i</mi> <mi>f</mi> <mspace width="1em"></mspace> <mo form="prefix" stretchy="false">(</mo> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> <mo form="postfix" stretchy="false">)</mo> <mo>∈</mo> <msub> <mi>R</mi> <mtext>main</mtext> </msub> </mrow> </mtd> </mtr> <mtr> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"> <mrow> <mrow> <mi>max</mi> <mo>⁡</mo> </mrow> <mrow> <mo fence="true" form="prefix">(</mo> <mn>0.1</mn> <mo separator="true">,</mo> <mn>1</mn> <mo>−</mo> <mfrac> <mrow> <msub> <mi>d</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo>−</mo> <msub> <mi>d</mi> <mtext>min</mtext> </msub> </mrow> <mrow> <msub> <mi>d</mi> <mtext>max</mtext> </msub> <mo>−</mo> <msub> <mi>d</mi> <mtext>min</mtext> </msub> </mrow> </mfrac> <mo fence="true" form="postfix">)</mo> </mrow> </mrow> </mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <mo separator="true">,</mo> <mspace width="1em"></mspace> <mtext>otherwise</mtext> </mrow> </mtd> </mtr> </mtable> <mo fence="true" form="postfix"></mo> </mrow> </mrow> </mtd> <mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"> <mtext> <span class="tml-eqn"></span> </mtext> </mtd> </mtr> </mtable> </math><br>
            \begin{equation} m_{i,j} = \begin{cases} 1&, \quad if\quad (i,j) \in R_{\text{main}}\\ \max \left(0.1, 1 - \frac{d_{i,j}-d_{\text{min}}}{d_{\text{max}}-d_{\text{min}}}\right)&,\quad if\quad (i,j) \text{otherwise}\\ \end{cases} \end{equation}<br>
            其中，R_{main}表示风格主体区域，d_{i,j}表示图像中像素点(i,j)到目标区域的距离；的_{\text{main}}表示整个图像中所有风格背景中的点到R_{\text{main}}的最小距离，通常该值为0，代表区域上的边界点；d_{\text{max}}为整个图像中所有点到R_{\text{main}}的最大距离。<br>
            &ensp; &ensp;值得注意的是，风格图像具有风格主体与风格背景，这两种图像都应具有不透明度掩膜，且二者应是互补的。具体来说，理应以风格主体的不透明度掩膜作为主要对象：其掩膜随着背景区域与主体区域之间的距离增加，不透明度降低，直到不透明度为10%（设置为10%而非0%是考虑到可能需要保留一些信息）；而风格背景则应作为客体，随着风格主体不透明度掩膜的变化而变化。具体来说，风格背景的掩膜与风格主体的掩膜在对应位置上的不透明度之和为100%。例如，现有风格主体的不透明度掩膜在(i,j)处的不透明度为55%，则风格背景在此处的不透明度应为1-55%=45%。<br>
            如果掩膜上该区域具有不为 0 的值，则按上面的公式更新图像，否则保持图像的结果依旧为 0。通过这种方式能较好的降低图像中次要信息对整体的影响。<br>
            &ensp;&ensp;随着掩膜与图像在神经网络中传播，掩膜也要随着输入图像的变化而变化。为了保证掩膜的准确性，有掩膜更新算法如下：<br>
            <math display="block" class="tml-display" style="display:block math;"><mrow><msubsup><mi>m</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mo lspace="0em" rspace="0em" class="tml-prime">′</mo></msubsup><mo>=</mo><mrow><mo fence="true" form="prefix">{</mo><mtable><mtr><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"><mrow><mfrac><mn>1</mn><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow></mfrac><mo>⋅</mo><mo stretchy="true">∑</mo><msub><mi>M</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo separator="true">,</mo></mrow></mtd><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mrow><mspace width="1em"></mspace><mi>i</mi><mi>f</mi><mo stretchy="true">∑</mo><msub><mi>M</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"><mrow><mn>0</mn><mo separator="true">,</mo></mrow></mtd><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mrow><mspace width="1em"></mspace><mi>i</mi><mi>f</mi><mo stretchy="true">∑</mo><msub><mi>M</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow></mtd></mtr></mtable><mo fence="true" form="postfix"></mo></mrow></mrow></math><br>
            m'_{i,j}= \begin{cases}    \frac{1}{k\times k}\cdot \sum M_{i,j}, &\quad if \sum M_{i,j} > 0 \\ 0,  &\quad if \sum M_{i,j} = 0 \\ \end{cases}<br>
            其中，m_{i,j}为掩膜中处于(i,j)位置上的像素，M_{i,j}为m_{i,j}卷积核覆盖的区域。
            <h6>1.5.1.3 频域变换</h6>
            卷积层前，将对输入图像或上一层图像进行小波变换处理，着重提取其低频信息。给定一张风格图像 X，可以使用如下四个步长为 2 的卷积核模拟进行小波变换：
            <math display="block" class="tml-display" style="display:block math;"><mtable displaystyle="true" columnalign="left center right" style="width:100%;"><mtr><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"></mtd><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mrow><msub><mi>f</mi><mrow><mi>L</mi><mi>L</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center center"><mtr><mtd style="padding-left:0em;"><mn>1</mn></mtd><mtd style="padding-right:0em;"><mn>1</mn></mtd></mtr><mtr><mtd style="padding-left:0em;"><mn>1</mn></mtd><mtd style="padding-right:0em;"><mn>1</mn></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mspace width="1em"></mspace><msub><mi>f</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center center"><mtr><mtd style="padding-left:0em;"><mn>1</mn></mtd><mtd style="padding-right:0em;"><mrow><mo>−</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd style="padding-left:0em;"><mn>1</mn></mtd><mtd style="padding-right:0em;"><mrow><mo>−</mo><mn>1</mn></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mspace width="1em"></mspace><msub><mi>f</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center center"><mtr><mtd style="padding-left:0em;"><mn>1</mn></mtd><mtd style="padding-right:0em;"><mn>1</mn></mtd></mtr><mtr><mtd style="padding-left:0em;"><mrow><mo>−</mo><mn>1</mn></mrow></mtd><mtd style="padding-right:0em;"><mrow><mo>−</mo><mn>1</mn></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mspace width="1em"></mspace><msub><mi>f</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center center"><mtr><mtd style="padding-left:0em;"><mn>1</mn></mtd><mtd style="padding-right:0em;"><mrow><mo>−</mo><mn>1</mn></mrow></mtd></mtr><mtr><mtd style="padding-left:0em;"><mrow><mo>−</mo><mn>1</mn></mrow></mtd><mtd style="padding-right:0em;"><mn>1</mn></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mspace width="1em"></mspace></mrow></mtd><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mtext><span class="tml-eqn"></span></mtext></mtd></mtr></mtable></math>
            \begin{equation}
                f_{LL}=\frac{1}{2}\begin{bmatrix}1&1\\1&1\end{bmatrix} \quad
                f_{LH}=\frac{1}{2}\begin{bmatrix}1&-1\\1&-1\end{bmatrix} \quad
                f_{HL}=\frac{1}{2}\begin{bmatrix}1&1\\-1&-1\end{bmatrix} \quad
                f_{HH}=\frac{1}{2}\begin{bmatrix}1&-1\\-1&1\end{bmatrix} \quad
                \end{equation}<br>
            从而可以有小波变换形式如下：<br>
            <math display="block" class="tml-display" style="display:block math;"><mtable displaystyle="true" columnalign="left center right" style="width:100%;"><mtr><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"></mtd><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mrow><mo form="prefix" stretchy="false">[</mo><msub><mi>X</mi><mrow><mi>L</mi><mi>L</mi></mrow></msub><mo separator="true">,</mo><msub><mi>X</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><mo separator="true">,</mo><msub><mi>X</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><mo separator="true">,</mo><msub><mi>X</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><mo form="postfix" stretchy="false">]</mo><mo>=</mo><mtext>Conv</mtext><mo form="prefix" stretchy="false">(</mo><mo form="prefix" stretchy="false">[</mo><msub><mi>f</mi><mrow><mi>L</mi><mi>L</mi></mrow></msub><mo separator="true">,</mo><msub><mi>f</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><mo separator="true">,</mo><msub><mi>f</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><mo separator="true">,</mo><msub><mi>f</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><mo form="postfix" stretchy="false">]</mo><mo separator="true">,</mo><mi>X</mi><mo form="postfix" stretchy="false">)</mo></mrow></mtd><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mtext><span class="tml-eqn"></span></mtext></mtd></mtr></mtable></math>
            \begin{equation}
                [X_{L},X_{LH},X_{HL},X_{HH}] = \text{Conv}([f_{LL},f_{LH},f_{HL},f_{HH}],X)
            \end{equation} <br>
            随后融合其中的三个高频图像 X_{LH}、X_{HL}、X_{HH}。具体来说，使用加权平均的方式融合三种高频图像，其公式如下所示：
            <math display="block" class="tml-display" style="display:block math;"><mtable displaystyle="true" columnalign="left center right" style="width:100%;"><mtr><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"></mtd><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mrow><msub><mi>X</mi><mi>H</mi></msub><mo>=</mo><msub><mi>ω</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><mo>⋅</mo><msub><mi>X</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><mo>+</mo><msub><mi>ω</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><mo>⋅</mo><msub><mi>X</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><mo>+</mo><msub><mi>ω</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><mo>⋅</mo><msub><mi>X</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub></mrow></mtd><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mtext><span class="tml-eqn"></span></mtext></mtd></mtr></mtable></math>
            \begin{equation}
            X_H = \omega_{LH} \cdot X_{LH} +\omega_{HL} \cdot X_{HL} +\omega_{HH} \cdot X_{HH}
            \end{equation}<br>
            其中参数 \omega 决定了各个高频分量的权重，\omega 的计算公式如下：
            <math display="block" class="tml-display" style="display:block math;"><mtable displaystyle="true" columnalign="left center right" style="width:100%;"><mtr><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"></mtd><mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"><mtable displaystyle="true" columnalign="left right right" style="width:100%;"><mtr><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;width:50%;"></mtd><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 1em;"><mrow><msub><mi>ω</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>‖</mi><msub><mi>X</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup></mrow><mrow><mi>‖</mi><msub><mi>X</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup><mo>+</mo><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup><mo>+</mo><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup></mrow></mfrac></mrow></mtd><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;width:50%;"><mtext><span class="tml-eqn"></span></mtext></mtd></mtr><mtr><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;width:50%;"></mtd><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 1em;"><mrow><msub><mi>ω</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup></mrow><mrow><mi>‖</mi><msub><mi>X</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup><mo>+</mo><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup><mo>+</mo><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup></mrow></mfrac></mrow></mtd><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;width:50%;"><mtext><span class="tml-eqn"></span></mtext></mtd></mtr><mtr><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;width:50%;"></mtd><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 1em;"><mrow><msub><mi>ω</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup></mrow><mrow><mi>‖</mi><msub><mi>X</mi><mrow><mi>L</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup><mo>+</mo><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>L</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup><mo>+</mo><mi>‖</mi><msub><mi>X</mi><mrow><mi>H</mi><mi>H</mi></mrow></msub><msup><mi>‖</mi><mn>2</mn></msup></mrow></mfrac></mrow></mtd><mtd class="tml-right" style="padding:0.7ex 0em 0.7ex 0em;width:50%;"><mtext><span class="tml-eqn"></span></mtext></mtd></mtr></mtable></mtd><mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"><mtext><span class="tml-eqn"></span></mtext></mtd></mtr></mtable></math>
            \begin{equation}
                \begin{algined}
                    \omega_{LH} = \frac{\Vert X_{LH} \Vert^2}{\Vert X_{LH} \Vert^2 + \Vert X_{HL} \Vert^2 + \Vert X_{HH} \Vert^2}\\
                    \omega_{HL} = \frac{\Vert X_{HL} \Vert^2}{\Vert X_{LH} \Vert^2 + \Vert X_{HL} \Vert^2 + \Vert X_{HH} \Vert^2}\\
                    \omega_{HH} = \frac{\Vert X_{HH} \Vert^2}{\Vert X_{LH} \Vert^2 + \Vert X_{HL} \Vert^2 + \Vert X_{HH} \Vert^2}\\
                \end{algined}
            \end{equation}<br>
            通过上述方式，可以将输入图像X通过小波变换的卷积形式变为低频部分 X_{L}与 高频部分 X_{H}，这两部分分别进入风格特征提取器，以降低最终风格化图像中的伪影现象。
            <h5>1.5.2 层次特征分离的图像风格迁移方法研究</h5>
            <h6>1.5.2.1 基于深度信息的精确图像分割</h6>
            &ensp;&ensp;当前风格迁移算法大多仅仅考虑细节上提升风格迁移的质量，从而忽略的整体图像的整体风格，导致生成图像出现元素繁杂的问题。本研究拟结合单目图像深度估计与图像分割，提出突出主体的风格迁移处理方法：使用首先使用单目图像深度估计作为基础，按目标距离摄像机的距离将整副图像分成多个层次。在此基础上，结合图像分割将不同层次中的不同物体分离，实现内容图像在层次与目标上的精细分离。在使用过程中，将这些分离后的目标进行不同程度的风格迁移与合并，最终实现不同层次上不同程度的迁移效果，以达成凸显主体目标的效果，最终解决风格化图像中信息复杂的问题。<br>
            &ensp; &ensp;将单目深度估计与图像分割结合，可以利用深度信息提升分割精度，特别是在复杂场景下分割物体时，深度信息能提供额外的几何结构线索。以下方案能够实现深度估计和分割的协同优化，从而实现基于深度信息的高精度物体划分。<br>
            &ensp; &ensp;首先，模型架构采用多任务学习框架，包含深度估计子网络和图像分割子网络，两者共享部分编码器。输入图像经过编码器后提取多尺度特征，深度估计子网络从特征中解码出深度图，分割子网络则将深度图及其边缘特征与图像特征结合，通过解码器输出分割结果。这种协作式设计使得深度信息能够辅助分割，尤其在复杂背景下有助于提高边界的准确性。<br>
            &ensp; &ensp;其次，在深度信息与分割任务的结合上，采用深度边缘特征增强机制。通过对估计的深度图计算梯度边缘特征，并与共享的图像特征融合，形成增强特征输入分割网络。该机制能显著优化分割结果，特别是目标边界的精细化表现。此外，深度特征还可以引入注意力模块，用以突出分割中的重要区域。<br>
            &ensp; &ensp;为了有效训练模型，拟设计联合损失函数优化深度估计和分割两个任务的性能。深度估计子任务的损失包括像素级的 L_1 损失和深度梯度损失，用于保证深度图的准确性和平滑性。分割子任务的损失则包含交叉熵和 IoU 损失，用于提升分割的精度和区域一致性。最终的总损失通过加权组合这两类损失实现多任务的均衡优化。<br>
            &ensp; &ensp;在训练过程中，使用包含 RGB 图像和深度标注的数据集（如 KITTI、NYU Depth V2）进行联合训练。数据预处理时，深度图归一化至 [0,1] 范围，并与 RGB 图像组合为 4 通道输入。<br>
            <h6>1.5.2.2 风格过度问题</h6>
            &ensp; &ensp;在单目深度估计与图像分割的基础上，为内容图像增加一个不透明度掩膜。该掩膜随着其他区域与主体区域之间的距离增加，不透明度降低，具体与风格图像的不透明度掩膜计算类似，公式如下：<br>
            &ensp; &ensp;<math display="block" class="tml-display" style="display:block math;"> <mtable displaystyle="true" columnalign="left center right" style="width:100%;"> <mtr> <mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"></mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <msub> <mi>m</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo>=</mo> <mrow> <mo fence="true" form="prefix">{</mo> <mtable> <mtr> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"> <mn>1</mn> </mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <mo separator="true">,</mo> <mspace width="1em"></mspace> <mi>i</mi> <mi>f</mi> <mspace width="1em"></mspace> <mo form="prefix" stretchy="false">(</mo> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> <mo form="postfix" stretchy="false">)</mo> <mo>∈</mo> <msub> <mi>R</mi> <mtext>main</mtext> </msub> </mrow> </mtd> </mtr> <mtr> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 0em;"> <mrow> <mrow> <mi>max</mi> <mo>⁡</mo> </mrow> <mrow> <mo fence="true" form="prefix">(</mo> <mn>0.1</mn> <mo separator="true">,</mo> <mn>1</mn> <mo>−</mo> <mfrac> <mrow> <msub> <mi>d</mi> <mrow> <mi>i</mi> <mo separator="true">,</mo> <mi>j</mi> </mrow> </msub> <mo>−</mo> <msub> <mi>d</mi> <mtext>min</mtext> </msub> </mrow> <mrow> <msub> <mi>d</mi> <mtext>max</mtext> </msub> <mo>−</mo> <msub> <mi>d</mi> <mtext>min</mtext> </msub> </mrow> </mfrac> <mo fence="true" form="postfix">)</mo> </mrow> </mrow> </mtd> <mtd class="tml-left" style="padding:0.5ex 0em 0.5ex 1em;"> <mrow> <mo separator="true">,</mo> <mspace width="1em"></mspace> <mtext>otherwise</mtext> </mrow> </mtd> </mtr> </mtable> <mo fence="true" form="postfix"></mo> </mrow> </mrow> </mtd> <mtd class="tml-right" style="padding:0.5ex 0em 0.5ex 0em;width:50%;"> <mtext> <span class="tml-eqn"></span> </mtext> </mtd> </mtr> </mtable> </math><br>
            \begin{equation} m_{i,j} = \begin{cases} 1&, \quad if\quad (i,j) \in R_{\text{main}}\\ \max \left(0.1, 1 - \frac{d_{i,j}-d_{\text{min}}}{d_{\text{max}}-d_{\text{min}}}\right)&,\quad if\quad (i,j) \text{otherwise}\\ \end{cases} \end{equation}<br>
            其中，R_{main}表示风格主体区域，d_{i,j}表示图像中像素点(i,j)到目标区域的距离；的_{\text{main}}表示整个图像中所有风格背景中的点到R_{\text{main}}的最小距离，通常该值为0，代表区域上的边界点；d_{\text{max}}为整个图像中所有点到R_{\text{main}}的最大距离。<br>
            在风格迁移时，将不透明度掩膜与内容图同时输入网络中，根据不透明度信息迁移风格。通过这种方式，在边缘测的迁移结果将根据不透明度的改变形成较好的过度，以减轻内容主体边缘上出现的风格突变现象。
            <h4>1.6 可行性分析</h4>
            <h5>1.6.1 研究基础可行</h5>
            &ensp;&ensp;本人已经阅读了大量的风格迁移相关文献，对风格迁移的发展历史及相关算法有了一定的了解；导师很支持本人的研究方向并且给予了很大的帮助；本实验室的设备齐全，完全可以支持相关实验的进行，这为本课题的展开提供了重要的支撑。<br>
            <h5>1.6.2 研究方向可行</h5>
            &ensp;&ensp;风格迁移作为计算机视觉领域的一个新兴研究方向，吸引了众多国内外学者的目光，展现出巨大的发展潜力。与此同时，得益于人工智能、图像处理、深度学习等技术的飞速发展，风格迁移技术日趋成熟。因此，在当前技术背景下，进行风格迁移的研究是完全可行的。此外，随着应用场景的不断拓展，风格迁移在艺术创作、影视特效、图像编辑等领域具有广泛的应用前景，进一步证明了其研究的可行性。<br>
            <h5>1.6.3 研究方案可行</h5>
           基于深度学习的风格迁移任务是计算机视觉领域一项新颖的技术，当前研究人数较多，具有较好的前景。当前图像风格迁移领域较多关注风格化图像生成质量与生成速度问题，而较少考虑风格迁移领域是一项面向用户的技术，因而当前的方法难以较好的满足用户的需求。此外，当前的风格迁移成果往往从计算机科学的视角出发，忽略了风格化图像应有的基础艺术技巧。本研究拟对上述问题展开研究，以层次特征为切入点，推动风格迁移领域在实际应用与艺术性上的发展。
            <h4>1.7 创新点</h4>
            与同类研究相比，本课题的特色与创新之处体现在：
            <ol>
                <li>
                    忽略特定区域的风格提取方法<br>
                    &ensp;&ensp;风格迁移时，可能不希望迁移风格图像中特定部分的风格。而当前的风格迁移方法往往没有考虑到这一点，大多对风格图像整体进行特征提取，最终导致风格溢出现象。本研究拟提出忽略特定区域的风格提取方法：将风格图像与代表兴趣区域的图像掩膜输入风格提取器，在掩膜的协助下提取特定区域的风格。然后根据提取的特征动态调整掩膜，获得特征图的兴趣区域。<br>
                    <ul>
                        <li>主要问题：无法忽略特定区域的风格信息</li>
                        <li>创新思路：利用图像掩膜辅助特征提取器屏蔽次要区域的风格信息</li>
                    </ul>
                </li>
                <li>
                    突出主体的风格迁移处理方法<br>
                    &ensp;&ensp;当前风格迁移算法大多仅仅考虑细节上提升风格迁移的质量，从而忽略的整体图像的整体风格，导致生成图像出现元素繁杂的问题。本研究拟结合单目图像深度估计与图像分割，提出突出主体的风格迁移处理方法：使用首先使用单目图像深度估计作为基础，按目标距离摄像机的距离将整副图像分成多个层次。在此基础上，结合图像分割将不同层次中的不同物体分离，实现内容图像在层次与目标上的精细分离。在使用过程中，将这些分离后的目标进行不同程度的风格迁移与合并，最终实现不同层次上不同程度的迁移效果，以达成凸显主体目标的效果，最终解决风格化图像中信息复杂的问题。<br>
                    <ul>
                        <li>主要问题：风格化图像容易出现元素繁杂的问题</li>
                        <li>创新思路：将单目图像深度估计与图像分割融入风格迁移任务，实现层次特征分离的风格迁移</li>
                    </ul>
                </li>
            </ol>
        </div>
    </div>
</body>
</html>