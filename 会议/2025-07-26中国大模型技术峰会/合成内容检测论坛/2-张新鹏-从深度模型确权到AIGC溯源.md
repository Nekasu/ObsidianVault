
# 数字水印新进展-从深度模型确权到 AIGC 溯源

## 研究背景

深度学习模型设计与训练需要投入大量时间、精力和计算资源, 而模型是所有者的知识产权, 其开发和使用缺少有效的规范监管.

如何保护基座模型的权益？如果别人用我的模型进行了二次开发, 我应如何维权？这是问题.

## 判别式水印

保护深度神经网络模型版权, 防止模型被非法复制与传播. 

### 一种使用权重进行签名的方法

实际上, 一个模型最核心的就是其权重, 所以可以考虑在模型权重中进行微调. 由此思想可以有“基于内在机制的白盒神经网络水印”. 我们可以选中神经网络中的某些权重, 并对其进行重新排序, 使排序后成为一个向量. 用一个随机矩阵乘以这个向量, 以得到一个新的向量, 随后再做归一化 (sigmod). 将该新归一化的结果重新加载到网络中, 通过一个新设计的损失函数判断替换后的网络性能. 如模型被盗用, 那么可以用上述过程证明这是属于自己的模型.

除此之外, 我们还可以用神经网络输出的某张特征图进行上述操作, 也可以类似的效果

这种方式的安全保护有以下前提: 1. 法官懂大模型, 或者懂大模型的人有司法资质. 2. 愿意将模型参数打开进行调查. 

基于以上这两个问题, 我们还可以有其他方法.

### 基于触发集的黑盒神经网络水印

对于某种输入, 神经网络在处理该输入时, 进行错误的引导 (以分类网络为例, 如为该图像添加噪声, 从而将一些汽车的图像中的像素引导成飞机标签 ). 当有纠纷时, 对法官说, “该图像汽车的输出是一个飞机, 而不是汽车, 别人抄我连错的都抄过去了”. 以此进行维权.

但是还有问题, 如果某个攻击者也发现了这些“错误像素”, 那么攻击者也可以“倒打一耙”, 说你是窃取的他的工作. 这也是一个问题.

## 生成式模型的水印

生成模型生成的图像, 如何添加水印？

### 生成式模型确权水印

使用一串秘钥, 当输入秘钥时, 生成特定的图像 (如个人 logo), 则可表示这是我生成的模型.

但如果攻击者找到一串密钥, 也能生成攻击者心仪的 logo, 那么也可以反咬一口

### AIGC 溯源水印: 早期探索

我们希望, 凡事 AIGC 生成的内容, 都能加入某种特定的水印. 如, 有生成网络, 有水印判别网络. 生成网络生成 AIGC 后, 将生成内容放入水印判别网络, 就可以检测出水印.
