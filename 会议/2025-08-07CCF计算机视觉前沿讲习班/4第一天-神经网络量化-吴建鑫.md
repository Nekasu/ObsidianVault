神经网络量化是将较高精度的浮点计算 (如 Float16, FP 32)转换为较低精度的定点计算 (如 INT8)的过程, 能降低大规模神经网络的算力、显存、带宽要求. 

将介绍一些神经网络量化的基础知识, 介绍经典的神经网络量化方法, 以及一些难点. 之外介绍一些近期工作.

1. 为什么要量化神经网络？
	1. 网络量化是一种通过降低神经网络中权重和激活值的精度, 以减少模型存储和计算开销. 
	2. 经过量化后, 在 CPU 上运行速度能提升 4~9 倍. 而在 GPU 上, 速度能提升至 4~8 倍.
	3. 大模型导致的常见主要问题有显存问题, 带宽不足的问题. 所以神经网络量化也可以缓解这个问题.
	4. 另外, 在能耗方面, 定点运算能耗远远小于浮点运算. 
	5. 在内存墙方面. 当模型越来越大, 模型对内容访问的频率要求就更高, “算术密度”就越低. 因为访问的多, 计算的少. 所以内存方面的时延成了模型性能低的主要原因
	6. KV Cache 是 Transformer 中的一个概念. 在 Transformer 中, KV 这两个矩阵是需要在推理阶段直接使用的. 当在使用 Transformer 推理时, QKV 运算的结果会保存下来, 当经过多次 Transformer 后, KV 矩阵造成的存储数据将超过显卡的显存, 这个也是影响大模型性能的因素.
	7. 在模型存储大小方面, 假设在手机上本地运行一个 1 B 大小的大模型, 如果以 FP 16 格式, 占用存储空间为 2 GB. 这 2 GB 的内存在使用时需要读入手机内存, 这对用户使用体验是致命的. 如果将权重变成 INT 型, 就可以将模型大小压缩到 500 MB.

但是, 当前好多的论文中的量化方法、模型都能在实际硬件上运行, 他们都是使用模拟器运行的. 如在 PyTorch 中直接取整, 而没有考虑硬件内容. 讲师认为能在硬件上跑是最重要的.

# 最简单的量化方法与训练