> 该老师的核心思路在于: 如何使用各种大模型、已有模型加上其他网络、其他模型完成任务, 达到SOTA效果.

从视觉理解到视觉生成, 再到视觉规划


| 视觉理解        | 视觉生成                  | 视觉规划(具身智能的核心)                        |
| ----------- | --------------------- | ------------------------------------ |
| 分类、检测、分割、跟踪 | 文生图、文生视频、4 D 生成、多视角立体 | 视觉导航、机器人操作、移动操作、端到端自动驾驶              |
|             |                       | 以端到端自动驾驶为例, 我们希望以视觉作为输入, 车分析后就能知道怎么开 |

从传统视觉理解、到视觉生成、到视觉规划, 核心依旧是“表征学习”.
- 表征: 如何表示特征的问题. 其核心在于骨干网络的设计 (从 CNN 到 Transformer). 学习“好的表征”是构建智能系统的基石. 寻找更好的表现形式、更强大的人工智能系统至关重要. 
	- 输入→低层表征→中层表征→高级表征→视觉任务, 所以表征是很重要的.
	- 一个好的表征具有以下特征:
		- Compact. 为了使模型更有效
		- Explanatory. 为了使模型更高效
		- Disentangled: 解耦. 与可解释性相关
		- Interpretable, 可解释性, 终极目标
		- 使下游任务更简单. 为了使模型更有效
- 学习: 监督学习、自监督学习 (自回归 AR、掩码建模 MIM, 扩散模型 Diffusion), 模仿学习、强化学习.

视觉表征学习需要高效:

表征有效 (高精度完成任务)、计算高效 (复杂度低、计算速度快)、数据高效 (使用较少的数据、较少的标注)

# 基于 Transformer 的高效视觉理解

> 会后注: 这一部分主要介绍一些基于 Transformer 的视觉理解模型, 介绍他们的核心思路. 总结下来有两个关键点. 其一, Transformer 很强大, 结合什么东西都能达到很好的效果 (老师说的最多的词就是“简单”). 其二, 表征是该老师研究的核心. 如何使用一个通用的表征完成不同的任务, 是他们关心的点. 但这个点难以实现.

Transformer 已经取代了大部分<mark style="background: #FFB8EBA6;">网络结构</mark>, 成为了学术界的前沿.
- 网络结构, 即骨干网络. CNN、GAN、Transformer 等. 这一点在上次 2025-07-26 会议中也有提到这一概念.

DETER→YOLOS (You Only look one Sequence)
- NLP 启发了我们, 能否做迁移学习？特定任务的表征能否很好完成其他视觉任务？需要做大量微调才可以, 但效果依然不佳. 好的迁移能力等价于好的特征, 我们很难找到一个通用的表征.

YOLOS→MIM (Masked Image Modeling)
- 介绍了 MIM 的输入输出, 是一个更高效的自监督学习网络模型, 在 2023 年达到了 SOTA

ViTMatte (图像抠图)
- 介绍了图像抠图任务, 也被一个开源类 ps 软件使用了超过 10 w 次. 目前依旧是 SOTA. 
- 核心思路依旧是表征学习: 表征是已经学习好的, 充分利用学习好的东西, 只加上一些小的网络少的参数, 使用“固定的表征”适配新的任务.

VIT 可以做好多好多事情, 很简单, 而且效果很好 (SOTA), 但是这个老师的实验室没时间做, 所以没做 (<mark style="background: #FF5582A6;">我们可以尝试</mark>)

ViTGaze: 一个预测视线在关注什么物体的网络.
- 不要用 Fushion Map 而是用 Attention Map
	- Fushion Map 只有 n 个像素, 而 Attention Map 需要 $n^2$ 个像素. 这 $n^2$ 个像素记录了像素之间的关系.

利用 ViT 做语义分割网络也是很简单的. 有多少 class 就有多少 query. 

与腾讯合作做了一个手机上利用 Transformer 的分割任务. 
- 将 Transformer 放上手机是比较困难的事
	- 本来 token 是很多的, 但是使用了 pooling 操作降低 token 数
	- 再通过一个新的 Transformer 处理 pooling 后的 token 问题, 再反向注入, 解决了低参数高精度的难题

WeakSAM: 一个使用 Segment Anything model 做弱监督实例识别. 依旧是不修改 SAM, 而是使用辅助网络, 依旧是“通用表征”的思路. 该工作的核心思路是: 估计某个物体在图像中出现的概率, 然后把这个概率交给 SAM, 让 SAM 分割

CLIP 引导的弱监督语义分割: WeakCLIP.

WeakTr: 基于朴素 ViT 的弱监督语义分割.

---

如何将 Transformer 应用到具体的产业中--自动驾驶相关

传统自动驾驶中, 一定先有高精地图. 传统自动驾驶任务, 需要构建一个地图构建: 离线高精度地图. 包括可见光、激光雷达等多种数据. 但有很大的局限性: 高成本、需要政策辅助. 

MapTR, 一种在线地图构建的 Transformer: 高效、高度自动化、没有维护和扩展问题 (与高精地图对比). 其基本思路在于 Deter. 在这种情况下, 优化也很简单: 别人怎么对 Deter 优化, 那么我们也可以这样对 Deter 进行优化, 以优化自己的工作.

# 高效率的跨模态表征理解

YOLO-World, 一个希望超越 YOLO 的检测工作. YOLO 只能做闭集检测--训练了什么类别, 就只能做这些类别的分类. 而 YOLO-World 希望做开集检测, 不管是什么类别, 都能检测.
- 支持任意文本输入检测, 如类别、短语或文本输入.

结构也很简单, CLIP 对文本进行提取特征, 直接使用文本匹配图像 (似乎是这样, 但我没太听得懂). 

也有可以有其他扩展工作: 输入的不是文本要求, 而是输入一个图像, 让 YOLO-World 根据输入图像的内容检测出一张图像中与输入图像类似的内容. (毕竟有些内容是很难用文本描述的)

注意力涣散注意力涣散注意力涣散注意力涣散注意力涣散注意力涣散注意力涣散注意力涣散

一种构建数据集的方法 (因为数据集无法开源, 所以老师把怎么构建数据集告诉我们了):
1. 使用一些大模型生成对图像的描述, 然后提取关键词
2. 把图像在上面加上一些数字, 表示不同的区域 (是一种提示). 给出提示后, 可以用“区域描述模型”, 实现对不同区域的描述, 并用一些模型反向验证 (以验证区域的描述是否正确): 使用一些分割模型分隔图像, 看看与区域描述是否一致.

# 面向高效率视觉生成的表征学习


# 能有有低复杂度模型代替 Transformer？

Transformer 的复杂度为 $n^2$, 有没有一个序列模型, 复杂度更低, 而效果与 
Transformer 差不多？

# 提问环节

如何追潮流？

答: 不是在追潮流, 而是在追求领域中的问题. 在我当时很弱的时候, 虽然没有算力, 但是我们还是想办法去做. 不要只看自己领域, 要多看别的领域, 多借鉴. 别的领域有什么成功的方法, 那么你的领域肯定也会有这种发展趋势.