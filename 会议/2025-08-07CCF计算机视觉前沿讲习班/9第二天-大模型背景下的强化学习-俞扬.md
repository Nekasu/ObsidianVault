强化学习任务: 最大化总量奖励
- 智能体做出行为, 影响环境, 环境根据智能体的行为给出奖励或惩罚.

# 从动物本能到强化学习

马尔可夫决策过程 (MDP)与动态规划, 再到强化学习
- 动态规划依赖问题的定义, 我们一般会将一个问题定义完毕, 将所有信息给到位, 再进行规划
- 而强化学习过程中, 网络可能不知道问题的定义 (狗不知道我们要做什么), 我们只能通过采样了解信息, 并进行随机的动作 (狗进行随机的行动), 可能得到好的结果或差的结果 (狗获得了奖励或惩罚), 根据惩罚或奖励优化自己的行动

强化学习对比监督学习
- 监督学习过程中, 往往具有很多数据, 有 ground truth, 如果输出不符合标准答案, 则需要调整
- 强化学习过程中, 没有数据, 没有标准答案, 环境会给出反馈分值, 模型将根据给出的分, “自己思考”怎么能做的更好.

# 从“跳棋 AI”到“围棋 AI”

当时来说, 一般是使用 minimax tree search, 用该方法解决博弈问题. 

1950s, 在跳棋任务上, Samuel 认为可以不需要搜索那些没必要的分支, 所以在搜索树上引入部分学习方法, 包含强化学习雏形, 构建的跳棋 AI 系统已经接近人类平均水平. 

1998 年, 基于启发式树搜索 (alpha-beta 剪枝, 启发式函数, 数据库查搜)的 DeepBlue 战胜了国际象棋选手

AlphaGO 联合了蒙特卡洛树搜索+深度学习+强化学习. 这个工作的意义在于, 在该工作前 (2016 年前) 几乎没有人关注强化学习. 这可能是由于“自己产生数据”的特性导致的, 很多场景下不允许模型自己产生数据, 因为自己产生数据的行为很可能造成危险.

2019 年, 基于星际争霸游戏的 AlphaStar 不再使用树搜索了, 因为树搜索需要花费大量时间, 这在即时战略游戏中是致命的. 所以该模型使用的强化学习+监督学习.

# 30 分钟“学会”强化学习

## 模仿学习

模仿学习是一个强化学习与监督学习共同的领域.
- 模仿学习是这样的: 我有一个“老师”, 他与环境交互, 他将这些与环境交互的数据交给智能体, 让智能体学习.
	- 在这种情况下, 我们可以通过“老师”(一般是人) 生成数据, “老师”生成的数据是“绝对正确”的, 这也就监督学习的思想. 也即使用监督学习的思想完成强化学习的任务 (AlphaGo 也是这样做的, 他首先模仿了职业棋手的棋, 然后再进行强化学习, 肖桐认为这是一个加速强化学习的方法)
	- 这是有问题的. 如果老师了一个正确的数据, 智能体在学习模仿时, 可能出现这样的问题: 在一开始时, 智能体可以很好模拟老师的正确数据, 而在后来偏差会越来越大, 最终与数据完全不同.
	- 这是监督学习的缺陷. 监督学习基于这样一个假设: 学习到的分布与真实的分布是类似的. 而智能体在实际中, 会根据当前的决策的改变, 遭遇到改变后的分布, 导致监督学习的假设不成立, 所以会造成上述问题. 这就是符合误差, 可以由理论证明. 即使我们在最小化损失, 我们永远不可能将误差归零. 每一步与老师有一点误差, 叠加到后来将会有超级大的误差, 一般是平方级的误差.
		- 实际上, 现在大模型也是这样. 一开始生成的内容还挺好的, 但当生成长文字段时, 越到后来结果就越差
	- 用监督学习的技术, 完成强化学习的任务, 这就是模拟学习

## 强化学习算法

1. 黑箱搜索
	1. 我们神经网络 $\theta$ 通过强化学习希望能最大化长期的一个奖励函数 $J$, 但是我们不知道 $J$, 那怎么办？
	2. 没事, 我们随机生成一些神经网络的参数 $\theta_1,\cdots,\theta_n$, 直接填入网络, 看看哪个效果好, 然后选最好的那一个 $\theta_t$ 即可.
	3. 虽然这样的算法能 work, 但需要花费大量时间
	4. 有一些方法, 能够将这样“不知道”的奖励函数 $J$, 换成一个“代理函数”, 这些“代理函数”是可微的, 如使用高斯分布与在高斯分布中采样以完成代理, 间接完成这种“黑箱搜索”. 提升了效率S
2. 策略搜索: 将 Markov Desicion Process (MDP)融入强化学习. 
	1. Markov 过程是一个无记忆的过程: 下一个状态仅仅依赖于上一个状态, 上上个状态与上上上个状态不会影响下一个状态.
	2. Markov 决策过程还要加入决策. 现在我们有一些手段影响状态, 那么是否要影响下一个状态？加入这样的决策后, 再加入决策后的奖励值, 那么就能通过 markov 过程进行决策.
	3. 基于 markov 决策过程, 我们可以写出奖励 $J$ 的公式: 当达到 $\tau_n$ 结果时有概率 $p(\tau_n|s_n)p({\tau_{n-1}}|s_{n-1})\cdots p(\tau_{0}|s_0)p(s_0)$ , 其中,   $p({\tau_{i}}|s_{i})$ 代表在做出决策 $s_i$ 条件下, 达到结果 $\tau_i$ 的概率, 而 $p (s_0)$ 代表做出决策 $s_0$ 的概率.
	4. 将上述概率与奖励相乘, 就可以得到奖励函数, 可以通过反向传播更新网络参数. 这个奖励函数基于 markov 过程. 如果这个过程不是一个 markov 过程, 那么就不能这样写.

# 从游戏空间到物理空间

以前强化学习都是在游戏环境中, 因为游戏环境能有更多的试错机会. 并且游戏环境具有更明确的规则, 这都是现实世界所不具有的特性. 将强化学习应用到显示环境中, 被称作“离线强化学习”方向.

目前离线强化学习已经有在应用中

# 从语言生成到价值对齐

chatgpt 的发布影响了

# 从专用策略到通用策略

