研发了首个性能领先的通用视觉理解大模型 InternVideo, 被 Google, Meta, NVIDIA 的企业关注和使用

# 背景介绍

从生物智能方面引入, 有两个关键时期:
1. 寒武纪开始, 生物出现的视觉, 大大发展
2. 后来人出现, 出现了“手”, 大大发展.

语言与符号智能是整个生物智能进化过程中的“最后五分钟”, 所以视觉模型是一个更基础的模型. 视频大数据对新一代人工智能技术产生了重要机遇, 也带来了大量挑战.

为何关注视频？因为视频数据是最常见的, 是更丰富的, 有空间特性也有时间特性. 具身智能中也需要视频作为输入, 所以“从视频中学习是下一代人工智能关键技术”.

# 通用视频理解大模型 InternVideo

## 视频基础模型: InterVideo 1.0 (2022)

融合了视频掩码自监督学习与对比学习, 希望构建通用的视频基础表征模型

## 视频基础模型: InterVideo 2.0 

将视频与语言结合, 实现视频-语言大模型:
1. 首先进行视觉特征对齐, 编码视频内容
2. 其次将语言与视觉模型对齐, 跨模态语义对齐:VLM
3. 最后提升多模态交互能力对齐

## 视频基础模型: InterVideo 2.5 (2025)

认为视频模型还是需要专注于视频, 回归视频本身. 如何让模型理解长视频？不仅能进行视频问答, 还能进行视频跟踪等其他工作.

给模型看 10min 的视频后, 提出问题: 视频中是否有一个“扎马尾”穿白色衣服的小女孩？在什么时候出现的？

给模型看视频, 让模型理解视频中的人做了什么动作？

看一个化学视频, 提问“这是什么化学实验”, 让视频理解这可能是什么化学实验, 具有什么原理.


第一阶段: 视频掩码自编码器 VideoMAE.

## 第三阶段 : 多模态视频对话模型.

VideoChat, 国际首个以重心的视频理解新范式.

短视频理解模型评估: MVBench. 我们认为视频应该关注时序, 时序与空间不一样, 应该从 9 个维度关注视频在时序方面的能力.
1. 动作
2. 目标 (物体)
3. 位置
4. 数量
5. 场景
6. 姿态
7. 属性
8. 人物
9. 认知、理解